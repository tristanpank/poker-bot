{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Poker Agent V8: Realistic Opponent Modeling\n",
                "\n",
                "This notebook expands the V7 Adaptive Agent to train against **Realistic Opponents**. \n",
                "\n",
                "### New Opponent Types (Heuristic Bots)\n",
                "Humans don't play like pure Maniacs or pure Nits. New bots use **Monte Carlo Hand Evaluation** to make decisions:\n",
                "\n",
                "1. **ValueBot (Tight-Aggressive)**: Only bets/raises with strong hands (high win probability). Folds weak hands. \n",
                "   * *Strategy*: If Win% > 70%, Raise. If Win% > 40%, Call. Else Fold.\n",
                "2. **BluffBot (Loose-Aggressive)**: Plays like ValueBot but mixes in bluffs.\n",
                "   * *Strategy*: Similar to ValueBot, but 30% of the time with weak hands, it raises to bluff.\n",
                "3. **BalancedBot (Basic Strategy)**: Considers **Pot Odds**.\n",
                "   * *Strategy*: Calls if Win% > Pot Odds. Raises with very strong hands.\n",
                "\n",
                "### Architecture (Same as V7)\n",
                "- **Dual-Branch DRQN**: (MLP for State + LSTM for History)\n",
                "- **Persistent Memory**: Remembers action history across hands to identify opponent style."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "id": "1096c10f",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Using device: cuda\n"
                    ]
                }
            ],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "import torch.nn.functional as F\n",
                "import numpy as np\n",
                "import gymnasium as gym\n",
                "from gymnasium import spaces\n",
                "from collections import deque\n",
                "import random\n",
                "import matplotlib.pyplot as plt\n",
                "from typing import Optional, Tuple, List, Dict, Any\n",
                "import copy\n",
                "from itertools import combinations\n",
                "\n",
                "from pokerkit import Automation, NoLimitTexasHoldem, Card, StandardHighHand, Deck\n",
                "\n",
                "# Constants\n",
                "SEED = 42\n",
                "MAX_HISTORY_LEN = 100\n",
                "ACTION_EMBED_DIM = 16\n",
                "HIDDEN_DIM_LSTM = 128\n",
                "\n",
                "# Actions\n",
                "ENV_FOLD = 0\n",
                "ENV_CHECK_CALL = 1\n",
                "ENV_BET_RAISE = 2\n",
                "NUM_ACTIONS = 3\n",
                "\n",
                "# History Tokens\n",
                "ACT_PAD = 0\n",
                "ACT_V_FOLD = 1\n",
                "ACT_V_CHECK_CALL = 2\n",
                "ACT_V_BET_RAISE = 3\n",
                "OPP_FOLD = 4\n",
                "OPP_CHECK_CALL = 5\n",
                "OPP_BET_RAISE = 6\n",
                "OUT_AGENT_WIN = 7\n",
                "OUT_AGENT_LOSS = 8\n",
                "OUT_TIE = 9\n",
                "OUT_NEW_HAND = 10\n",
                "HISTORY_VOCAB_SIZE = 11\n",
                "\n",
                "random.seed(SEED)\n",
                "np.random.seed(SEED)\n",
                "torch.manual_seed(SEED)\n",
                "\n",
                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "print(f\"Using device: {device}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "id": "781fd653",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Re-using V7 Environment and Model Components\n",
                "\n",
                "class PersistentPokerEnv(gym.Env):\n",
                "    def __init__(self, num_players: int = 2, starting_stack: int = 1000, \n",
                "                 small_blind: int = 5, big_blind: int = 10):\n",
                "        super().__init__()\n",
                "        self.num_players = num_players\n",
                "        self.starting_stack = starting_stack\n",
                "        self.small_blind = small_blind\n",
                "        self.big_blind = big_blind\n",
                "        self.game_state_dim = 52*2 + 52*5 + num_players + 1 + 1 + 4 + 1 \n",
                "        self.observation_space = spaces.Dict({\n",
                "            'game_state': spaces.Box(low=0, high=1, shape=(self.game_state_dim,), dtype=np.float32),\n",
                "            'history': spaces.Box(low=0, high=HISTORY_VOCAB_SIZE-1, shape=(MAX_HISTORY_LEN,), dtype=np.int64)\n",
                "        })\n",
                "        self.action_space = spaces.Discrete(NUM_ACTIONS)\n",
                "        self.state = None\n",
                "        self.agent_player_index = 0\n",
                "        self.global_history = deque(maxlen=MAX_HISTORY_LEN)\n",
                "        for _ in range(MAX_HISTORY_LEN): self.global_history.append(ACT_PAD)\n",
                "        \n",
                "    def _card_to_index(self, card: Card) -> int:\n",
                "        ranks = '23456789TJQKA'\n",
                "        suits = 'cdhs'\n",
                "        rank_idx = ranks.index(card.rank)\n",
                "        suit_idx = suits.index(card.suit)\n",
                "        return rank_idx * 4 + suit_idx\n",
                "    \n",
                "    def _encode_card(self, card: Optional[Card]) -> np.ndarray:\n",
                "        encoding = np.zeros(52, dtype=np.float32)\n",
                "        if card is not None: encoding[self._card_to_index(card)] = 1.0\n",
                "        return encoding\n",
                "    \n",
                "    def _flatten_cards(self, cards) -> List:\n",
                "        flat = []\n",
                "        # Handle both list of cards and single card case safely\n",
                "        if isinstance(cards, Card): return [cards]\n",
                "        for item in cards:\n",
                "            if hasattr(item, 'rank'): flat.append(item)\n",
                "            else: flat.extend(self._flatten_cards(item))\n",
                "        return flat\n",
                "    \n",
                "    def _get_observation(self) -> Dict[str, Any]:\n",
                "        state_vector = []\n",
                "        hole_cards = self._flatten_cards(self.state.hole_cards[self.agent_player_index])\n",
                "        for i in range(2):\n",
                "            if i < len(hole_cards): state_vector.extend(self._encode_card(hole_cards[i]))\n",
                "            else: state_vector.extend(np.zeros(52, dtype=np.float32))\n",
                "        board_cards = self._flatten_cards(self.state.board_cards)\n",
                "        for i in range(5):\n",
                "            if i < len(board_cards): state_vector.extend(self._encode_card(board_cards[i]))\n",
                "            else: state_vector.extend(np.zeros(52, dtype=np.float32))\n",
                "        for i in range(self.num_players):\n",
                "            stack = self.state.stacks[i] / self.starting_stack\n",
                "            state_vector.append(min(stack, 2.0))\n",
                "        total_pot = sum(self.state.bets)\n",
                "        state_vector.append(total_pot / (self.starting_stack * self.num_players))\n",
                "        if self.state.actor_index is not None:\n",
                "            state_vector.append(self.state.actor_index / max(1, self.num_players - 1))\n",
                "        else: state_vector.append(0.0)\n",
                "        street = [0.0, 0.0, 0.0, 0.0]\n",
                "        num_board = len(board_cards)\n",
                "        if num_board == 0: street[0] = 1.0\n",
                "        elif num_board == 3: street[1] = 1.0\n",
                "        elif num_board == 4: street[2] = 1.0\n",
                "        else: street[3] = 1.0\n",
                "        state_vector.extend(street)\n",
                "        state_vector.append(float(self.agent_player_index))\n",
                "        \n",
                "        return {\n",
                "            'game_state': np.array(state_vector, dtype=np.float32),\n",
                "            'history': np.array(list(self.global_history), dtype=np.int64)\n",
                "        }\n",
                "    \n",
                "    def _update_history(self, player_idx: int, action: int):\n",
                "        if player_idx == self.agent_player_index:\n",
                "            if action == ENV_FOLD: token = ACT_V_FOLD\n",
                "            elif action == ENV_CHECK_CALL: token = ACT_V_CHECK_CALL\n",
                "            else: token = ACT_V_BET_RAISE\n",
                "        else:\n",
                "            if action == ENV_FOLD: token = OPP_FOLD\n",
                "            elif action == ENV_CHECK_CALL: token = OPP_CHECK_CALL\n",
                "            else: token = OPP_BET_RAISE\n",
                "        self.global_history.append(token)\n",
                "\n",
                "    def append_outcome_token(self, final_reward: float):\n",
                "        if final_reward > 0: self.global_history.append(OUT_AGENT_WIN)\n",
                "        elif final_reward < 0: self.global_history.append(OUT_AGENT_LOSS)\n",
                "        else: self.global_history.append(OUT_TIE)\n",
                "\n",
                "    def _get_legal_actions(self) -> List[int]:\n",
                "        legal = []\n",
                "        if self.state.can_fold(): legal.append(ENV_FOLD)\n",
                "        if self.state.can_check_or_call(): legal.append(ENV_CHECK_CALL)\n",
                "        if self.state.can_complete_bet_or_raise_to(): legal.append(ENV_BET_RAISE)\n",
                "        return legal if legal else [ENV_CHECK_CALL]\n",
                "    \n",
                "    def _execute_action(self, action: int) -> None:\n",
                "        if action == ENV_FOLD:\n",
                "            if self.state.can_fold(): self.state.fold()\n",
                "            elif self.state.can_check_or_call(): self.state.check_or_call()\n",
                "        elif action == ENV_CHECK_CALL:\n",
                "            if self.state.can_check_or_call(): self.state.check_or_call()\n",
                "            elif self.state.can_fold(): self.state.fold()\n",
                "        elif action == ENV_BET_RAISE:\n",
                "            if self.state.can_complete_bet_or_raise_to():\n",
                "                min_r = self.state.min_completion_betting_or_raising_to_amount\n",
                "                max_r = self.state.max_completion_betting_or_raising_to_amount\n",
                "                self.state.complete_bet_or_raise_to(min(min_r * 2, max_r))\n",
                "            elif self.state.can_check_or_call():\n",
                "                self.state.check_or_call()\n",
                "    \n",
                "    def _run_automations(self) -> None:\n",
                "        while self.state.can_burn_card(): self.state.burn_card('??')\n",
                "        while self.state.can_deal_board(): self.state.deal_board()\n",
                "        while self.state.can_push_chips(): self.state.push_chips()\n",
                "        while self.state.can_pull_chips(): self.state.pull_chips()\n",
                "    \n",
                "    def reset(self, seed=None, options=None) -> Tuple[Dict, Dict]:\n",
                "        self.global_history.append(OUT_NEW_HAND)\n",
                "        super().reset(seed=seed)\n",
                "        self.state = NoLimitTexasHoldem.create_state(\n",
                "            automations=(Automation.ANTE_POSTING, Automation.BET_COLLECTION, Automation.BLIND_OR_STRADDLE_POSTING, Automation.HOLE_CARDS_SHOWING_OR_MUCKING, Automation.HAND_KILLING, Automation.CHIPS_PUSHING, Automation.CHIPS_PULLING),\n",
                "            ante_trimming_status=True,\n",
                "            raw_antes={-1: 0},\n",
                "            raw_blinds_or_straddles=(self.small_blind, self.big_blind),\n",
                "            min_bet=self.big_blind,\n",
                "            raw_starting_stacks=[self.starting_stack] * self.num_players,\n",
                "            player_count=self.num_players,\n",
                "        )\n",
                "        while self.state.can_deal_hole(): self.state.deal_hole()\n",
                "        self._run_automations()\n",
                "        return self._get_observation(), {'legal_actions': self._get_legal_actions()}\n",
                "    \n",
                "    def step(self, action: int) -> Tuple[Dict, float, bool, bool, Dict]:\n",
                "        if self.state.actor_index is not None:\n",
                "             self._update_history(self.state.actor_index, action)\n",
                "        self._execute_action(action)\n",
                "        self._run_automations()\n",
                "        done = self.state.status is False\n",
                "        reward = 0.0\n",
                "        if done:\n",
                "            reward = (self.state.stacks[self.agent_player_index] - self.starting_stack) / self.big_blind\n",
                "        obs = self._get_observation()\n",
                "        info = {'legal_actions': self._get_legal_actions() if not done else []}\n",
                "        return obs, reward, done, False, info\n",
                "    \n",
                "    def get_final_reward(self) -> float:\n",
                "        return (self.state.stacks[self.agent_player_index] - self.starting_stack) / self.big_blind\n",
                "    \n",
                "    def update_opponent_history(self, action: int):\n",
                "        opp_idx = 1 - self.agent_player_index\n",
                "        self._update_history(opp_idx, action)\n",
                "\n",
                "class DualBranchV8(nn.Module):\n",
                "    def __init__(self, state_dim: int, action_dim: int):\n",
                "        super().__init__()\n",
                "        self.state_net = nn.Sequential(\n",
                "            nn.Linear(state_dim, 256), nn.LayerNorm(256), nn.ReLU(),\n",
                "            nn.Linear(256, 128), nn.ReLU()\n",
                "        )\n",
                "        self.action_embedding = nn.Embedding(HISTORY_VOCAB_SIZE, ACTION_EMBED_DIM)\n",
                "        self.lstm = nn.LSTM(input_size=ACTION_EMBED_DIM, hidden_size=HIDDEN_DIM_LSTM, batch_first=True)\n",
                "        self.value_head = nn.Sequential(\n",
                "            nn.Linear(128 + HIDDEN_DIM_LSTM, 256), nn.ReLU(),\n",
                "            nn.Linear(256, action_dim)\n",
                "        )\n",
                "        \n",
                "    def forward(self, state, history):\n",
                "        s_feat = self.state_net(state)\n",
                "        h_embed = self.action_embedding(history)\n",
                "        lstm_out, (hn, cn) = self.lstm(h_embed)\n",
                "        h_context = hn[-1]\n",
                "        combined = torch.cat([s_feat, h_context], dim=1)\n",
                "        return self.value_head(combined)\n",
                "\n",
                "class ReplayBufferV8:\n",
                "    def __init__(self, capacity=50000):\n",
                "        self.buffer = deque(maxlen=capacity)\n",
                "    def push(self, transition):\n",
                "        self.buffer.append(transition)\n",
                "    def sample(self, batch_size):\n",
                "        return random.sample(self.buffer, min(len(self.buffer), batch_size))\n",
                "    def __len__(self): return len(self.buffer)\n",
                "\n",
                "class AdaptiveAgentV8:\n",
                "    def __init__(self, state_dim, action_dim=NUM_ACTIONS, lr=1e-4):\n",
                "        self.model = DualBranchV8(state_dim, action_dim).to(device)\n",
                "        self.target_model = DualBranchV8(state_dim, action_dim).to(device)\n",
                "        self.target_model.load_state_dict(self.model.state_dict())\n",
                "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
                "        self.gamma = 0.99\n",
                "        self.epsilon = 1.0\n",
                "        self.epsilon_min = 0.05\n",
                "        self.epsilon_decay = 0.99995\n",
                "        \n",
                "    def select_action(self, obs, legal_actions, eval_mode=False):\n",
                "        if not eval_mode and random.random() < self.epsilon:\n",
                "            return random.choice(legal_actions)\n",
                "        state_t = torch.FloatTensor(obs['game_state']).unsqueeze(0).to(device)\n",
                "        h_t = torch.LongTensor(obs['history']).unsqueeze(0).to(device)\n",
                "        with torch.no_grad():\n",
                "            q_values = self.model(state_t, h_t)\n",
                "        q_numpy = q_values.cpu().numpy().flatten()\n",
                "        masked_q = np.full(NUM_ACTIONS, -np.inf)\n",
                "        for a in legal_actions: masked_q[a] = q_numpy[a]\n",
                "        return int(np.argmax(masked_q))\n",
                "\n",
                "    def train(self, buffer, batch_size=64):\n",
                "        if len(buffer) < batch_size: return None\n",
                "        batch = buffer.sample(batch_size)\n",
                "        states = torch.FloatTensor(np.array([t[0] for t in batch])).to(device)\n",
                "        histories = torch.LongTensor(np.array([t[1] for t in batch])).to(device)\n",
                "        actions = torch.LongTensor(np.array([t[2] for t in batch])).to(device)\n",
                "        rewards = torch.FloatTensor(np.array([t[3] for t in batch])).to(device)\n",
                "        next_states = torch.FloatTensor(np.array([t[4] for t in batch])).to(device)\n",
                "        next_histories = torch.LongTensor(np.array([t[5] for t in batch])).to(device)\n",
                "        dones = torch.FloatTensor(np.array([t[6] for t in batch])).to(device)\n",
                "        \n",
                "        current_q = self.model(states, histories).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
                "        with torch.no_grad():\n",
                "            next_actions = self.model(next_states, next_histories).argmax(1).unsqueeze(1)\n",
                "            target_q_next = self.target_model(next_states, next_histories).gather(1, next_actions).squeeze(1)\n",
                "            target = rewards + (1 - dones) * self.gamma * target_q_next\n",
                "        loss = F.mse_loss(current_q, target)\n",
                "        self.optimizer.zero_grad()\n",
                "        loss.backward()\n",
                "        torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
                "        self.optimizer.step()\n",
                "        return loss.item()\n",
                "    \n",
                "    def update_target(self): self.target_model.load_state_dict(self.model.state_dict())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "id": "bc2675fc",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Heuristic Hand Evaluator\n",
                "\n",
                "def monte_carlo_equity(hole_cards: List[Card], board_cards: List[Card], iterations=50) -> float:\n",
                "    \"\"\"Simulate hand outcome against a random hand to estimate equity.\"\"\"\n",
                "    if not hole_cards: return 0.5 \n",
                "    \n",
                "    wins = 0\n",
                "    known_cards = set(hole_cards + board_cards)\n",
                "    \n",
                "    for _ in range(iterations):\n",
                "        deck_cards = [c for c in Deck.STANDARD if c not in known_cards]\n",
                "        random.shuffle(deck_cards)\n",
                "        \n",
                "        opp_hole = deck_cards[:2]\n",
                "        \n",
                "        needed_board = 5 - len(board_cards)\n",
                "        sim_board = board_cards + deck_cards[2:2+needed_board]\n",
                "        \n",
                "        # Total 7 cards for each\n",
                "        my_total = hole_cards + sim_board\n",
                "        opp_total = opp_hole + sim_board\n",
                "        \n",
                "        # FIX: StandardHighHand takes exactly 5 cards. \n",
                "        # We must find the best 5-card subset from the 7 available cards.\n",
                "        my_hand = max(StandardHighHand(c) for c in combinations(my_total, 5))\n",
                "        opp_hand = max(StandardHighHand(c) for c in combinations(opp_total, 5))\n",
                "        \n",
                "        if my_hand > opp_hand:\n",
                "            wins += 1\n",
                "        elif my_hand == opp_hand:\n",
                "            wins += 0.5\n",
                "            \n",
                "    return wins / iterations\n",
                "\n",
                "# Flattening helper for extracting cards safely\n",
                "def flatten_cards_list(items):\n",
                "    out = []\n",
                "    # Handle single card or list\n",
                "    if isinstance(items, Card): return [items]\n",
                "    for x in items:\n",
                "        if isinstance(x, (list, tuple)):\n",
                "            out.extend(flatten_cards_list(x))\n",
                "        else:\n",
                "            out.append(x)\n",
                "    return out\n",
                "\n",
                "class HeuristicBot:\n",
                "    def __init__(self, player_idx=1):\n",
                "        self.player_idx = player_idx\n",
                "        \n",
                "    def get_equity(self, state):\n",
                "        # Extract cards from state and flatten them to ensure simple list of Cards\n",
                "        # state.hole_cards[idx] might be [c1, c2]\n",
                "        hole = flatten_cards_list(state.hole_cards[self.player_idx])\n",
                "        board = flatten_cards_list(state.board_cards)\n",
                "        return monte_carlo_equity(hole, board, iterations=40)\n",
                "        \n",
                "    def select_action(self, state, legal_actions):\n",
                "        raise NotImplementedError\n",
                "\n",
                "class ValueBot(HeuristicBot):\n",
                "    \"\"\"Bets only with strong hands.\"\"\"\n",
                "    def select_action(self, state, legal_actions):\n",
                "        equity = self.get_equity(state)\n",
                "        \n",
                "        if equity > 0.70:\n",
                "            if ENV_BET_RAISE in legal_actions: return ENV_BET_RAISE\n",
                "        \n",
                "        if equity > 0.45:\n",
                "            if ENV_CHECK_CALL in legal_actions: return ENV_CHECK_CALL\n",
                "        \n",
                "        # Fold if weak\n",
                "        if ENV_FOLD in legal_actions: return ENV_FOLD\n",
                "        return ENV_CHECK_CALL\n",
                "\n",
                "class BluffBot(HeuristicBot):\n",
                "    \"\"\"Mixes value bets with bluffs.\"\"\"\n",
                "    def select_action(self, state, legal_actions):\n",
                "        equity = self.get_equity(state)\n",
                "        \n",
                "        # Strong: Raise \n",
                "        if equity > 0.70:\n",
                "             if ENV_BET_RAISE in legal_actions: return ENV_BET_RAISE\n",
                "        \n",
                "        # Weak: Bluff chance (30%)\n",
                "        if equity < 0.40:\n",
                "            if random.random() < 0.30:\n",
                "                 if ENV_BET_RAISE in legal_actions: return ENV_BET_RAISE\n",
                "        \n",
                "        # Medium: Check/Call\n",
                "        if equity > 0.45:\n",
                "             if ENV_CHECK_CALL in legal_actions: return ENV_CHECK_CALL\n",
                "             \n",
                "        if ENV_FOLD in legal_actions: return ENV_FOLD\n",
                "        return ENV_CHECK_CALL\n",
                "\n",
                "class BalancedBot(HeuristicBot):\n",
                "    \"\"\"Considers Pot Odds.\"\"\"\n",
                "    def select_action(self, state, legal_actions):\n",
                "        equity = self.get_equity(state)\n",
                "        \n",
                "        # Calculate Pot Odds\n",
                "        # Pot odds = Call Amount / (Total Pot + Call Amount)\n",
                "        # We approximate relative to pot size\n",
                "        \n",
                "        # If equity is huge, raise\n",
                "        if equity > 0.8:\n",
                "             if ENV_BET_RAISE in legal_actions: return ENV_BET_RAISE\n",
                "             \n",
                "        # Call logic based on Odds\n",
                "        if ENV_CHECK_CALL in legal_actions:\n",
                "            # Simple threshold: if > 50% equity always call\n",
                "            if equity > 0.5: return ENV_CHECK_CALL\n",
                "            \n",
                "            # If equity > 0.3 and it's just a check or small bet, call\n",
                "            current_bet = max(state.bets)\n",
                "            my_bet = state.bets[self.player_idx]\n",
                "            to_call = current_bet - my_bet\n",
                "            total_pot = sum(state.bets)\n",
                "            \n",
                "            if to_call == 0: return ENV_CHECK_CALL # Check\n",
                "            \n",
                "            pot_odds = to_call / (total_pot + to_call + 1e-5)\n",
                "            if equity > pot_odds:\n",
                "                return ENV_CHECK_CALL\n",
                "        \n",
                "        if ENV_FOLD in legal_actions: return ENV_FOLD\n",
                "        return ENV_CHECK_CALL\n",
                "\n",
                "# Original Opponents\n",
                "class ManiacAgent:\n",
                "    def select_action(self, state, legal_actions):\n",
                "        if ENV_BET_RAISE in legal_actions: return ENV_BET_RAISE\n",
                "        if ENV_CHECK_CALL in legal_actions: return ENV_CHECK_CALL\n",
                "        return ENV_FOLD\n",
                "class NitAgent:\n",
                "    def select_action(self, state, legal_actions):\n",
                "        if ENV_FOLD in legal_actions and ENV_CHECK_CALL in legal_actions:\n",
                "            if random.random() < 0.9: return ENV_FOLD\n",
                "        if ENV_CHECK_CALL in legal_actions: return ENV_CHECK_CALL\n",
                "        return ENV_FOLD\n",
                "class RandomAgent:\n",
                "    def select_action(self, state, legal_actions): return random.choice(legal_actions)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "id": "23edb5a2",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Training V8 for 50000 hands against full realistic opponent pool...\n",
                        "\n",
                        "=== Checkpoint Hand 2500 (Eps: 0.88) ===\n",
                        "--- Extreme Opponents ---\n",
                        "Maniac: Avg -10.44 BB | Win 11.7%\n",
                        "Nit: Avg 0.50 BB | Win 98.8%\n",
                        "Random: Avg -2.23 BB | Win 66.4%\n",
                        "--- Realistic Opponents ---\n",
                        "ValueBot: Avg -3.32 BB | Win 74.2%\n",
                        "BluffBot: Avg -4.68 BB | Win 67.5%\n",
                        "Balanced: Avg -4.46 BB | Win 66.3%\n",
                        "\n",
                        "=== Checkpoint Hand 5000 (Eps: 0.78) ===\n",
                        "--- Extreme Opponents ---\n",
                        "Maniac: Avg -14.62 BB | Win 9.5%\n",
                        "Nit: Avg 0.58 BB | Win 99.2%\n",
                        "Random: Avg 2.06 BB | Win 65.5%\n",
                        "--- Realistic Opponents ---\n",
                        "ValueBot: Avg -5.74 BB | Win 73.3%\n",
                        "BluffBot: Avg -0.81 BB | Win 67.4%\n",
                        "Balanced: Avg -2.36 BB | Win 66.5%\n",
                        "\n",
                        "=== Checkpoint Hand 7500 (Eps: 0.69) ===\n",
                        "--- Extreme Opponents ---\n",
                        "Maniac: Avg -8.07 BB | Win 8.5%\n",
                        "Nit: Avg 0.53 BB | Win 99.0%\n",
                        "Random: Avg -0.35 BB | Win 64.4%\n",
                        "--- Realistic Opponents ---\n",
                        "ValueBot: Avg -0.27 BB | Win 72.4%\n",
                        "BluffBot: Avg -2.23 BB | Win 64.9%\n",
                        "Balanced: Avg -2.78 BB | Win 64.5%\n",
                        "\n",
                        "=== Checkpoint Hand 10000 (Eps: 0.61) ===\n",
                        "--- Extreme Opponents ---\n",
                        "Maniac: Avg -5.32 BB | Win 8.0%\n",
                        "Nit: Avg 0.49 BB | Win 98.7%\n",
                        "Random: Avg -1.01 BB | Win 62.4%\n",
                        "--- Realistic Opponents ---\n",
                        "ValueBot: Avg -1.57 BB | Win 72.2%\n",
                        "BluffBot: Avg -2.38 BB | Win 63.7%\n",
                        "Balanced: Avg -1.25 BB | Win 63.4%\n",
                        "\n",
                        "=== Checkpoint Hand 12500 (Eps: 0.54) ===\n",
                        "--- Extreme Opponents ---\n",
                        "Maniac: Avg -6.72 BB | Win 7.4%\n",
                        "Nit: Avg 0.52 BB | Win 98.5%\n",
                        "Random: Avg -0.62 BB | Win 61.3%\n",
                        "--- Realistic Opponents ---\n",
                        "ValueBot: Avg -0.19 BB | Win 72.3%\n",
                        "BluffBot: Avg -4.42 BB | Win 63.0%\n",
                        "Balanced: Avg -0.52 BB | Win 62.6%\n",
                        "\n",
                        "=== Checkpoint Hand 15000 (Eps: 0.47) ===\n",
                        "--- Extreme Opponents ---\n",
                        "Maniac: Avg -8.66 BB | Win 6.7%\n",
                        "Nit: Avg 0.53 BB | Win 98.5%\n",
                        "Random: Avg -0.92 BB | Win 60.4%\n",
                        "--- Realistic Opponents ---\n",
                        "ValueBot: Avg -0.04 BB | Win 71.3%\n",
                        "BluffBot: Avg -2.78 BB | Win 62.8%\n",
                        "Balanced: Avg -2.27 BB | Win 61.9%\n",
                        "\n",
                        "=== Checkpoint Hand 17500 (Eps: 0.42) ===\n",
                        "--- Extreme Opponents ---\n",
                        "Maniac: Avg -3.88 BB | Win 6.1%\n",
                        "Nit: Avg 0.48 BB | Win 98.4%\n",
                        "Random: Avg -1.13 BB | Win 59.7%\n",
                        "--- Realistic Opponents ---\n",
                        "ValueBot: Avg -0.91 BB | Win 70.5%\n",
                        "BluffBot: Avg -0.94 BB | Win 62.2%\n",
                        "Balanced: Avg -2.15 BB | Win 61.9%\n",
                        "\n",
                        "=== Checkpoint Hand 20000 (Eps: 0.37) ===\n",
                        "--- Extreme Opponents ---\n",
                        "Maniac: Avg -5.04 BB | Win 5.5%\n",
                        "Nit: Avg 0.51 BB | Win 98.3%\n",
                        "Random: Avg -0.79 BB | Win 58.8%\n",
                        "--- Realistic Opponents ---\n",
                        "ValueBot: Avg -0.70 BB | Win 70.0%\n",
                        "BluffBot: Avg -2.45 BB | Win 62.0%\n",
                        "Balanced: Avg -2.14 BB | Win 61.3%\n",
                        "\n",
                        "=== Checkpoint Hand 22500 (Eps: 0.32) ===\n",
                        "--- Extreme Opponents ---\n",
                        "Maniac: Avg -4.64 BB | Win 5.1%\n",
                        "Nit: Avg 0.51 BB | Win 98.3%\n",
                        "Random: Avg -0.11 BB | Win 58.5%\n",
                        "--- Realistic Opponents ---\n",
                        "ValueBot: Avg -2.48 BB | Win 69.4%\n",
                        "BluffBot: Avg -0.51 BB | Win 61.6%\n",
                        "Balanced: Avg 0.03 BB | Win 61.0%\n",
                        "\n",
                        "=== Checkpoint Hand 25000 (Eps: 0.29) ===\n",
                        "--- Extreme Opponents ---\n",
                        "Maniac: Avg -2.65 BB | Win 4.7%\n",
                        "Nit: Avg 0.52 BB | Win 98.3%\n",
                        "Random: Avg -0.54 BB | Win 57.7%\n",
                        "--- Realistic Opponents ---\n",
                        "ValueBot: Avg -1.57 BB | Win 69.0%\n",
                        "BluffBot: Avg -1.05 BB | Win 61.2%\n",
                        "Balanced: Avg -0.61 BB | Win 60.7%\n",
                        "\n",
                        "=== Checkpoint Hand 27500 (Eps: 0.25) ===\n",
                        "--- Extreme Opponents ---\n",
                        "Maniac: Avg -2.69 BB | Win 4.4%\n",
                        "Nit: Avg 0.54 BB | Win 98.3%\n",
                        "Random: Avg -1.05 BB | Win 57.0%\n",
                        "--- Realistic Opponents ---\n",
                        "ValueBot: Avg -0.93 BB | Win 68.7%\n",
                        "BluffBot: Avg 0.00 BB | Win 61.1%\n",
                        "Balanced: Avg -1.49 BB | Win 60.1%\n",
                        "\n",
                        "=== Checkpoint Hand 30000 (Eps: 0.22) ===\n",
                        "--- Extreme Opponents ---\n",
                        "Maniac: Avg -2.42 BB | Win 4.1%\n",
                        "Nit: Avg 0.49 BB | Win 98.1%\n",
                        "Random: Avg -0.76 BB | Win 56.5%\n",
                        "--- Realistic Opponents ---\n",
                        "ValueBot: Avg -0.95 BB | Win 68.3%\n",
                        "BluffBot: Avg -2.30 BB | Win 60.8%\n",
                        "Balanced: Avg -0.57 BB | Win 59.8%\n",
                        "\n",
                        "=== Checkpoint Hand 32500 (Eps: 0.20) ===\n",
                        "--- Extreme Opponents ---\n",
                        "Maniac: Avg -3.25 BB | Win 3.9%\n",
                        "Nit: Avg 0.50 BB | Win 98.1%\n",
                        "Random: Avg -0.82 BB | Win 56.0%\n",
                        "--- Realistic Opponents ---\n",
                        "ValueBot: Avg -1.29 BB | Win 68.1%\n",
                        "BluffBot: Avg -0.76 BB | Win 60.1%\n",
                        "Balanced: Avg -0.36 BB | Win 59.2%\n",
                        "\n",
                        "=== Checkpoint Hand 35000 (Eps: 0.17) ===\n",
                        "--- Extreme Opponents ---\n",
                        "Maniac: Avg -2.50 BB | Win 3.8%\n",
                        "Nit: Avg 0.50 BB | Win 98.1%\n",
                        "Random: Avg -1.31 BB | Win 55.4%\n",
                        "--- Realistic Opponents ---\n",
                        "ValueBot: Avg -0.37 BB | Win 68.2%\n",
                        "BluffBot: Avg -0.86 BB | Win 59.9%\n",
                        "Balanced: Avg -1.06 BB | Win 58.7%\n",
                        "\n",
                        "=== Checkpoint Hand 37500 (Eps: 0.15) ===\n",
                        "--- Extreme Opponents ---\n",
                        "Maniac: Avg -2.71 BB | Win 3.5%\n",
                        "Nit: Avg 0.53 BB | Win 98.0%\n",
                        "Random: Avg -1.08 BB | Win 54.9%\n",
                        "--- Realistic Opponents ---\n",
                        "ValueBot: Avg -0.13 BB | Win 68.2%\n",
                        "BluffBot: Avg -1.05 BB | Win 59.4%\n",
                        "Balanced: Avg -0.39 BB | Win 58.3%\n",
                        "\n",
                        "=== Checkpoint Hand 40000 (Eps: 0.14) ===\n",
                        "--- Extreme Opponents ---\n",
                        "Maniac: Avg -1.58 BB | Win 3.4%\n",
                        "Nit: Avg 0.48 BB | Win 97.9%\n",
                        "Random: Avg -0.98 BB | Win 54.4%\n",
                        "--- Realistic Opponents ---\n",
                        "ValueBot: Avg -0.51 BB | Win 68.2%\n",
                        "BluffBot: Avg -0.73 BB | Win 59.3%\n",
                        "Balanced: Avg -0.73 BB | Win 58.1%\n",
                        "\n",
                        "=== Checkpoint Hand 42500 (Eps: 0.12) ===\n",
                        "--- Extreme Opponents ---\n",
                        "Maniac: Avg -1.95 BB | Win 3.2%\n",
                        "Nit: Avg 0.53 BB | Win 97.8%\n",
                        "Random: Avg -0.17 BB | Win 54.0%\n",
                        "--- Realistic Opponents ---\n",
                        "ValueBot: Avg -0.19 BB | Win 68.2%\n",
                        "BluffBot: Avg -0.55 BB | Win 58.7%\n",
                        "Balanced: Avg -1.03 BB | Win 57.7%\n",
                        "\n",
                        "=== Checkpoint Hand 45000 (Eps: 0.11) ===\n",
                        "--- Extreme Opponents ---\n",
                        "Maniac: Avg -2.21 BB | Win 3.2%\n",
                        "Nit: Avg 0.49 BB | Win 97.8%\n",
                        "Random: Avg -0.52 BB | Win 53.7%\n",
                        "--- Realistic Opponents ---\n",
                        "ValueBot: Avg -0.69 BB | Win 68.2%\n",
                        "BluffBot: Avg -0.90 BB | Win 58.4%\n",
                        "Balanced: Avg -0.60 BB | Win 57.2%\n"
                    ]
                },
                {
                    "ename": "KeyboardInterrupt",
                    "evalue": "",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
                        "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
                        "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 115\u001b[39m\n\u001b[32m    112\u001b[39m     plt.show()\n\u001b[32m    113\u001b[39m     plt.savefig(\u001b[33m\"\u001b[39m\u001b[33mv8_results.png\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m agent, stats = \u001b[43mtrain_v8\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m50000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    116\u001b[39m plot_v8_results(stats)\n",
                        "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 48\u001b[39m, in \u001b[36mtrain_v8\u001b[39m\u001b[34m(num_hands)\u001b[39m\n\u001b[32m     45\u001b[39m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     47\u001b[39m     \u001b[38;5;66;03m# Pass 'state' to opponent logic for hand evaluation\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m     action = \u001b[43mopponent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minfo\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlegal_actions\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     49\u001b[39m     env.update_opponent_history(action)\n\u001b[32m     50\u001b[39m     env._execute_action(action)\n",
                        "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 64\u001b[39m, in \u001b[36mValueBot.select_action\u001b[39m\u001b[34m(self, state, legal_actions)\u001b[39m\n\u001b[32m     63\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mselect_action\u001b[39m(\u001b[38;5;28mself\u001b[39m, state, legal_actions):\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m     equity = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_equity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     66\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m equity > \u001b[32m0.70\u001b[39m:\n\u001b[32m     67\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m ENV_BET_RAISE \u001b[38;5;129;01min\u001b[39;00m legal_actions: \u001b[38;5;28;01mreturn\u001b[39;00m ENV_BET_RAISE\n",
                        "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 56\u001b[39m, in \u001b[36mHeuristicBot.get_equity\u001b[39m\u001b[34m(self, state)\u001b[39m\n\u001b[32m     54\u001b[39m hole = flatten_cards_list(state.hole_cards[\u001b[38;5;28mself\u001b[39m.player_idx])\n\u001b[32m     55\u001b[39m board = flatten_cards_list(state.board_cards)\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmonte_carlo_equity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhole\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mboard\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterations\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m40\u001b[39;49m\u001b[43m)\u001b[49m\n",
                        "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 25\u001b[39m, in \u001b[36mmonte_carlo_equity\u001b[39m\u001b[34m(hole_cards, board_cards, iterations)\u001b[39m\n\u001b[32m     21\u001b[39m opp_total = opp_hole + sim_board\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# FIX: StandardHighHand takes exactly 5 cards. \u001b[39;00m\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# We must find the best 5-card subset from the 7 available cards.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m my_hand = \u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mStandardHighHand\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcombinations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmy_total\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m opp_hand = \u001b[38;5;28mmax\u001b[39m(StandardHighHand(c) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m combinations(opp_total, \u001b[32m5\u001b[39m))\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m my_hand > opp_hand:\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/functools.py:91\u001b[39m, in \u001b[36m_gt_from_lt\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m     89\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_gt_from_lt\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[32m     90\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mReturn a > b.  Computed by @total_ordering from (not a < b) and (a != b).\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m     op_result = \u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__lt__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     92\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m op_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[32m     93\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m op_result\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/repos/poker-bot-test/.venv/lib/python3.12/site-packages/pokerkit/hands.py:148\u001b[39m, in \u001b[36mHand.__lt__\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m    146\u001b[39m     ordering = \u001b[38;5;28mself\u001b[39m.entry > other.entry\n\u001b[32m    147\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m148\u001b[39m     ordering = \u001b[38;5;28mself\u001b[39m.entry < \u001b[43mother\u001b[49m\u001b[43m.\u001b[49m\u001b[43mentry\u001b[49m\n\u001b[32m    150\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m ordering\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/repos/poker-bot-test/.venv/lib/python3.12/site-packages/pokerkit/hands.py:184\u001b[39m, in \u001b[36mHand.entry\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    172\u001b[39m \u001b[38;5;129m@property\u001b[39m\n\u001b[32m    173\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mentry\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> Entry:\n\u001b[32m    174\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return the hand entry.\u001b[39;00m\n\u001b[32m    175\u001b[39m \n\u001b[32m    176\u001b[39m \u001b[33;03m    >>> hole = 'AsAc'\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    182\u001b[39m \u001b[33;03m    :return: The hand entry.\u001b[39;00m\n\u001b[32m    183\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m184\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlookup\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_entry\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcards\u001b[49m\u001b[43m)\u001b[49m\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/repos/poker-bot-test/.venv/lib/python3.12/site-packages/pokerkit/lookups.py:222\u001b[39m, in \u001b[36mLookup.get_entry\u001b[39m\u001b[34m(self, cards)\u001b[39m\n\u001b[32m    203\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_entry\u001b[39m(\u001b[38;5;28mself\u001b[39m, cards: CardsLike) -> Entry:\n\u001b[32m    204\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return the corresponding lookup entry of the hand that the\u001b[39;00m\n\u001b[32m    205\u001b[39m \u001b[33;03m    cards form.\u001b[39;00m\n\u001b[32m    206\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    220\u001b[39m \u001b[33;03m    :raises ValueError: If cards do not form a valid hand.\u001b[39;00m\n\u001b[32m    221\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m222\u001b[39m     key = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_key\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcards\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    224\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.__entries:\n\u001b[32m    225\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mThe cards \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(cards)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m form an invalid hand.\u001b[39m\u001b[33m'\u001b[39m)\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/repos/poker-bot-test/.venv/lib/python3.12/site-packages/pokerkit/lookups.py:249\u001b[39m, in \u001b[36mLookup._get_key\u001b[39m\u001b[34m(self, cards)\u001b[39m\n\u001b[32m    247\u001b[39m cards = Card.clean(cards)\n\u001b[32m    248\u001b[39m hash_ = \u001b[38;5;28mself\u001b[39m.__hash(Card.get_ranks(cards))\n\u001b[32m--> \u001b[39m\u001b[32m249\u001b[39m suitedness = \u001b[43mCard\u001b[49m\u001b[43m.\u001b[49m\u001b[43mare_suited\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcards\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m hash_, suitedness\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/repos/poker-bot-test/.venv/lib/python3.12/site-packages/pokerkit/utilities.py:361\u001b[39m, in \u001b[36mCard.are_suited\u001b[39m\u001b[34m(cls, cards)\u001b[39m\n\u001b[32m    343\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m    344\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mare_suited\u001b[39m(\u001b[38;5;28mcls\u001b[39m, cards: CardsLike) -> \u001b[38;5;28mbool\u001b[39m:\n\u001b[32m    345\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return the suitedness of the given cards.\u001b[39;00m\n\u001b[32m    346\u001b[39m \n\u001b[32m    347\u001b[39m \u001b[33;03m    The cards are suited if all of the cards share a common suit.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    359\u001b[39m \u001b[33;03m    :return: The suitedness of the cards.\u001b[39;00m\n\u001b[32m    360\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m361\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28;43mset\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_suits\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcards\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m) <= \u001b[32m1\u001b[39m\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/repos/poker-bot-test/.venv/lib/python3.12/site-packages/pokerkit/utilities.py:317\u001b[39m, in \u001b[36mCard.get_suits\u001b[39m\u001b[34m(cls, cards)\u001b[39m\n\u001b[32m    305\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m    306\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_suits\u001b[39m(\u001b[38;5;28mcls\u001b[39m, cards: CardsLike) -> Iterator[Suit]:\n\u001b[32m    307\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return an iterator of the suits of each card.\u001b[39;00m\n\u001b[32m    308\u001b[39m \n\u001b[32m    309\u001b[39m \u001b[33;03m    >>> Card.get_suits('2sKh')  # doctest: +ELLIPSIS\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    315\u001b[39m \u001b[33;03m    :return: The iterator of the suits of each card.\u001b[39;00m\n\u001b[32m    316\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m317\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m card \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcards\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    318\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m card.suit\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/repos/poker-bot-test/.venv/lib/python3.12/site-packages/pokerkit/utilities.py:385\u001b[39m, in \u001b[36mCard.clean\u001b[39m\u001b[34m(cls, values)\u001b[39m\n\u001b[32m    381\u001b[39m     suits = \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mcls\u001b[39m.get_suits(cards))\n\u001b[32m    383\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m(suits)) == \u001b[38;5;28mlen\u001b[39m(suits)\n\u001b[32m--> \u001b[39m\u001b[32m385\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m    386\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mclean\u001b[39m(\u001b[38;5;28mcls\u001b[39m, values: CardsLike) -> \u001b[38;5;28mtuple\u001b[39m[Card, ...]:\n\u001b[32m    387\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Clean the cards.\u001b[39;00m\n\u001b[32m    388\u001b[39m \n\u001b[32m    389\u001b[39m \u001b[33;03m    This method \"cleans\" any cards-like object (e.g., raw string\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    405\u001b[39m \u001b[33;03m    :return: The cleaned cards.\u001b[39;00m\n\u001b[32m    406\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m    407\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(values, Card):\n",
                        "\u001b[31mKeyboardInterrupt\u001b[39m: "
                    ]
                }
            ],
            "source": [
                "def train_v8(num_hands=50000):\n",
                "    env = PersistentPokerEnv()\n",
                "    agent = AdaptiveAgentV8(env.game_state_dim)\n",
                "    buffer = ReplayBufferV8(capacity=50000)\n",
                "    \n",
                "    # Expanded Opponent Pool\n",
                "    opps = {\n",
                "        'Maniac': ManiacAgent(),\n",
                "        'Nit': NitAgent(),\n",
                "        'Random': RandomAgent(),\n",
                "        'ValueBot': ValueBot(),\n",
                "        'BluffBot': BluffBot(),\n",
                "        'Balanced': BalancedBot()\n",
                "    }\n",
                "    opp_names = list(opps.keys())\n",
                "    \n",
                "    stats = {name: {'rewards': [], 'wins': 0, 'hands': 0} for name in opp_names}\n",
                "    \n",
                "    print(f\"Training V8 for {num_hands} hands against full realistic opponent pool...\")\n",
                "    current_opp_name = 'Random'\n",
                "    \n",
                "    for hand in range(num_hands):\n",
                "        if hand % 20 == 0:\n",
                "            current_opp_name = random.choice(opp_names)\n",
                "            \n",
                "        opponent = opps[current_opp_name]\n",
                "        \n",
                "        obs, info = env.reset()\n",
                "        done = False\n",
                "        episode_transitions = []\n",
                "        pending_agent_obs = None\n",
                "        pending_agent_action = None\n",
                "        \n",
                "        while not done:\n",
                "            if env.state.actor_index == env.agent_player_index:\n",
                "                if pending_agent_obs is not None:\n",
                "                    episode_transitions.append((pending_agent_obs['game_state'], pending_agent_obs['history'], pending_agent_action, 0.0, obs['game_state'], obs['history'], False, info['legal_actions']))\n",
                "                \n",
                "                action = agent.select_action(obs, info['legal_actions'])\n",
                "                pending_agent_obs = obs\n",
                "                pending_agent_action = action\n",
                "                obs, reward, done, _, info = env.step(action)\n",
                "                \n",
                "                if done:\n",
                "                    pass\n",
                "            else:\n",
                "                # Pass 'state' to opponent logic for hand evaluation\n",
                "                action = opponent.select_action(env.state, info['legal_actions'])\n",
                "                env.update_opponent_history(action)\n",
                "                env._execute_action(action)\n",
                "                env._run_automations()\n",
                "                done = env.state.status is False\n",
                "                if not done:\n",
                "                    obs = env._get_observation()\n",
                "                    info['legal_actions'] = env._get_legal_actions()\n",
                "        \n",
                "        final_reward = env.get_final_reward()\n",
                "        env.append_outcome_token(final_reward)\n",
                "        term_obs = env._get_observation()\n",
                "        \n",
                "        if pending_agent_obs is not None:\n",
                "             episode_transitions.append((pending_agent_obs['game_state'], pending_agent_obs['history'], pending_agent_action, 0.0, term_obs['game_state'], term_obs['history'], True, []))\n",
                "        \n",
                "        stats[current_opp_name]['rewards'].append(final_reward)\n",
                "        stats[current_opp_name]['hands'] += 1\n",
                "        if final_reward > 0: stats[current_opp_name]['wins'] += 1\n",
                "        \n",
                "        for i in range(len(episode_transitions)):\n",
                "            s, h, a, r, ns, nh, d, l = episode_transitions[i]\n",
                "            buffer.push((s, h, a, final_reward, ns, nh, d, l))\n",
                "            \n",
                "        if len(buffer) > 1000:\n",
                "            agent.train(buffer)\n",
                "        agent.epsilon = max(agent.epsilon_min, agent.epsilon * agent.epsilon_decay)\n",
                "        if hand % 500 == 0: agent.update_target()\n",
                "        \n",
                "        if hand % 2500 == 0 and hand > 0:\n",
                "            print(f\"\\n=== Checkpoint Hand {hand} (Eps: {agent.epsilon:.2f}) ===\")\n",
                "            # Split output for readability\n",
                "            print(\"--- Extreme Opponents ---\")\n",
                "            for name in ['Maniac', 'Nit', 'Random']:\n",
                "                 if stats[name]['hands'] > 0:\n",
                "                     print(f\"{name}: Avg {np.mean(stats[name]['rewards'][-200:]):.2f} BB | Win {stats[name]['wins']/stats[name]['hands']:.1%}\")\n",
                "            print(\"--- Realistic Opponents ---\")\n",
                "            for name in ['ValueBot', 'BluffBot', 'Balanced']:\n",
                "                 if stats[name]['hands'] > 0:\n",
                "                     print(f\"{name}: Avg {np.mean(stats[name]['rewards'][-200:]):.2f} BB | Win {stats[name]['wins']/stats[name]['hands']:.1%}\")\n",
                "\n",
                "    return agent, stats\n",
                "\n",
                "def plot_v8_results(stats):\n",
                "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
                "    \n",
                "    # Ax1: Extreme\n",
                "    for name in ['Maniac', 'Nit', 'Random']:\n",
                "        data = stats[name]['rewards']\n",
                "        if len(data) > 100:\n",
                "            ma = np.convolve(data, np.ones(100)/100, mode='valid')\n",
                "            ax1.plot(ma, label=name)\n",
                "    ax1.set_title(\"Performance vs Extreme Bots\")\n",
                "    ax1.legend()\n",
                "    \n",
                "    # Ax2: Realistic\n",
                "    for name in ['ValueBot', 'BluffBot', 'Balanced']:\n",
                "        data = stats[name]['rewards']\n",
                "        if len(data) > 100:\n",
                "            ma = np.convolve(data, np.ones(100)/100, mode='valid')\n",
                "            ax2.plot(ma, label=name)\n",
                "    ax2.set_title(\"Performance vs Realistic Bots\")\n",
                "    ax2.legend()\n",
                "    \n",
                "    plt.show()\n",
                "    plt.savefig(\"v8_results.png\")\n",
                "\n",
                "agent, stats = train_v8(50000)\n",
                "plot_v8_results(stats)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
