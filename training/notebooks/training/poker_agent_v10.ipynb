{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Poker Agent V10: Context-Aware Memory & Targeted Training\n",
                "\n",
                "This notebook refines the V9 Equity model by introducing **Context Switching** and **Dynamic Parameters** to solve the \"Learning Plateau\" problem.\n",
                "\n",
                "### The Problem with V9\n",
                "V9 reached break-even (-0.75 BB) but plateaued. Why?\n",
                "1. **Memory Contamination**: When switching from Maniac to Nit, the LSTM still had \"Maniac Actions\" in its history buffer for the first ~15 hands. This confused the agent.\n",
                "2. **Noise**: Training against extreme bots (Maniac/Nit) distracted the network from learning nuanced strategy against realistic bots.\n",
                "\n",
                "### V10 Solution: Opponent-Specific History\n",
                "We now simulate a \"Known Player Database\". The environment maintains **separate history buffers** for each opponent ID.\n",
                "- When we switch to `ValueBot`, we load the strict `ValueBot` history.\n",
                "- When we switch to `BluffBot`, we load the `BluffBot` history.\n",
                "\n",
                "This gives the LSTM a **clean, continuous signal** for every opponent, maximizing adaptation speed.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "id": "dafda2b2",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Using device: cuda\n"
                    ]
                }
            ],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "import torch.nn.functional as F\n",
                "import numpy as np\n",
                "import gymnasium as gym\n",
                "from gymnasium import spaces\n",
                "from collections import deque\n",
                "import random\n",
                "import matplotlib.pyplot as plt\n",
                "from typing import Optional, Tuple, List, Dict, Any\n",
                "import copy\n",
                "from itertools import combinations\n",
                "\n",
                "from pokerkit import Automation, NoLimitTexasHoldem, Card, StandardHighHand, Deck\n",
                "\n",
                "# Constants\n",
                "SEED = 42\n",
                "MAX_HISTORY_LEN = 100\n",
                "ACTION_EMBED_DIM = 16\n",
                "HIDDEN_DIM_LSTM = 128\n",
                "\n",
                "# Actions\n",
                "ENV_FOLD = 0\n",
                "ENV_CHECK_CALL = 1\n",
                "ENV_BET_RAISE = 2\n",
                "NUM_ACTIONS = 3\n",
                "\n",
                "# History Tokens\n",
                "ACT_PAD = 0\n",
                "ACT_V_FOLD = 1\n",
                "ACT_V_CHECK_CALL = 2\n",
                "ACT_V_BET_RAISE = 3\n",
                "OPP_FOLD = 4\n",
                "OPP_CHECK_CALL = 5\n",
                "OPP_BET_RAISE = 6\n",
                "OUT_AGENT_WIN = 7\n",
                "OUT_AGENT_LOSS = 8\n",
                "OUT_TIE = 9\n",
                "OUT_NEW_HAND = 10\n",
                "HISTORY_VOCAB_SIZE = 11\n",
                "\n",
                "random.seed(SEED)\n",
                "np.random.seed(SEED)\n",
                "torch.manual_seed(SEED)\n",
                "\n",
                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "print(f\"Using device: {device}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "id": "d56fce56",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Universal Helpers \n",
                "def flatten_cards_list(items):\n",
                "    out = []\n",
                "    if isinstance(items, Card): return [items]\n",
                "    for x in items:\n",
                "        if isinstance(x, (list, tuple)): out.extend(flatten_cards_list(x))\n",
                "        else: out.append(x)\n",
                "    return out\n",
                "\n",
                "def monte_carlo_equity(hole_cards: List[Card], board_cards: List[Card], iterations=30) -> float:\n",
                "    if not hole_cards: return 0.5 \n",
                "    wins = 0\n",
                "    hole_cards = flatten_cards_list(hole_cards)\n",
                "    board_cards = flatten_cards_list(board_cards)\n",
                "    known_cards = set(hole_cards + board_cards)\n",
                "    \n",
                "    for _ in range(iterations):\n",
                "        deck_cards = [c for c in Deck.STANDARD if c not in known_cards]\n",
                "        random.shuffle(deck_cards)\n",
                "        opp_hole = deck_cards[:2]\n",
                "        needed_board = 5 - len(board_cards)\n",
                "        sim_board = board_cards + deck_cards[2:2+needed_board]\n",
                "        my_total = hole_cards + sim_board\n",
                "        opp_total = opp_hole + sim_board\n",
                "        my_hand = max(StandardHighHand(c) for c in combinations(my_total, 5))\n",
                "        opp_hand = max(StandardHighHand(c) for c in combinations(opp_total, 5))\n",
                "        if my_hand > opp_hand: wins += 1\n",
                "        elif my_hand == opp_hand: wins += 0.5\n",
                "    return wins / iterations"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "id": "c43a18f2",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Reusing V9 Env Logic but adding Memory Swapping\n",
                "class ContextAwarePokerEnv(gym.Env):\n",
                "    def __init__(self, num_players: int = 2, starting_stack: int = 1000, \n",
                "                 small_blind: int = 5, big_blind: int = 10):\n",
                "        super().__init__()\n",
                "        self.num_players = num_players\n",
                "        self.starting_stack = starting_stack\n",
                "        self.small_blind = small_blind\n",
                "        self.big_blind = big_blind\n",
                "        self.base_state_dim = 52*2 + 52*5 + num_players + 1 + 1 + 4 + 1\n",
                "        self.game_state_dim = self.base_state_dim + 3 \n",
                "        self.observation_space = spaces.Dict({\n",
                "            'game_state': spaces.Box(low=0, high=1, shape=(self.game_state_dim,), dtype=np.float32),\n",
                "            'history': spaces.Box(low=0, high=HISTORY_VOCAB_SIZE-1, shape=(MAX_HISTORY_LEN,), dtype=np.int64)\n",
                "        })\n",
                "        self.action_space = spaces.Discrete(NUM_ACTIONS)\n",
                "        self.state = None\n",
                "        self.agent_player_index = 0\n",
                "        self.global_history = deque(maxlen=MAX_HISTORY_LEN)\n",
                "        for _ in range(MAX_HISTORY_LEN): self.global_history.append(ACT_PAD)\n",
                "        \n",
                "        # --- V10: History Banks ---\n",
                "        self.history_bank = {}\n",
                "        self.current_opp_id = \"default\"\n",
                "        \n",
                "    def load_opponent_history(self, opp_id: str):\n",
                "        # Save current history to previous opp bank\n",
                "        if self.current_opp_id:\n",
                "            self.history_bank[self.current_opp_id] = copy.deepcopy(self.global_history)\n",
                "            \n",
                "        # Load or Init new history\n",
                "        self.current_opp_id = opp_id\n",
                "        if opp_id in self.history_bank:\n",
                "            self.global_history = copy.deepcopy(self.history_bank[opp_id])\n",
                "        else:\n",
                "            self.global_history = deque(maxlen=MAX_HISTORY_LEN)\n",
                "            for _ in range(MAX_HISTORY_LEN): self.global_history.append(ACT_PAD)\n",
                "            \n",
                "    def _card_to_index(self, card: Card) -> int:\n",
                "        ranks = '23456789TJQKA'; suits = 'cdhs'\n",
                "        return ranks.index(card.rank) * 4 + suits.index(card.suit)\n",
                "    \n",
                "    def _encode_card(self, card: Optional[Card]) -> np.ndarray:\n",
                "        encoding = np.zeros(52, dtype=np.float32)\n",
                "        if card is not None: encoding[self._card_to_index(card)] = 1.0\n",
                "        return encoding\n",
                "    \n",
                "    def _get_observation(self) -> Dict[str, Any]:\n",
                "        hole = flatten_cards_list(self.state.hole_cards[self.agent_player_index])\n",
                "        board = flatten_cards_list(self.state.board_cards)\n",
                "        equity = monte_carlo_equity(hole, board, iterations=25)\n",
                "        total_pot = sum(self.state.bets)\n",
                "        current_bet = max(self.state.bets)\n",
                "        my_bet = self.state.bets[self.agent_player_index]\n",
                "        to_call = current_bet - my_bet\n",
                "        pot_odds = 0.0\n",
                "        if (total_pot + to_call) > 0: pot_odds = to_call / (total_pot + to_call)\n",
                "        spr = 0.0\n",
                "        if total_pot > 0: spr = min((self.state.stacks[self.agent_player_index] / total_pot) / 20.0, 1.0)\n",
                "        \n",
                "        state_vector = [equity, pot_odds, spr]\n",
                "        for i in range(2):\n",
                "            if i < len(hole): state_vector.extend(self._encode_card(hole[i]))\n",
                "            else: state_vector.extend(np.zeros(52, dtype=np.float32))\n",
                "        for i in range(5):\n",
                "            if i < len(board): state_vector.extend(self._encode_card(board[i]))\n",
                "            else: state_vector.extend(np.zeros(52, dtype=np.float32))\n",
                "        for i in range(self.num_players):\n",
                "            state_vector.append(min(self.state.stacks[i] / self.starting_stack, 2.0))\n",
                "        state_vector.append(total_pot / (self.starting_stack * self.num_players))\n",
                "        state_vector.append(self.state.actor_index / max(1, self.num_players - 1) if self.state.actor_index is not None else 0.0)\n",
                "        street = [0.0]*4\n",
                "        if len(board) == 0: street[0] = 1.0\n",
                "        elif len(board) == 3: street[1] = 1.0\n",
                "        elif len(board) == 4: street[2] = 1.0\n",
                "        else: street[3] = 1.0\n",
                "        state_vector.extend(street)\n",
                "        state_vector.append(float(self.agent_player_index))\n",
                "        return {'game_state': np.array(state_vector, dtype=np.float32), 'history': np.array(list(self.global_history), dtype=np.int64)}\n",
                "    \n",
                "    def _update_history(self, player_idx: int, action: int):\n",
                "        if player_idx == self.agent_player_index:\n",
                "            token = [ACT_V_FOLD, ACT_V_CHECK_CALL, ACT_V_BET_RAISE][action]\n",
                "        else:\n",
                "            token = [OPP_FOLD, OPP_CHECK_CALL, OPP_BET_RAISE][action]\n",
                "        self.global_history.append(token)\n",
                "\n",
                "    def append_outcome_token(self, final_reward: float):\n",
                "        if final_reward > 0: self.global_history.append(OUT_AGENT_WIN)\n",
                "        elif final_reward < 0: self.global_history.append(OUT_AGENT_LOSS)\n",
                "        else: self.global_history.append(OUT_TIE)\n",
                "\n",
                "    def _get_legal_actions(self) -> List[int]:\n",
                "        legal = []\n",
                "        if self.state.can_fold(): legal.append(ENV_FOLD)\n",
                "        if self.state.can_check_or_call(): legal.append(ENV_CHECK_CALL)\n",
                "        if self.state.can_complete_bet_or_raise_to(): legal.append(ENV_BET_RAISE)\n",
                "        return legal if legal else [ENV_CHECK_CALL]\n",
                "    \n",
                "    def _execute_action(self, action: int) -> None:\n",
                "        if action == ENV_FOLD:\n",
                "            if self.state.can_fold(): self.state.fold()\n",
                "            elif self.state.can_check_or_call(): self.state.check_or_call()\n",
                "        elif action == ENV_CHECK_CALL:\n",
                "            if self.state.can_check_or_call(): self.state.check_or_call()\n",
                "            elif self.state.can_fold(): self.state.fold()\n",
                "        elif action == ENV_BET_RAISE:\n",
                "            if self.state.can_complete_bet_or_raise_to():\n",
                "                min_r = self.state.min_completion_betting_or_raising_to_amount\n",
                "                max_r = self.state.max_completion_betting_or_raising_to_amount\n",
                "                self.state.complete_bet_or_raise_to(min(min_r * 2, max_r))\n",
                "            elif self.state.can_check_or_call():\n",
                "                self.state.check_or_call()\n",
                "    \n",
                "    def _run_automations(self) -> None:\n",
                "        while self.state.can_burn_card(): self.state.burn_card('??')\n",
                "        while self.state.can_deal_board(): self.state.deal_board()\n",
                "        while self.state.can_push_chips(): self.state.push_chips()\n",
                "        while self.state.can_pull_chips(): self.state.pull_chips()\n",
                "    \n",
                "    def reset(self, seed=None, options=None) -> Tuple[Dict, Dict]:\n",
                "        self.global_history.append(OUT_NEW_HAND)\n",
                "        self.state = NoLimitTexasHoldem.create_state(\n",
                "            automations=(Automation.ANTE_POSTING, Automation.BET_COLLECTION, Automation.BLIND_OR_STRADDLE_POSTING, Automation.HOLE_CARDS_SHOWING_OR_MUCKING, Automation.HAND_KILLING, Automation.CHIPS_PUSHING, Automation.CHIPS_PULLING),\n",
                "            ante_trimming_status=True, raw_antes={-1: 0}, raw_blinds_or_straddles=(self.small_blind, self.big_blind),\n",
                "            min_bet=self.big_blind, raw_starting_stacks=[self.starting_stack] * self.num_players, player_count=self.num_players)\n",
                "        while self.state.can_deal_hole(): self.state.deal_hole()\n",
                "        self._run_automations()\n",
                "        return self._get_observation(), {'legal_actions': self._get_legal_actions()}\n",
                "    \n",
                "    def step(self, action: int) -> Tuple[Dict, float, bool, bool, Dict]:\n",
                "        if self.state.actor_index is not None: self._update_history(self.state.actor_index, action)\n",
                "        self._execute_action(action)\n",
                "        self._run_automations()\n",
                "        done = self.state.status is False\n",
                "        reward = (self.state.stacks[self.agent_player_index] - self.starting_stack) / self.big_blind if done else 0.0\n",
                "        return self._get_observation(), reward, done, False, {'legal_actions': self._get_legal_actions() if not done else []}\n",
                "    \n",
                "    def get_final_reward(self) -> float:\n",
                "        return (self.state.stacks[self.agent_player_index] - self.starting_stack) / self.big_blind\n",
                "    \n",
                "    def update_opponent_history(self, action: int):\n",
                "        self._update_history(1 - self.agent_player_index, action)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "id": "4808ce70",
            "metadata": {},
            "outputs": [],
            "source": [
                "class FeatureAwareV10(nn.Module):\n",
                "    def __init__(self, state_dim: int, action_dim: int):\n",
                "        super().__init__()\n",
                "        self.state_net = nn.Sequential(\n",
                "            nn.Linear(state_dim, 256), nn.LayerNorm(256), nn.ReLU(),\n",
                "            nn.Linear(256, 128), nn.ReLU()\n",
                "        )\n",
                "        self.action_embedding = nn.Embedding(HISTORY_VOCAB_SIZE, ACTION_EMBED_DIM)\n",
                "        self.lstm = nn.LSTM(input_size=ACTION_EMBED_DIM, hidden_size=HIDDEN_DIM_LSTM, batch_first=True)\n",
                "        self.value_head = nn.Sequential(\n",
                "            nn.Linear(128 + HIDDEN_DIM_LSTM, 256), nn.ReLU(),\n",
                "            nn.Linear(256, action_dim)\n",
                "        )\n",
                "    def forward(self, state, history):\n",
                "        s_feat = self.state_net(state)\n",
                "        h_embed = self.action_embedding(history)\n",
                "        lstm_out, _ = self.lstm(h_embed)\n",
                "        combined = torch.cat([s_feat, lstm_out[:, -1, :]], dim=1)\n",
                "        return self.value_head(combined)\n",
                "\n",
                "class ReplayBufferV10:\n",
                "    def __init__(self, capacity=50000):\n",
                "        self.buffer = deque(maxlen=capacity)\n",
                "    def push(self, transition): self.buffer.append(transition)\n",
                "    def sample(self, batch_size): return random.sample(self.buffer, min(len(self.buffer), batch_size))\n",
                "    def __len__(self): return len(self.buffer)\n",
                "\n",
                "class HybridAgentV10:\n",
                "    def __init__(self, state_dim, action_dim=NUM_ACTIONS, lr=1e-4):\n",
                "        self.model = FeatureAwareV10(state_dim, action_dim).to(device)\n",
                "        self.target_model = FeatureAwareV10(state_dim, action_dim).to(device)\n",
                "        self.target_model.load_state_dict(self.model.state_dict())\n",
                "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
                "        self.gamma, self.epsilon, self.epsilon_min = 0.99, 1.0, 0.05\n",
                "        \n",
                "    def select_action(self, obs, legal_actions, eval_mode=False):\n",
                "        if not eval_mode and random.random() < self.epsilon:\n",
                "            return random.choice(legal_actions)\n",
                "        state_t = torch.FloatTensor(obs['game_state']).unsqueeze(0).to(device)\n",
                "        h_t = torch.LongTensor(obs['history']).unsqueeze(0).to(device)\n",
                "        with torch.no_grad():\n",
                "            q_values = self.model(state_t, h_t)\n",
                "        q_numpy = q_values.cpu().numpy().flatten()\n",
                "        masked_q = np.full(NUM_ACTIONS, -np.inf)\n",
                "        for a in legal_actions: masked_q[a] = q_numpy[a]\n",
                "        return int(np.argmax(masked_q))\n",
                "\n",
                "    def train(self, buffer, batch_size=64):\n",
                "        if len(buffer) < batch_size: return None\n",
                "        batch = buffer.sample(batch_size)\n",
                "        states = torch.FloatTensor(np.array([t[0] for t in batch])).to(device)\n",
                "        histories = torch.LongTensor(np.array([t[1] for t in batch])).to(device)\n",
                "        actions = torch.LongTensor(np.array([t[2] for t in batch])).to(device)\n",
                "        rewards = torch.FloatTensor(np.array([t[3] for t in batch])).to(device)\n",
                "        next_states = torch.FloatTensor(np.array([t[4] for t in batch])).to(device)\n",
                "        next_histories = torch.LongTensor(np.array([t[5] for t in batch])).to(device)\n",
                "        dones = torch.FloatTensor(np.array([t[6] for t in batch])).to(device)\n",
                "        current_q = self.model(states, histories).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
                "        with torch.no_grad():\n",
                "            next_actions = self.model(next_states, next_histories).argmax(1).unsqueeze(1)\n",
                "            target_q_next = self.target_model(next_states, next_histories).gather(1, next_actions).squeeze(1)\n",
                "            target = rewards + (1 - dones) * self.gamma * target_q_next\n",
                "        loss = F.mse_loss(current_q, target)\n",
                "        self.optimizer.zero_grad()\n",
                "        loss.backward()\n",
                "        torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
                "        self.optimizer.step()\n",
                "        return loss.item()\n",
                "    def update_target(self): self.target_model.load_state_dict(self.model.state_dict())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "id": "845c10ba",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Realistic Bots (Value, Bluff, Balanced) ONLY\n",
                "class HeuristicBot:\n",
                "    def __init__(self, player_idx=1): self.player_idx = player_idx\n",
                "    def get_equity(self, state):\n",
                "        return monte_carlo_equity(flatten_cards_list(state.hole_cards[self.player_idx]), flatten_cards_list(state.board_cards), iterations=40)\n",
                "\n",
                "class ValueBot(HeuristicBot):\n",
                "    def select_action(self, state, legal_actions):\n",
                "        equity = self.get_equity(state)\n",
                "        if equity > 0.75 and ENV_BET_RAISE in legal_actions: return ENV_BET_RAISE\n",
                "        if equity > 0.50 and ENV_CHECK_CALL in legal_actions: return ENV_CHECK_CALL\n",
                "        if ENV_FOLD in legal_actions: return ENV_FOLD\n",
                "        return ENV_CHECK_CALL\n",
                "\n",
                "class BluffBot(HeuristicBot):\n",
                "    def select_action(self, state, legal_actions):\n",
                "        equity = self.get_equity(state)\n",
                "        if equity > 0.70 and ENV_BET_RAISE in legal_actions: return ENV_BET_RAISE\n",
                "        if equity < 0.40 and random.random() < 0.35 and ENV_BET_RAISE in legal_actions: return ENV_BET_RAISE\n",
                "        if equity > 0.45 and ENV_CHECK_CALL in legal_actions: return ENV_CHECK_CALL\n",
                "        if ENV_FOLD in legal_actions: return ENV_FOLD\n",
                "        return ENV_CHECK_CALL\n",
                "\n",
                "class BalancedBot(HeuristicBot):\n",
                "    def select_action(self, state, legal_actions):\n",
                "        equity = self.get_equity(state)\n",
                "        if equity > 0.8 and ENV_BET_RAISE in legal_actions: return ENV_BET_RAISE\n",
                "        if ENV_CHECK_CALL in legal_actions:\n",
                "            if equity > 0.6: return ENV_CHECK_CALL\n",
                "            to_call = max(state.bets) - state.bets[self.player_idx]\n",
                "            pot_odds = to_call / (sum(state.bets) + to_call + 1e-5)\n",
                "            if equity > pot_odds + 0.05: return ENV_CHECK_CALL # +5% edge required\n",
                "        if ENV_FOLD in legal_actions: return ENV_FOLD\n",
                "        return ENV_CHECK_CALL"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "id": "ec92bf20",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Training V10 (Targeted) for 25000 hands...\n",
                        "Dynamic Epsilon Decay Factor: 0.999850\n",
                        "\n",
                        "=== Checkpoint Hand 2500 (Eps: 0.69) ===\n",
                        "ValueBot: Avg -0.14 BB | Win 82.2%\n",
                        "BluffBot: Avg -4.81 BB | Win 61.9%\n",
                        "Balanced: Avg -2.22 BB | Win 70.5%\n",
                        "\n",
                        "=== Checkpoint Hand 5000 (Eps: 0.47) ===\n",
                        "ValueBot: Avg 0.52 BB | Win 79.3%\n",
                        "BluffBot: Avg -2.32 BB | Win 61.1%\n",
                        "Balanced: Avg -1.99 BB | Win 68.5%\n",
                        "\n",
                        "=== Checkpoint Hand 7500 (Eps: 0.33) ===\n",
                        "ValueBot: Avg -0.49 BB | Win 79.3%\n",
                        "BluffBot: Avg -1.89 BB | Win 60.3%\n",
                        "Balanced: Avg -0.53 BB | Win 65.0%\n",
                        "\n",
                        "=== Checkpoint Hand 10000 (Eps: 0.22) ===\n",
                        "ValueBot: Avg -0.03 BB | Win 77.9%\n",
                        "BluffBot: Avg -1.00 BB | Win 59.4%\n",
                        "Balanced: Avg -1.14 BB | Win 62.7%\n",
                        "\n",
                        "=== Checkpoint Hand 12500 (Eps: 0.15) ===\n",
                        "ValueBot: Avg -0.00 BB | Win 77.3%\n",
                        "BluffBot: Avg -0.60 BB | Win 58.0%\n",
                        "Balanced: Avg -0.30 BB | Win 61.3%\n",
                        "\n",
                        "=== Checkpoint Hand 15000 (Eps: 0.11) ===\n",
                        "ValueBot: Avg -0.59 BB | Win 76.6%\n",
                        "BluffBot: Avg -2.47 BB | Win 57.4%\n",
                        "Balanced: Avg 0.11 BB | Win 60.4%\n",
                        "\n",
                        "=== Checkpoint Hand 17500 (Eps: 0.07) ===\n",
                        "ValueBot: Avg -0.90 BB | Win 76.0%\n",
                        "BluffBot: Avg -1.04 BB | Win 55.9%\n",
                        "Balanced: Avg -0.26 BB | Win 60.0%\n",
                        "\n",
                        "=== Checkpoint Hand 20000 (Eps: 0.05) ===\n",
                        "ValueBot: Avg 0.03 BB | Win 75.8%\n",
                        "BluffBot: Avg -0.79 BB | Win 55.5%\n",
                        "Balanced: Avg -0.60 BB | Win 59.3%\n",
                        "\n",
                        "=== Checkpoint Hand 22500 (Eps: 0.05) ===\n",
                        "ValueBot: Avg 0.14 BB | Win 75.4%\n",
                        "BluffBot: Avg -0.85 BB | Win 54.9%\n",
                        "Balanced: Avg -0.04 BB | Win 59.1%\n",
                        "\n",
                        "=== FINAL V10 EVALUATION (Epsilon=0.0) ===\n",
                        "Vs ValueBot: Avg -0.26 BB | Total: -526.5 BB | Win: 76.5%\n",
                        "Vs BluffBot: Avg -1.11 BB | Total: -2214.5 BB | Win: 51.2%\n",
                        "Vs Balanced: Avg -0.58 BB | Total: -1163.5 BB | Win: 58.2%\n"
                    ]
                }
            ],
            "source": [
                "def train_v10(num_hands=25000):\n",
                "    env = ContextAwarePokerEnv()\n",
                "    agent = HybridAgentV10(env.game_state_dim)\n",
                "    buffer = ReplayBufferV10(capacity=50000)\n",
                "    \n",
                "    # REFINED Opponent Pool (No Maniac/Nit)\n",
                "    opps = {\n",
                "        'ValueBot': ValueBot(),\n",
                "        'BluffBot': BluffBot(),\n",
                "        'Balanced': BalancedBot()\n",
                "    }\n",
                "    opp_names = list(opps.keys())\n",
                "    stats = {name: {'rewards': [], 'wins': 0, 'hands': 0} for name in opp_names}\n",
                "    \n",
                "    print(f\"Training V10 (Targeted) for {num_hands} hands...\")\n",
                "    current_opp_name = 'ValueBot'\n",
                "    \n",
                "    # Dynamic Epsilon Decay Calculation\n",
                "    # Goal: Reach 0.05 at 80% of training (hand 20000)\n",
                "    # Formula: 1.0 * (decay)^20000 = 0.05  => decay = 0.05^(1/20000)\n",
                "    decay_steps = int(num_hands * 0.8)\n",
                "    agent.epsilon_decay = (0.05) ** (1 / decay_steps)\n",
                "    print(f\"Dynamic Epsilon Decay Factor: {agent.epsilon_decay:.6f}\")\n",
                "\n",
                "    for hand in range(num_hands):\n",
                "        if hand % 50 == 0: \n",
                "            current_opp_name = random.choice(opp_names)\n",
                "            # CRITICAL: Switch History Context\n",
                "            env.load_opponent_history(current_opp_name)\n",
                "            \n",
                "        opponent = opps[current_opp_name]\n",
                "        \n",
                "        obs, info = env.reset()\n",
                "        done = False\n",
                "        episode_transitions = []\n",
                "        pending_agent_obs = None\n",
                "        pending_agent_action = None\n",
                "        \n",
                "        while not done:\n",
                "            if env.state.actor_index == env.agent_player_index:\n",
                "                if pending_agent_obs is not None:\n",
                "                    episode_transitions.append((pending_agent_obs['game_state'], pending_agent_obs['history'], pending_agent_action, 0.0, obs['game_state'], obs['history'], False, info['legal_actions']))\n",
                "                action = agent.select_action(obs, info['legal_actions'])\n",
                "                pending_agent_obs, pending_agent_action = obs, action\n",
                "                obs, reward, done, _, info = env.step(action)\n",
                "            else:\n",
                "                action = opponent.select_action(env.state, info['legal_actions'])\n",
                "                env.update_opponent_history(action)\n",
                "                env._execute_action(action)\n",
                "                env._run_automations()\n",
                "                done = env.state.status is False\n",
                "                if not done:\n",
                "                    obs = env._get_observation()\n",
                "                    info['legal_actions'] = env._get_legal_actions()\n",
                "        \n",
                "        final_reward = env.get_final_reward()\n",
                "        env.append_outcome_token(final_reward)\n",
                "        term_obs = env._get_observation()\n",
                "        if pending_agent_obs is not None:\n",
                "             episode_transitions.append((pending_agent_obs['game_state'], pending_agent_obs['history'], pending_agent_action, 0.0, term_obs['game_state'], term_obs['history'], True, []))\n",
                "        \n",
                "        stats[current_opp_name]['rewards'].append(final_reward)\n",
                "        stats[current_opp_name]['hands'] += 1\n",
                "        if final_reward > 0: stats[current_opp_name]['wins'] += 1\n",
                "        \n",
                "        for i in range(len(episode_transitions)):\n",
                "            s, h, a, r, ns, nh, d, l = episode_transitions[i]\n",
                "            buffer.push((s, h, a, final_reward, ns, nh, d, l))\n",
                "            \n",
                "        if len(buffer) > 1000: agent.train(buffer)\n",
                "        agent.epsilon = max(agent.epsilon_min, agent.epsilon * agent.epsilon_decay)\n",
                "        if hand % 500 == 0: agent.update_target()\n",
                "        \n",
                "        if hand % 2500 == 0 and hand > 0:\n",
                "            print(f\"\\n=== Checkpoint Hand {hand} (Eps: {agent.epsilon:.2f}) ===\")\n",
                "            for name in opp_names:\n",
                "                 if stats[name]['hands'] > 0:\n",
                "                     print(f\"{name}: Avg {np.mean(stats[name]['rewards'][-200:]):.2f} BB | Win {stats[name]['wins']/stats[name]['hands']:.1%}\")\n",
                "    return agent, stats\n",
                "\n",
                "def evaluate_agent_v10(agent):\n",
                "    print(\"\\n=== FINAL V10 EVALUATION (Epsilon=0.0) ===\")\n",
                "    agent.epsilon = 0.0\n",
                "    env = ContextAwarePokerEnv()\n",
                "    opps = {'ValueBot': ValueBot(), 'BluffBot': BluffBot(), 'Balanced': BalancedBot()}\n",
                "    \n",
                "    for name, opponent in opps.items():\n",
                "        env.load_opponent_history(name) # Start fresh for this opp\n",
                "        rewards, wins = [], 0\n",
                "        for _ in range(2000):\n",
                "            obs, info = env.reset()\n",
                "            done = False\n",
                "            while not done:\n",
                "                if env.state.actor_index == env.agent_player_index:\n",
                "                    action = agent.select_action(obs, info['legal_actions'])\n",
                "                    obs, _, done, _, info = env.step(action)\n",
                "                else:\n",
                "                    action = opponent.select_action(env.state, info['legal_actions'])\n",
                "                    env.update_opponent_history(action)\n",
                "                    env._execute_action(action)\n",
                "                    env._run_automations()\n",
                "                    done = env.state.status is False\n",
                "                    if not done: obs = env._get_observation(); info['legal_actions'] = env._get_legal_actions()\n",
                "            final_reward = env.get_final_reward()\n",
                "            env.append_outcome_token(final_reward)\n",
                "            rewards.append(final_reward)\n",
                "            if final_reward > 0: wins += 1\n",
                "        print(f\"Vs {name}: Avg {np.mean(rewards):.2f} BB | Total: {sum(rewards):.1f} BB | Win: {wins/2000:.1%}\")\n",
                "\n",
                "agent, stats = train_v10(25000)\n",
                "evaluate_agent_v10(agent)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
