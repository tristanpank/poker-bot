{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Poker Agent V6: Adaptive Opponent Modeling\n",
                "\n",
                "This notebook implements the V6 architecture designed to fix the lack of adaptability in previous versions. \n",
                "\n",
                "### Key Improvements:\n",
                "1. **Dual-Branch Network**: \n",
                "   - **MLP Branch**: Analyses cards and board (static state).\n",
                "   - **LSTM Branch**: Analyses the **sequence of actions** in the hand. This is the \"memory\" that allows the agent to distinguish between a Maniac (constant raises) and a Nit (passive play).\n",
                "2. **Explicit History Tracking**: The environment now returns a sequence of the last 20 actions in the current hand.\n",
                "3. **Adaptive Training**: The agent trains against a mixed pool of opponents (Maniac, Nit, Random) to learn context-specific strategies.\n",
                "\n",
                "### Architecture Diagram\n",
                "```\n",
                "State (Cards, Pot) --> MLP --> Feat1 --\\\n",
                "                                        (+)--> FC -> Q-Values\n",
                "Action History     --> LSTM -> Feat2 --/\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Using device: cuda\n"
                    ]
                }
            ],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "import torch.nn.functional as F\n",
                "import numpy as np\n",
                "import gymnasium as gym\n",
                "from gymnasium import spaces\n",
                "from collections import deque\n",
                "import random\n",
                "import matplotlib.pyplot as plt\n",
                "from typing import Optional, Tuple, List, Dict, Any\n",
                "\n",
                "from pokerkit import Automation, NoLimitTexasHoldem, Card\n",
                "\n",
                "# Constants\n",
                "SEED = 42\n",
                "MAX_HISTORY_LEN = 20\n",
                "ACTION_EMBED_DIM = 8\n",
                "HIDDEN_DIM_LSTM = 64\n",
                "HIDDEN_DIM_MLP = 128\n",
                "\n",
                "# Action Constants\n",
                "ENV_FOLD = 0\n",
                "ENV_CHECK_CALL = 1\n",
                "ENV_BET_RAISE = 2\n",
                "NUM_ACTIONS = 3\n",
                "\n",
                "# History Tokens\n",
                "ACT_PAD = 0\n",
                "ACT_V_FOLD = 1\n",
                "ACT_V_CHECK_CALL = 2\n",
                "ACT_V_BET_RAISE = 3\n",
                "OPP_FOLD = 4\n",
                "OPP_CHECK_CALL = 5\n",
                "OPP_BET_RAISE = 6\n",
                "HISTORY_VOCAB_SIZE = 7\n",
                "\n",
                "random.seed(SEED)\n",
                "np.random.seed(SEED)\n",
                "torch.manual_seed(SEED)\n",
                "\n",
                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "print(f\"Using device: {device}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "class PokerKitGymEnvV6(gym.Env):\n",
                "    \"\"\"\n",
                "    Gymnasium wrapper for PokerKit's No-Limit Texas Hold'em.\n",
                "    Returns 'action_history' in observation to allow opponent modeling.\n",
                "    \"\"\"\n",
                "    \n",
                "    def __init__(self, num_players: int = 2, starting_stack: int = 1000, \n",
                "                 small_blind: int = 5, big_blind: int = 10):\n",
                "        super().__init__()\n",
                "        self.num_players = num_players\n",
                "        self.starting_stack = starting_stack\n",
                "        self.small_blind = small_blind\n",
                "        self.big_blind = big_blind\n",
                "        \n",
                "        self.game_state_dim = 52*2 + 52*5 + num_players + 1 + 1 + 4 + 1 \n",
                "        \n",
                "        self.observation_space = spaces.Dict({\n",
                "            'game_state': spaces.Box(low=0, high=1, shape=(self.game_state_dim,), dtype=np.float32),\n",
                "            'history': spaces.Box(low=0, high=HISTORY_VOCAB_SIZE-1, shape=(MAX_HISTORY_LEN,), dtype=np.int64)\n",
                "        })\n",
                "        self.action_space = spaces.Discrete(NUM_ACTIONS)\n",
                "        self.state = None\n",
                "        self.agent_player_index = 0\n",
                "        self.action_history = deque(maxlen=MAX_HISTORY_LEN)\n",
                "        \n",
                "    def _card_to_index(self, card: Card) -> int:\n",
                "        ranks = '23456789TJQKA'\n",
                "        suits = 'cdhs'\n",
                "        rank_idx = ranks.index(card.rank)\n",
                "        suit_idx = suits.index(card.suit)\n",
                "        return rank_idx * 4 + suit_idx\n",
                "    \n",
                "    def _encode_card(self, card: Optional[Card]) -> np.ndarray:\n",
                "        encoding = np.zeros(52, dtype=np.float32)\n",
                "        if card is not None:\n",
                "            encoding[self._card_to_index(card)] = 1.0\n",
                "        return encoding\n",
                "    \n",
                "    def _flatten_cards(self, cards) -> List:\n",
                "        flat = []\n",
                "        for item in cards:\n",
                "            if hasattr(item, 'rank'):\n",
                "                flat.append(item)\n",
                "            else:\n",
                "                flat.extend(self._flatten_cards(item))\n",
                "        return flat\n",
                "    \n",
                "    def _get_observation(self) -> Dict[str, Any]:\n",
                "        state_vector = []\n",
                "        hole_cards = self._flatten_cards(self.state.hole_cards[self.agent_player_index])\n",
                "        for i in range(2):\n",
                "            if i < len(hole_cards):\n",
                "                state_vector.extend(self._encode_card(hole_cards[i]))\n",
                "            else:\n",
                "                state_vector.extend(np.zeros(52, dtype=np.float32))\n",
                "        \n",
                "        board_cards = self._flatten_cards(self.state.board_cards)\n",
                "        for i in range(5):\n",
                "            if i < len(board_cards):\n",
                "                state_vector.extend(self._encode_card(board_cards[i]))\n",
                "            else:\n",
                "                state_vector.extend(np.zeros(52, dtype=np.float32))\n",
                "        \n",
                "        for i in range(self.num_players):\n",
                "            stack = self.state.stacks[i] / self.starting_stack\n",
                "            state_vector.append(min(stack, 2.0))\n",
                "        \n",
                "        total_pot = sum(self.state.bets)\n",
                "        state_vector.append(total_pot / (self.starting_stack * self.num_players))\n",
                "        \n",
                "        if self.state.actor_index is not None:\n",
                "            state_vector.append(self.state.actor_index / max(1, self.num_players - 1))\n",
                "        else:\n",
                "            state_vector.append(0.0)\n",
                "        \n",
                "        street = [0.0, 0.0, 0.0, 0.0]\n",
                "        num_board = len(board_cards)\n",
                "        if num_board == 0: street[0] = 1.0\n",
                "        elif num_board == 3: street[1] = 1.0\n",
                "        elif num_board == 4: street[2] = 1.0\n",
                "        else: street[3] = 1.0\n",
                "        state_vector.extend(street)\n",
                "        state_vector.append(float(self.agent_player_index))\n",
                "\n",
                "        history_seq = list(self.action_history)\n",
                "        pad_len = MAX_HISTORY_LEN - len(history_seq)\n",
                "        history_padded = [ACT_PAD] * pad_len + history_seq\n",
                "        \n",
                "        return {\n",
                "            'game_state': np.array(state_vector, dtype=np.float32),\n",
                "            'history': np.array(history_padded, dtype=np.int64)\n",
                "        }\n",
                "    \n",
                "    def _update_history(self, player_idx: int, action: int):\n",
                "        if player_idx == self.agent_player_index:\n",
                "            if action == ENV_FOLD: token = ACT_V_FOLD\n",
                "            elif action == ENV_CHECK_CALL: token = ACT_V_CHECK_CALL\n",
                "            else: token = ACT_V_BET_RAISE\n",
                "        else:\n",
                "            if action == ENV_FOLD: token = OPP_FOLD\n",
                "            elif action == ENV_CHECK_CALL: token = OPP_CHECK_CALL\n",
                "            else: token = OPP_BET_RAISE\n",
                "        self.action_history.append(token)\n",
                "\n",
                "    def _get_legal_actions(self) -> List[int]:\n",
                "        legal = []\n",
                "        if self.state.can_fold(): legal.append(ENV_FOLD)\n",
                "        if self.state.can_check_or_call(): legal.append(ENV_CHECK_CALL)\n",
                "        if self.state.can_complete_bet_or_raise_to(): legal.append(ENV_BET_RAISE)\n",
                "        return legal if legal else [ENV_CHECK_CALL]\n",
                "    \n",
                "    def _execute_action(self, action: int) -> None:\n",
                "        if action == ENV_FOLD:\n",
                "            if self.state.can_fold(): self.state.fold()\n",
                "            elif self.state.can_check_or_call(): self.state.check_or_call()\n",
                "        elif action == ENV_CHECK_CALL:\n",
                "            if self.state.can_check_or_call(): self.state.check_or_call()\n",
                "            elif self.state.can_fold(): self.state.fold()\n",
                "        elif action == ENV_BET_RAISE:\n",
                "            if self.state.can_complete_bet_or_raise_to():\n",
                "                min_r = self.state.min_completion_betting_or_raising_to_amount\n",
                "                max_r = self.state.max_completion_betting_or_raising_to_amount\n",
                "                self.state.complete_bet_or_raise_to(min(min_r * 2, max_r))\n",
                "            elif self.state.can_check_or_call():\n",
                "                self.state.check_or_call()\n",
                "    \n",
                "    def _run_automations(self) -> None:\n",
                "        while self.state.can_burn_card(): self.state.burn_card('??')\n",
                "        while self.state.can_deal_board(): self.state.deal_board()\n",
                "        while self.state.can_push_chips(): self.state.push_chips()\n",
                "        while self.state.can_pull_chips(): self.state.pull_chips()\n",
                "    \n",
                "    def reset(self, seed=None, options=None) -> Tuple[Dict, Dict]:\n",
                "        super().reset(seed=seed)\n",
                "        self.action_history.clear()\n",
                "        self.state = NoLimitTexasHoldem.create_state(\n",
                "            automations=(Automation.ANTE_POSTING, Automation.BET_COLLECTION, Automation.BLIND_OR_STRADDLE_POSTING, Automation.HOLE_CARDS_SHOWING_OR_MUCKING, Automation.HAND_KILLING, Automation.CHIPS_PUSHING, Automation.CHIPS_PULLING),\n",
                "            ante_trimming_status=True,\n",
                "            raw_antes={-1: 0},\n",
                "            raw_blinds_or_straddles=(self.small_blind, self.big_blind),\n",
                "            min_bet=self.big_blind,\n",
                "            raw_starting_stacks=[self.starting_stack] * self.num_players,\n",
                "            player_count=self.num_players,\n",
                "        )\n",
                "        while self.state.can_deal_hole(): self.state.deal_hole()\n",
                "        self._run_automations()\n",
                "        return self._get_observation(), {'legal_actions': self._get_legal_actions()}\n",
                "    \n",
                "    def step(self, action: int) -> Tuple[Dict, float, bool, bool, Dict]:\n",
                "        if self.state.actor_index is not None:\n",
                "             self._update_history(self.state.actor_index, action)\n",
                "        self._execute_action(action)\n",
                "        self._run_automations()\n",
                "        done = self.state.status is False\n",
                "        reward = 0.0\n",
                "        if done:\n",
                "            reward = (self.state.stacks[self.agent_player_index] - self.starting_stack) / self.big_blind\n",
                "        obs = self._get_observation()\n",
                "        info = {'legal_actions': self._get_legal_actions() if not done else []}\n",
                "        return obs, reward, done, False, info\n",
                "\n",
                "    def get_final_reward(self) -> float:\n",
                "        return (self.state.stacks[self.agent_player_index] - self.starting_stack) / self.big_blind\n",
                "    \n",
                "    def update_opponent_history(self, action: int):\n",
                "        opp_idx = 1 - self.agent_player_index\n",
                "        self._update_history(opp_idx, action)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "class DualBranchDRQN(nn.Module):\n",
                "    def __init__(self, state_dim: int, action_dim: int):\n",
                "        super().__init__()\n",
                "        self.state_net = nn.Sequential(\n",
                "            nn.Linear(state_dim, 256), nn.LayerNorm(256), nn.ReLU(),\n",
                "            nn.Linear(256, 128), nn.ReLU()\n",
                "        )\n",
                "        self.action_embedding = nn.Embedding(HISTORY_VOCAB_SIZE, ACTION_EMBED_DIM)\n",
                "        self.lstm = nn.LSTM(input_size=ACTION_EMBED_DIM, hidden_size=HIDDEN_DIM_LSTM, batch_first=True)\n",
                "        self.value_head = nn.Sequential(\n",
                "            nn.Linear(128 + HIDDEN_DIM_LSTM, 128), nn.ReLU(),\n",
                "            nn.Linear(128, action_dim)\n",
                "        )\n",
                "        \n",
                "    def forward(self, state, history):\n",
                "        s_feat = self.state_net(state)\n",
                "        h_embed = self.action_embedding(history)\n",
                "        lstm_out, (hn, cn) = self.lstm(h_embed)\n",
                "        h_context = hn[-1]\n",
                "        combined = torch.cat([s_feat, h_context], dim=1)\n",
                "        return self.value_head(combined)\n",
                "\n",
                "class ReplayBufferV6:\n",
                "    def __init__(self, capacity=50000):\n",
                "        self.buffer = deque(maxlen=capacity)\n",
                "    def push(self, transition):\n",
                "        self.buffer.append(transition)\n",
                "    def sample(self, batch_size):\n",
                "        return random.sample(self.buffer, min(len(self.buffer), batch_size))\n",
                "    def __len__(self): return len(self.buffer)\n",
                "\n",
                "class AdaptiveAgent:\n",
                "    def __init__(self, state_dim, action_dim=NUM_ACTIONS, lr=1e-4):\n",
                "        self.model = DualBranchDRQN(state_dim, action_dim).to(device)\n",
                "        self.target_model = DualBranchDRQN(state_dim, action_dim).to(device)\n",
                "        self.target_model.load_state_dict(self.model.state_dict())\n",
                "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
                "        self.gamma = 0.99\n",
                "        self.epsilon = 1.0\n",
                "        self.epsilon_min = 0.05\n",
                "        self.epsilon_decay = 0.99995\n",
                "        \n",
                "    def select_action(self, obs, legal_actions, eval_mode=False):\n",
                "        if not eval_mode and random.random() < self.epsilon:\n",
                "            return random.choice(legal_actions)\n",
                "        state_t = torch.FloatTensor(obs['game_state']).unsqueeze(0).to(device)\n",
                "        h_t = torch.LongTensor(obs['history']).unsqueeze(0).to(device)\n",
                "        with torch.no_grad():\n",
                "            q_values = self.model(state_t, h_t)\n",
                "        q_numpy = q_values.cpu().numpy().flatten()\n",
                "        masked_q = np.full(NUM_ACTIONS, -np.inf)\n",
                "        for a in legal_actions: masked_q[a] = q_numpy[a]\n",
                "        return int(np.argmax(masked_q))\n",
                "\n",
                "    def train(self, buffer, batch_size=64):\n",
                "        if len(buffer) < batch_size: return None\n",
                "        batch = buffer.sample(batch_size)\n",
                "        states = torch.FloatTensor(np.array([t[0] for t in batch])).to(device)\n",
                "        histories = torch.LongTensor(np.array([t[1] for t in batch])).to(device)\n",
                "        actions = torch.LongTensor(np.array([t[2] for t in batch])).to(device)\n",
                "        rewards = torch.FloatTensor(np.array([t[3] for t in batch])).to(device)\n",
                "        next_states = torch.FloatTensor(np.array([t[4] for t in batch])).to(device)\n",
                "        next_histories = torch.LongTensor(np.array([t[5] for t in batch])).to(device)\n",
                "        dones = torch.FloatTensor(np.array([t[6] for t in batch])).to(device)\n",
                "        \n",
                "        current_q = self.model(states, histories).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
                "        with torch.no_grad():\n",
                "            next_actions = self.model(next_states, next_histories).argmax(1).unsqueeze(1)\n",
                "            target_q_next = self.target_model(next_states, next_histories).gather(1, next_actions).squeeze(1)\n",
                "            target = rewards + (1 - dones) * self.gamma * target_q_next\n",
                "        loss = F.mse_loss(current_q, target)\n",
                "        self.optimizer.zero_grad()\n",
                "        loss.backward()\n",
                "        torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
                "        self.optimizer.step()\n",
                "        return loss.item()\n",
                "    \n",
                "    def update_target(self): self.target_model.load_state_dict(self.model.state_dict())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "class ManiacAgent:\n",
                "    def select_action(self, legal_actions):\n",
                "        if ENV_BET_RAISE in legal_actions: return ENV_BET_RAISE\n",
                "        if ENV_CHECK_CALL in legal_actions: return ENV_CHECK_CALL\n",
                "        return ENV_FOLD\n",
                "class NitAgent:\n",
                "    def select_action(self, legal_actions):\n",
                "        # Fold to aggression 90% of time\n",
                "        if ENV_FOLD in legal_actions and ENV_CHECK_CALL in legal_actions:\n",
                "            if random.random() < 0.9: return ENV_FOLD\n",
                "        if ENV_CHECK_CALL in legal_actions: return ENV_CHECK_CALL\n",
                "        return ENV_FOLD\n",
                "class RandomAgent:\n",
                "    def select_action(self, legal_actions): return random.choice(legal_actions)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Training V6 for 30000 hands against MIXED opponents...\n",
                        "Hand 0 | Avg Reward: 14.00\n",
                        "Hand 2500 | Avg Reward: -1.47\n",
                        "Hand 5000 | Avg Reward: -2.10\n",
                        "Hand 7500 | Avg Reward: 0.56\n",
                        "Hand 10000 | Avg Reward: 1.65\n",
                        "Hand 12500 | Avg Reward: -2.98\n",
                        "Hand 15000 | Avg Reward: -1.67\n",
                        "Hand 17500 | Avg Reward: -1.30\n",
                        "Hand 20000 | Avg Reward: -2.08\n",
                        "Hand 22500 | Avg Reward: -3.95\n",
                        "Hand 25000 | Avg Reward: -3.31\n",
                        "Hand 27500 | Avg Reward: -2.42\n",
                        "\n",
                        "--- EVALUATION ---\n",
                        "Vs Maniac: -1.27 BB/hand\n",
                        "Vs Nit: 0.53 BB/hand\n",
                        "Vs Random: -0.09 BB/hand\n"
                    ]
                }
            ],
            "source": [
                "def train_v6(num_hands=25000):\n",
                "    env = PokerKitGymEnvV6()\n",
                "    agent = AdaptiveAgent(env.game_state_dim)\n",
                "    buffer = ReplayBufferV6(capacity=50000)\n",
                "    opponents = [ManiacAgent(), NitAgent(), RandomAgent()]\n",
                "    opp_names = ['Maniac', 'Nit', 'Random']\n",
                "    \n",
                "    rewards_history = []\n",
                "    \n",
                "    print(f\"Training V6 for {num_hands} hands against MIXED opponents...\")\n",
                "    \n",
                "    for hand in range(num_hands):\n",
                "        opp_idx = random.randint(0, 2)\n",
                "        opponent = opponents[opp_idx]\n",
                "        \n",
                "        obs, info = env.reset()\n",
                "        done = False\n",
                "        episode_transitions = []\n",
                "        pending_agent_obs = None\n",
                "        pending_agent_action = None\n",
                "        \n",
                "        while not done:\n",
                "            if env.state.actor_index == env.agent_player_index:\n",
                "                if pending_agent_obs is not None:\n",
                "                    episode_transitions.append((pending_agent_obs['game_state'], pending_agent_obs['history'], pending_agent_action, 0.0, obs['game_state'], obs['history'], False, info['legal_actions']))\n",
                "                \n",
                "                action = agent.select_action(obs, info['legal_actions'])\n",
                "                pending_agent_obs = obs\n",
                "                pending_agent_action = action\n",
                "                obs, reward, done, _, info = env.step(action)\n",
                "                \n",
                "                if done:\n",
                "                    episode_transitions.append((pending_agent_obs['game_state'], pending_agent_obs['history'], pending_agent_action, 0.0, obs['game_state'], obs['history'], True, []))\n",
                "            else:\n",
                "                action = opponent.select_action(info['legal_actions'])\n",
                "                env.update_opponent_history(action)\n",
                "                env._execute_action(action)\n",
                "                env._run_automations()\n",
                "                done = env.state.status is False\n",
                "                if done and pending_agent_obs is not None:\n",
                "                    term_obs = env._get_observation()\n",
                "                    episode_transitions.append((pending_agent_obs['game_state'], pending_agent_obs['history'], pending_agent_action, 0.0, term_obs['game_state'], term_obs['history'], True, []))\n",
                "                elif not done:\n",
                "                    obs = env._get_observation()\n",
                "                    info['legal_actions'] = env._get_legal_actions()\n",
                "        \n",
                "        final_reward = env.get_final_reward()\n",
                "        rewards_history.append(final_reward)\n",
                "        for i in range(len(episode_transitions)):\n",
                "            s, h, a, r, ns, nh, d, l = episode_transitions[i]\n",
                "            buffer.push((s, h, a, final_reward, ns, nh, d, l))\n",
                "            \n",
                "        if len(buffer) > 1000:\n",
                "            agent.train(buffer)\n",
                "        agent.epsilon = max(agent.epsilon_min, agent.epsilon * agent.epsilon_decay)\n",
                "        if hand % 500 == 0: agent.update_target()\n",
                "        if hand % 2500 == 0: print(f\"Hand {hand} | Avg Reward: {np.mean(rewards_history[-100:]):.2f}\")\n",
                "            \n",
                "    return agent\n",
                "\n",
                "def evaluate(agent, num_hands=500):\n",
                "    print(\"\\n--- EVALUATION ---\")\n",
                "    env = PokerKitGymEnvV6()\n",
                "    opps = {'Maniac': ManiacAgent(), 'Nit': NitAgent(), 'Random': RandomAgent()}\n",
                "    for name, opponent in opps.items():\n",
                "        total = 0\n",
                "        for _ in range(num_hands):\n",
                "            obs, info = env.reset()\n",
                "            done = False\n",
                "            while not done:\n",
                "                if env.state.actor_index == env.agent_player_index:\n",
                "                    action = agent.select_action(obs, info['legal_actions'], eval_mode=True)\n",
                "                    obs, _, done, _, info = env.step(action)\n",
                "                else:\n",
                "                    action = opponent.select_action(info['legal_actions'])\n",
                "                    env.update_opponent_history(action)\n",
                "                    env._execute_action(action)\n",
                "                    env._run_automations()\n",
                "                    if env.state.status is False: done = True\n",
                "                    else: \n",
                "                        obs = env._get_observation()\n",
                "                        info['legal_actions'] = env._get_legal_actions()\n",
                "            total += env.get_final_reward()\n",
                "        print(f\"Vs {name}: {total/num_hands:.2f} BB/hand\")\n",
                "\n",
                "agent = train_v6(30000)\n",
                "evaluate(agent)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
