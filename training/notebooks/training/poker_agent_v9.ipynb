{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Poker Agent V9: Equity-Aware Hybrid Architecture\n",
    "\n",
    "This notebook introduces **Explicit Poker Knowledge** into the RL Agent to fix the negative profitability issues of V8.\n",
    "\n",
    "### The Problem with V8\n",
    "V8 had a high win rate (>60%) but negative profit per hand. This means it was winning small pots (stealing blinds) but losing big pots or failing to extract value. The Neural Network struggled to learn basic hand strength (e.g., \"AA is good\") from raw card bits alone.\n",
    "\n",
    "### V9 Solution: Feature Injection\n",
    "Instead of forcing the agent to derive probability theory from scratch, we calculate key metrics and feed them as **direct inputs**:\n",
    "\n",
    "1. **Monte Carlo Equity**: We run 30 simulations per step to estimate win probability (e.g., `0.85`).\n",
    "2. **Pot Odds**: We calculate the ratio of `Call Cost / Total Pot` (e.g., `0.33`).\n",
    "3. **Stack-to-Pot Ratio (SPR)**: Critical for decision making.\n",
    "\n",
    "### Why this helps?\n",
    "The agent will instantly see correlations like `Equity=0.9` -> `Action=Raise` -> `Reward=$$$`. This \"installs\" basic poker competence immediately, allowing the RL to focus on the harder part: adapting to opponent styles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fe65be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from collections import deque\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Optional, Tuple, List, Dict, Any\n",
    "import copy\n",
    "from itertools import combinations\n",
    "\n",
    "from pokerkit import Automation, NoLimitTexasHoldem, Card, StandardHighHand, Deck\n",
    "\n",
    "# Constants\n",
    "SEED = 42\n",
    "MAX_HISTORY_LEN = 100\n",
    "ACTION_EMBED_DIM = 16\n",
    "HIDDEN_DIM_LSTM = 128\n",
    "\n",
    "# Actions\n",
    "ENV_FOLD = 0\n",
    "ENV_CHECK_CALL = 1\n",
    "ENV_BET_RAISE = 2\n",
    "NUM_ACTIONS = 3\n",
    "\n",
    "# History Tokens\n",
    "ACT_PAD = 0\n",
    "ACT_V_FOLD = 1\n",
    "ACT_V_CHECK_CALL = 2\n",
    "ACT_V_BET_RAISE = 3\n",
    "OPP_FOLD = 4\n",
    "OPP_CHECK_CALL = 5\n",
    "OPP_BET_RAISE = 6\n",
    "OUT_AGENT_WIN = 7\n",
    "OUT_AGENT_LOSS = 8\n",
    "OUT_TIE = 9\n",
    "OUT_NEW_HAND = 10\n",
    "HISTORY_VOCAB_SIZE = 11\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0089c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Universal Helpers (Monte Carlo & Flatten)\n",
    "\n",
    "def flatten_cards_list(items):\n",
    "    out = []\n",
    "    if isinstance(items, Card): return [items]\n",
    "    for x in items:\n",
    "        if isinstance(x, (list, tuple)): out.extend(flatten_cards_list(x))\n",
    "        else: out.append(x)\n",
    "    return out\n",
    "\n",
    "def monte_carlo_equity(hole_cards: List[Card], board_cards: List[Card], iterations=30) -> float:\n",
    "    \"\"\"Run fast MC simulation to estimate win probability.\"\"\"\n",
    "    if not hole_cards: return 0.5 \n",
    "    \n",
    "    wins = 0\n",
    "    # Flatten inputs just in case\n",
    "    hole_cards = flatten_cards_list(hole_cards)\n",
    "    board_cards = flatten_cards_list(board_cards)\n",
    "    known_cards = set(hole_cards + board_cards)\n",
    "    \n",
    "    for _ in range(iterations):\n",
    "        deck_cards = [c for c in Deck.STANDARD if c not in known_cards]\n",
    "        random.shuffle(deck_cards)\n",
    "        \n",
    "        # Opponent hand (2 cards)\n",
    "        opp_hole = deck_cards[:2]\n",
    "        \n",
    "        # Remaining board\n",
    "        needed_board = 5 - len(board_cards)\n",
    "        sim_board = board_cards + deck_cards[2:2+needed_board]\n",
    "        \n",
    "        my_total = hole_cards + sim_board\n",
    "        opp_total = opp_hole + sim_board\n",
    "        \n",
    "        # Best 5 of 7\n",
    "        my_hand = max(StandardHighHand(c) for c in combinations(my_total, 5))\n",
    "        opp_hand = max(StandardHighHand(c) for c in combinations(opp_total, 5))\n",
    "        \n",
    "        if my_hand > opp_hand: wins += 1\n",
    "        elif my_hand == opp_hand: wins += 0.5\n",
    "            \n",
    "    return wins / iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c97d596",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EquityPokerEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    V9 Environment: Adds Equity, PotOdds, and SPR to the observation vector.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_players: int = 2, starting_stack: int = 1000, \n",
    "                 small_blind: int = 5, big_blind: int = 10):\n",
    "        super().__init__()\n",
    "        self.num_players = num_players\n",
    "        self.starting_stack = starting_stack\n",
    "        self.small_blind = small_blind\n",
    "        self.big_blind = big_blind\n",
    "        \n",
    "        # Base dim (from V8/V7) was ~320. We add 3 floats: Equity, PotOdds, SPR.\n",
    "        self.base_state_dim = 52*2 + 52*5 + num_players + 1 + 1 + 4 + 1\n",
    "        self.game_state_dim = self.base_state_dim + 3 \n",
    "        \n",
    "        self.observation_space = spaces.Dict({\n",
    "            'game_state': spaces.Box(low=0, high=1, shape=(self.game_state_dim,), dtype=np.float32),\n",
    "            'history': spaces.Box(low=0, high=HISTORY_VOCAB_SIZE-1, shape=(MAX_HISTORY_LEN,), dtype=np.int64)\n",
    "        })\n",
    "        self.action_space = spaces.Discrete(NUM_ACTIONS)\n",
    "        self.state = None\n",
    "        self.agent_player_index = 0\n",
    "        self.global_history = deque(maxlen=MAX_HISTORY_LEN)\n",
    "        for _ in range(MAX_HISTORY_LEN): self.global_history.append(ACT_PAD)\n",
    "        \n",
    "    # --- Card Encoding Helpers (Same as V8) ---\n",
    "    def _card_to_index(self, card: Card) -> int:\n",
    "        ranks = '23456789TJQKA'\n",
    "        suits = 'cdhs'\n",
    "        rank_idx = ranks.index(card.rank)\n",
    "        suit_idx = suits.index(card.suit)\n",
    "        return rank_idx * 4 + suit_idx\n",
    "    \n",
    "    def _encode_card(self, card: Optional[Card]) -> np.ndarray:\n",
    "        encoding = np.zeros(52, dtype=np.float32)\n",
    "        if card is not None: encoding[self._card_to_index(card)] = 1.0\n",
    "        return encoding\n",
    "    \n",
    "    # --- Observation with Feature Injection ---\n",
    "    def _get_observation(self) -> Dict[str, Any]:\n",
    "        # 1. Feature Calculation\n",
    "        hole = flatten_cards_list(self.state.hole_cards[self.agent_player_index])\n",
    "        board = flatten_cards_list(self.state.board_cards)\n",
    "        \n",
    "        # A. Monte Carlo Equity\n",
    "        equity = monte_carlo_equity(hole, board, iterations=25)\n",
    "        \n",
    "        # B. Pot Odds\n",
    "        total_pot = sum(self.state.bets)\n",
    "        current_bet = max(self.state.bets)\n",
    "        my_bet = self.state.bets[self.agent_player_index]\n",
    "        to_call = current_bet - my_bet\n",
    "        pot_odds = 0.0\n",
    "        if (total_pot + to_call) > 0:\n",
    "            pot_odds = to_call / (total_pot + to_call)\n",
    "            \n",
    "        # C. SPR (Stack to Pot Ratio)\n",
    "        my_stack = self.state.stacks[self.agent_player_index]\n",
    "        spr = 0.0\n",
    "        if total_pot > 0:\n",
    "            spr = my_stack / total_pot\n",
    "        # Normalize SPR (clamp at 20)\n",
    "        spr = min(spr / 20.0, 1.0)\n",
    "        \n",
    "        # 2. Build State Vector\n",
    "        state_vector = [equity, pot_odds, spr]\n",
    "        \n",
    "        # Append Standard Card/Board Vector (Same as V8)\n",
    "        for i in range(2):\n",
    "            if i < len(hole): state_vector.extend(self._encode_card(hole[i]))\n",
    "            else: state_vector.extend(np.zeros(52, dtype=np.float32))\n",
    "        for i in range(5):\n",
    "            if i < len(board): state_vector.extend(self._encode_card(board[i]))\n",
    "            else: state_vector.extend(np.zeros(52, dtype=np.float32))\n",
    "        for i in range(self.num_players):\n",
    "            stack = self.state.stacks[i] / self.starting_stack\n",
    "            state_vector.append(min(stack, 2.0))\n",
    "        state_vector.append(total_pot / (self.starting_stack * self.num_players))\n",
    "        if self.state.actor_index is not None:\n",
    "            state_vector.append(self.state.actor_index / max(1, self.num_players - 1))\n",
    "        else: state_vector.append(0.0)\n",
    "        street = [0.0, 0.0, 0.0, 0.0]\n",
    "        num_board = len(board)\n",
    "        if num_board == 0: street[0] = 1.0\n",
    "        elif num_board == 3: street[1] = 1.0\n",
    "        elif num_board == 4: street[2] = 1.0\n",
    "        else: street[3] = 1.0\n",
    "        state_vector.extend(street)\n",
    "        state_vector.append(float(self.agent_player_index))\n",
    "        \n",
    "        return {\n",
    "            'game_state': np.array(state_vector, dtype=np.float32),\n",
    "            'history': np.array(list(self.global_history), dtype=np.int64)\n",
    "        }\n",
    "    \n",
    "    # --- Env Logic (V8 Standard) ---\n",
    "    def _update_history(self, player_idx: int, action: int):\n",
    "        if player_idx == self.agent_player_index:\n",
    "            if action == ENV_FOLD: token = ACT_V_FOLD\n",
    "            elif action == ENV_CHECK_CALL: token = ACT_V_CHECK_CALL\n",
    "            else: token = ACT_V_BET_RAISE\n",
    "        else:\n",
    "            if action == ENV_FOLD: token = OPP_FOLD\n",
    "            elif action == ENV_CHECK_CALL: token = OPP_CHECK_CALL\n",
    "            else: token = OPP_BET_RAISE\n",
    "        self.global_history.append(token)\n",
    "\n",
    "    def append_outcome_token(self, final_reward: float):\n",
    "        if final_reward > 0: self.global_history.append(OUT_AGENT_WIN)\n",
    "        elif final_reward < 0: self.global_history.append(OUT_AGENT_LOSS)\n",
    "        else: self.global_history.append(OUT_TIE)\n",
    "\n",
    "    def _get_legal_actions(self) -> List[int]:\n",
    "        legal = []\n",
    "        if self.state.can_fold(): legal.append(ENV_FOLD)\n",
    "        if self.state.can_check_or_call(): legal.append(ENV_CHECK_CALL)\n",
    "        if self.state.can_complete_bet_or_raise_to(): legal.append(ENV_BET_RAISE)\n",
    "        return legal if legal else [ENV_CHECK_CALL]\n",
    "    \n",
    "    def _execute_action(self, action: int) -> None:\n",
    "        if action == ENV_FOLD:\n",
    "            if self.state.can_fold(): self.state.fold()\n",
    "            elif self.state.can_check_or_call(): self.state.check_or_call()\n",
    "        elif action == ENV_CHECK_CALL:\n",
    "            if self.state.can_check_or_call(): self.state.check_or_call()\n",
    "            elif self.state.can_fold(): self.state.fold()\n",
    "        elif action == ENV_BET_RAISE:\n",
    "            if self.state.can_complete_bet_or_raise_to():\n",
    "                min_r = self.state.min_completion_betting_or_raising_to_amount\n",
    "                max_r = self.state.max_completion_betting_or_raising_to_amount\n",
    "                self.state.complete_bet_or_raise_to(min(min_r * 2, max_r))\n",
    "            elif self.state.can_check_or_call():\n",
    "                self.state.check_or_call()\n",
    "    \n",
    "    def _run_automations(self) -> None:\n",
    "        while self.state.can_burn_card(): self.state.burn_card('??')\n",
    "        while self.state.can_deal_board(): self.state.deal_board()\n",
    "        while self.state.can_push_chips(): self.state.push_chips()\n",
    "        while self.state.can_pull_chips(): self.state.pull_chips()\n",
    "    \n",
    "    def reset(self, seed=None, options=None) -> Tuple[Dict, Dict]:\n",
    "        self.global_history.append(OUT_NEW_HAND)\n",
    "        # Use super().reset() but we wrap basic pokerkit init manually\n",
    "        # gym.Env.reset(self, seed=seed) # Fixed super call\n",
    "        self.state = NoLimitTexasHoldem.create_state(\n",
    "            automations=(Automation.ANTE_POSTING, Automation.BET_COLLECTION, Automation.BLIND_OR_STRADDLE_POSTING, Automation.HOLE_CARDS_SHOWING_OR_MUCKING, Automation.HAND_KILLING, Automation.CHIPS_PUSHING, Automation.CHIPS_PULLING),\n",
    "            ante_trimming_status=True,\n",
    "            raw_antes={-1: 0},\n",
    "            raw_blinds_or_straddles=(self.small_blind, self.big_blind),\n",
    "            min_bet=self.big_blind,\n",
    "            raw_starting_stacks=[self.starting_stack] * self.num_players,\n",
    "            player_count=self.num_players,\n",
    "        )\n",
    "        while self.state.can_deal_hole(): self.state.deal_hole()\n",
    "        self._run_automations()\n",
    "        return self._get_observation(), {'legal_actions': self._get_legal_actions()}\n",
    "    \n",
    "    def step(self, action: int) -> Tuple[Dict, float, bool, bool, Dict]:\n",
    "        if self.state.actor_index is not None:\n",
    "             self._update_history(self.state.actor_index, action)\n",
    "        self._execute_action(action)\n",
    "        self._run_automations()\n",
    "        done = self.state.status is False\n",
    "        reward = 0.0\n",
    "        if done:\n",
    "            reward = (self.state.stacks[self.agent_player_index] - self.starting_stack) / self.big_blind\n",
    "        obs = self._get_observation()\n",
    "        info = {'legal_actions': self._get_legal_actions() if not done else []}\n",
    "        return obs, reward, done, False, info\n",
    "    \n",
    "    def get_final_reward(self) -> float:\n",
    "        return (self.state.stacks[self.agent_player_index] - self.starting_stack) / self.big_blind\n",
    "    \n",
    "    def update_opponent_history(self, action: int):\n",
    "        opp_idx = 1 - self.agent_player_index\n",
    "        self._update_history(opp_idx, action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0020e875",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureAwareV9(nn.Module):\n",
    "    def __init__(self, state_dim: int, action_dim: int):\n",
    "        super().__init__()\n",
    "        # State Branch (Including new Features)\n",
    "        self.state_net = nn.Sequential(\n",
    "            nn.Linear(state_dim, 256), nn.LayerNorm(256), nn.ReLU(),\n",
    "            nn.Linear(256, 128), nn.ReLU()\n",
    "        )\n",
    "        # History Branch\n",
    "        self.action_embedding = nn.Embedding(HISTORY_VOCAB_SIZE, ACTION_EMBED_DIM)\n",
    "        self.lstm = nn.LSTM(input_size=ACTION_EMBED_DIM, hidden_size=HIDDEN_DIM_LSTM, batch_first=True)\n",
    "        \n",
    "        # Merger\n",
    "        self.value_head = nn.Sequential(\n",
    "            nn.Linear(128 + HIDDEN_DIM_LSTM, 256), nn.ReLU(),\n",
    "            nn.Linear(256, action_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, state, history):\n",
    "        s_feat = self.state_net(state)\n",
    "        h_embed = self.action_embedding(history)\n",
    "        lstm_out, (hn, cn) = self.lstm(h_embed)\n",
    "        h_context = hn[-1]\n",
    "        combined = torch.cat([s_feat, h_context], dim=1)\n",
    "        return self.value_head(combined)\n",
    "\n",
    "class ReplayBufferV9:\n",
    "    def __init__(self, capacity=50000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    def push(self, transition):\n",
    "        self.buffer.append(transition)\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, min(len(self.buffer), batch_size))\n",
    "    def __len__(self): return len(self.buffer)\n",
    "\n",
    "class HybridAgentV9:\n",
    "    def __init__(self, state_dim, action_dim=NUM_ACTIONS, lr=1e-4):\n",
    "        self.model = FeatureAwareV9(state_dim, action_dim).to(device)\n",
    "        self.target_model = FeatureAwareV9(state_dim, action_dim).to(device)\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.gamma = 0.99\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.05\n",
    "        self.epsilon_decay = 0.99995\n",
    "        \n",
    "    def select_action(self, obs, legal_actions, eval_mode=False):\n",
    "        if not eval_mode and random.random() < self.epsilon:\n",
    "            return random.choice(legal_actions)\n",
    "        state_t = torch.FloatTensor(obs['game_state']).unsqueeze(0).to(device)\n",
    "        h_t = torch.LongTensor(obs['history']).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.model(state_t, h_t)\n",
    "        q_numpy = q_values.cpu().numpy().flatten()\n",
    "        masked_q = np.full(NUM_ACTIONS, -np.inf)\n",
    "        for a in legal_actions: masked_q[a] = q_numpy[a]\n",
    "        return int(np.argmax(masked_q))\n",
    "\n",
    "    def train(self, buffer, batch_size=64):\n",
    "        if len(buffer) < batch_size: return None\n",
    "        batch = buffer.sample(batch_size)\n",
    "        # Batch unpacking\n",
    "        states = torch.FloatTensor(np.array([t[0] for t in batch])).to(device)\n",
    "        histories = torch.LongTensor(np.array([t[1] for t in batch])).to(device)\n",
    "        actions = torch.LongTensor(np.array([t[2] for t in batch])).to(device)\n",
    "        rewards = torch.FloatTensor(np.array([t[3] for t in batch])).to(device)\n",
    "        next_states = torch.FloatTensor(np.array([t[4] for t in batch])).to(device)\n",
    "        next_histories = torch.LongTensor(np.array([t[5] for t in batch])).to(device)\n",
    "        dones = torch.FloatTensor(np.array([t[6] for t in batch])).to(device)\n",
    "        \n",
    "        # DQN Logic\n",
    "        current_q = self.model(states, histories).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "        with torch.no_grad():\n",
    "            next_actions = self.model(next_states, next_histories).argmax(1).unsqueeze(1)\n",
    "            target_q_next = self.target_model(next_states, next_histories).gather(1, next_actions).squeeze(1)\n",
    "            target = rewards + (1 - dones) * self.gamma * target_q_next\n",
    "        loss = F.mse_loss(current_q, target)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "        self.optimizer.step()\n",
    "        return loss.item()\n",
    "    \n",
    "    def update_target(self): self.target_model.load_state_dict(self.model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50326b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy Bots from V8 (ValueBot, BluffBot, BalancedBot)\n",
    "# Ensure they use the robust flatten_cards_list logic too\n",
    "class HeuristicBot:\n",
    "    def __init__(self, player_idx=1):\n",
    "        self.player_idx = player_idx\n",
    "        \n",
    "    def get_equity(self, state):\n",
    "        # Use V9 robust evaluator\n",
    "        hole = flatten_cards_list(state.hole_cards[self.player_idx])\n",
    "        board = flatten_cards_list(state.board_cards)\n",
    "        return monte_carlo_equity(hole, board, iterations=40)\n",
    "        \n",
    "    def select_action(self, state, legal_actions):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class ValueBot(HeuristicBot):\n",
    "    def select_action(self, state, legal_actions):\n",
    "        equity = self.get_equity(state)\n",
    "        if equity > 0.70 and ENV_BET_RAISE in legal_actions: return ENV_BET_RAISE\n",
    "        if equity > 0.45 and ENV_CHECK_CALL in legal_actions: return ENV_CHECK_CALL\n",
    "        if ENV_FOLD in legal_actions: return ENV_FOLD\n",
    "        return ENV_CHECK_CALL\n",
    "\n",
    "class BluffBot(HeuristicBot):\n",
    "    def select_action(self, state, legal_actions):\n",
    "        equity = self.get_equity(state)\n",
    "        if equity > 0.70 and ENV_BET_RAISE in legal_actions: return ENV_BET_RAISE\n",
    "        if equity < 0.40 and random.random() < 0.30 and ENV_BET_RAISE in legal_actions: return ENV_BET_RAISE\n",
    "        if equity > 0.45 and ENV_CHECK_CALL in legal_actions: return ENV_CHECK_CALL\n",
    "        if ENV_FOLD in legal_actions: return ENV_FOLD\n",
    "        return ENV_CHECK_CALL\n",
    "\n",
    "class BalancedBot(HeuristicBot):\n",
    "    def select_action(self, state, legal_actions):\n",
    "        equity = self.get_equity(state)\n",
    "        if equity > 0.8 and ENV_BET_RAISE in legal_actions: return ENV_BET_RAISE\n",
    "        if ENV_CHECK_CALL in legal_actions:\n",
    "            if equity > 0.5: return ENV_CHECK_CALL\n",
    "            current_bet = max(state.bets)\n",
    "            my_bet = state.bets[self.player_idx]\n",
    "            to_call = current_bet - my_bet\n",
    "            pot_odds = to_call / (sum(state.bets) + to_call + 1e-5)\n",
    "            if equity > pot_odds: return ENV_CHECK_CALL\n",
    "        if ENV_FOLD in legal_actions: return ENV_FOLD\n",
    "        return ENV_CHECK_CALL\n",
    "\n",
    "# Standard Bots\n",
    "class ManiacAgent:\n",
    "    def select_action(self, state, legal_actions):\n",
    "        if ENV_BET_RAISE in legal_actions: return ENV_BET_RAISE\n",
    "        if ENV_CHECK_CALL in legal_actions: return ENV_CHECK_CALL\n",
    "        return ENV_FOLD\n",
    "class NitAgent:\n",
    "    def select_action(self, state, legal_actions):\n",
    "        if ENV_FOLD in legal_actions and ENV_CHECK_CALL in legal_actions:\n",
    "            if random.random() < 0.9: return ENV_FOLD\n",
    "        if ENV_CHECK_CALL in legal_actions: return ENV_CHECK_CALL\n",
    "        return ENV_FOLD\n",
    "class RandomAgent:\n",
    "    def select_action(self, state, legal_actions): return random.choice(legal_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b771aea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_v9(num_hands=50000):\n",
    "    env = EquityPokerEnv()\n",
    "    agent = HybridAgentV9(env.game_state_dim)\n",
    "    buffer = ReplayBufferV9(capacity=50000)\n",
    "    \n",
    "    opps = {\n",
    "        'Maniac': ManiacAgent(),\n",
    "        'Nit': NitAgent(),\n",
    "        'Random': RandomAgent(),\n",
    "        'ValueBot': ValueBot(),\n",
    "        'BluffBot': BluffBot(),\n",
    "        'Balanced': BalancedBot()\n",
    "    }\n",
    "    opp_names = list(opps.keys())\n",
    "    stats = {name: {'rewards': [], 'wins': 0, 'hands': 0} for name in opp_names}\n",
    "    \n",
    "    print(f\"Training V9 (Equity-Hybrid) for {num_hands} hands...\")\n",
    "    current_opp_name = 'Random'\n",
    "    \n",
    "    for hand in range(num_hands):\n",
    "        if hand % 20 == 0: current_opp_name = random.choice(opp_names)\n",
    "        opponent = opps[current_opp_name]\n",
    "        \n",
    "        obs, info = env.reset()\n",
    "        done = False\n",
    "        episode_transitions = []\n",
    "        pending_agent_obs = None\n",
    "        pending_agent_action = None\n",
    "        \n",
    "        while not done:\n",
    "            if env.state.actor_index == env.agent_player_index:\n",
    "                if pending_agent_obs is not None:\n",
    "                    episode_transitions.append((pending_agent_obs['game_state'], pending_agent_obs['history'], pending_agent_action, 0.0, obs['game_state'], obs['history'], False, info['legal_actions']))\n",
    "                \n",
    "                action = agent.select_action(obs, info['legal_actions'])\n",
    "                pending_agent_obs = obs\n",
    "                pending_agent_action = action\n",
    "                obs, reward, done, _, info = env.step(action)\n",
    "            else:\n",
    "                action = opponent.select_action(env.state, info['legal_actions'])\n",
    "                env.update_opponent_history(action)\n",
    "                env._execute_action(action)\n",
    "                env._run_automations()\n",
    "                done = env.state.status is False\n",
    "                if not done:\n",
    "                    obs = env._get_observation()\n",
    "                    info['legal_actions'] = env._get_legal_actions()\n",
    "        \n",
    "        final_reward = env.get_final_reward()\n",
    "        env.append_outcome_token(final_reward)\n",
    "        term_obs = env._get_observation()\n",
    "        \n",
    "        if pending_agent_obs is not None:\n",
    "             episode_transitions.append((pending_agent_obs['game_state'], pending_agent_obs['history'], pending_agent_action, 0.0, term_obs['game_state'], term_obs['history'], True, []))\n",
    "        \n",
    "        stats[current_opp_name]['rewards'].append(final_reward)\n",
    "        stats[current_opp_name]['hands'] += 1\n",
    "        if final_reward > 0: stats[current_opp_name]['wins'] += 1\n",
    "        \n",
    "        for i in range(len(episode_transitions)):\n",
    "            s, h, a, r, ns, nh, d, l = episode_transitions[i]\n",
    "            buffer.push((s, h, a, final_reward, ns, nh, d, l))\n",
    "            \n",
    "        if len(buffer) > 1000:\n",
    "            agent.train(buffer)\n",
    "        agent.epsilon = max(agent.epsilon_min, agent.epsilon * agent.epsilon_decay)\n",
    "        if hand % 500 == 0: agent.update_target()\n",
    "        \n",
    "        if hand % 2500 == 0 and hand > 0:\n",
    "            print(f\"\\n=== Checkpoint Hand {hand} (Eps: {agent.epsilon:.2f}) ===\")\n",
    "            print(\"--- Extreme Opponents ---\")\n",
    "            for name in ['Maniac', 'Nit', 'Random']:\n",
    "                 if stats[name]['hands'] > 0:\n",
    "                     print(f\"{name}: Avg {np.mean(stats[name]['rewards'][-200:]):.2f} BB | Win {stats[name]['wins']/stats[name]['hands']:.1%}\")\n",
    "            print(\"--- Realistic Opponents ---\")\n",
    "            for name in ['ValueBot', 'BluffBot', 'Balanced']:\n",
    "                 if stats[name]['hands'] > 0:\n",
    "                     print(f\"{name}: Avg {np.mean(stats[name]['rewards'][-200:]):.2f} BB | Win {stats[name]['wins']/stats[name]['hands']:.1%}\")\n",
    "    return agent, stats\n",
    "\n",
    "def plot_v9_results(stats):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    for name in ['Maniac', 'Nit', 'Random']:\n",
    "        if len(stats[name]['rewards']) > 100:\n",
    "            ax1.plot(np.convolve(stats[name]['rewards'], np.ones(100)/100, mode='valid'), label=name)\n",
    "    ax1.set_title(\"V9 vs Extreme Bots\")\n",
    "    ax1.legend()\n",
    "    for name in ['ValueBot', 'BluffBot', 'Balanced']:\n",
    "        if len(stats[name]['rewards']) > 100:\n",
    "            ax2.plot(np.convolve(stats[name]['rewards'], np.ones(100)/100, mode='valid'), label=name)\n",
    "    ax2.set_title(\"V9 vs Realistic Bots\")\n",
    "    ax2.legend()\n",
    "    plt.show()\n",
    "    plt.savefig(\"v9_results.png\")\n",
    "\n",
    "agent, stats = train_v9(50000)\n",
    "plot_v9_results(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d69b386b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== FINAL EVALUATION (Epsilon=0.0) ===\n",
      "Vs Maniac: Avg -3.55 BB/hand | Win Rate 0.4% | Total Profit: -10652.0 BB\n",
      "Vs Nit: Avg 0.47 BB/hand | Win Rate 95.6% | Total Profit: 1402.5 BB\n",
      "Vs Random: Avg -0.72 BB/hand | Win Rate 43.6% | Total Profit: -2159.0 BB\n",
      "Vs ValueBot: Avg -0.75 BB/hand | Win Rate 62.6% | Total Profit: -2248.0 BB\n",
      "Vs BluffBot: Avg -0.86 BB/hand | Win Rate 52.7% | Total Profit: -2572.5 BB\n",
      "Vs Balanced: Avg -1.11 BB/hand | Win Rate 49.1% | Total Profit: -3331.5 BB\n"
     ]
    }
   ],
   "source": [
    "# Final Evaluation Mode (Zero Noise)\n",
    "def evaluate_agent(agent, num_hands_per_opp=1000):\n",
    "    print(f\"\\n=== FINAL EVALUATION (Epsilon=0.0) ===\")\n",
    "    # Force pure exploitation\n",
    "    agent.epsilon = 0.0\n",
    "    env = EquityPokerEnv()\n",
    "    \n",
    "    opps = {\n",
    "        'Maniac': ManiacAgent(),\n",
    "        'Nit': NitAgent(),\n",
    "        'Random': RandomAgent(),\n",
    "        'ValueBot': ValueBot(),\n",
    "        'BluffBot': BluffBot(),\n",
    "        'Balanced': BalancedBot()\n",
    "    }\n",
    "    \n",
    "    # We want to give the LSTM time to adapt, so we play a contiguous block\n",
    "    for name, opponent in opps.items():\n",
    "        rewards = []\n",
    "        wins = 0\n",
    "        \n",
    "        # Reset History for new opponent to give clean slate (optional choice)\n",
    "        env.global_history.clear()\n",
    "        for _ in range(MAX_HISTORY_LEN): env.global_history.append(ACT_PAD)\n",
    "        \n",
    "        for _ in range(num_hands_per_opp):\n",
    "            obs, info = env.reset()\n",
    "            done = False\n",
    "            while not done:\n",
    "                if env.state.actor_index == env.agent_player_index:\n",
    "                    action = agent.select_action(obs, info['legal_actions'])\n",
    "                    obs, _, done, _, info = env.step(action)\n",
    "                else:\n",
    "                    action = opponent.select_action(env.state, info['legal_actions'])\n",
    "                    env.update_opponent_history(action)\n",
    "                    env._execute_action(action)\n",
    "                    env._run_automations()\n",
    "                    done = env.state.status is False\n",
    "                    if not done:\n",
    "                        obs = env._get_observation()\n",
    "                        info['legal_actions'] = env._get_legal_actions()\n",
    "            \n",
    "            final_reward = env.get_final_reward()\n",
    "            env.append_outcome_token(final_reward)\n",
    "            rewards.append(final_reward)\n",
    "            if final_reward > 0: wins += 1\n",
    "            \n",
    "        avg_score = np.mean(rewards)\n",
    "        win_rate = wins / num_hands_per_opp\n",
    "        print(f\"Vs {name}: Avg {avg_score:.2f} BB/hand | Win Rate {win_rate:.1%} | Total Profit: {sum(rewards):.1f} BB\")\n",
    "\n",
    "evaluate_agent(agent, 3000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
