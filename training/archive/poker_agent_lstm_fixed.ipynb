{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# LSTM Poker Agent with PokerKit - FIXED VERSION\n",
                "\n",
                "This notebook implements a Deep Recurrent Q-Network (DRQN) agent for No-Limit Texas Hold'em.\n",
                "\n",
                "## Fixes Applied:\n",
                "- **Epsilon decay per HAND** (not per session) - faster learning\n",
                "- **epsilon_min: 0.05** (was 0.1) - more exploitation\n",
                "- **epsilon_decay: 0.9995** per hand (was 0.995 per session)\n",
                "- **Training: 100 sessions x 100 hands = 10,000 hands** (was 1,000)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Using device: cuda\n"
                    ]
                }
            ],
            "source": [
                "# Cell 1: Dependencies & Setup\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "import torch.nn.functional as F\n",
                "import numpy as np\n",
                "import gymnasium as gym\n",
                "from gymnasium import spaces\n",
                "from collections import deque\n",
                "import random\n",
                "import matplotlib.pyplot as plt\n",
                "from typing import Optional, Tuple, List, Dict, Any\n",
                "\n",
                "from pokerkit import Automation, NoLimitTexasHoldem, Card\n",
                "\n",
                "# Constants\n",
                "MAX_HISTORY_LEN = 20\n",
                "SEED = 42\n",
                "\n",
                "# Set random seeds\n",
                "random.seed(SEED)\n",
                "np.random.seed(SEED)\n",
                "torch.manual_seed(SEED)\n",
                "\n",
                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "print(f\"Using device: {device}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 2: Action Encoding\n",
                "\n",
                "# Action tokens for history encoding\n",
                "ACTION_FOLD = 0\n",
                "ACTION_CHECK_CALL = 1\n",
                "ACTION_BET_RAISE = 2\n",
                "ACTION_PAD = 3  # For padding shorter sequences\n",
                "NUM_ACTION_TOKENS = 4\n",
                "\n",
                "# Environment action space\n",
                "ENV_FOLD = 0\n",
                "ENV_CHECK_CALL = 1\n",
                "ENV_BET_RAISE = 2\n",
                "NUM_ACTIONS = 3"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 3: Custom Gym Wrapper for PokerKit\n",
                "\n",
                "class PokerKitGymEnv(gym.Env):\n",
                "    \"\"\"\n",
                "    Gymnasium wrapper for PokerKit's No-Limit Texas Hold'em.\n",
                "    \"\"\"\n",
                "    \n",
                "    def __init__(self, num_players: int = 2, starting_stack: int = 1000, \n",
                "                 small_blind: int = 5, big_blind: int = 10):\n",
                "        super().__init__()\n",
                "        \n",
                "        self.num_players = num_players\n",
                "        self.starting_stack = starting_stack\n",
                "        self.small_blind = small_blind\n",
                "        self.big_blind = big_blind\n",
                "        \n",
                "        self.game_state_dim = 104 + 260 + num_players + 1 + 1 + 4\n",
                "        \n",
                "        self.observation_space = spaces.Dict({\n",
                "            'game_state': spaces.Box(low=0, high=1, shape=(self.game_state_dim,), dtype=np.float32),\n",
                "            'action_history': spaces.Box(low=0, high=NUM_ACTION_TOKENS-1, \n",
                "                                         shape=(MAX_HISTORY_LEN,), dtype=np.int64)\n",
                "        })\n",
                "        \n",
                "        self.action_space = spaces.Discrete(NUM_ACTIONS)\n",
                "        \n",
                "        self.state = None\n",
                "        self.action_history = deque(maxlen=MAX_HISTORY_LEN)\n",
                "        self.agent_player_index = 0\n",
                "        \n",
                "    def _card_to_index(self, card: Card) -> int:\n",
                "        ranks = '23456789TJQKA'\n",
                "        suits = 'cdhs'\n",
                "        rank_idx = ranks.index(card.rank)\n",
                "        suit_idx = suits.index(card.suit)\n",
                "        return rank_idx * 4 + suit_idx\n",
                "    \n",
                "    def _encode_card(self, card: Optional[Card]) -> np.ndarray:\n",
                "        encoding = np.zeros(52, dtype=np.float32)\n",
                "        if card is not None:\n",
                "            encoding[self._card_to_index(card)] = 1.0\n",
                "        return encoding\n",
                "    \n",
                "    def _flatten_cards(self, cards) -> List:\n",
                "        flat = []\n",
                "        for item in cards:\n",
                "            if hasattr(item, 'rank'):\n",
                "                flat.append(item)\n",
                "            else:\n",
                "                flat.extend(self._flatten_cards(item))\n",
                "        return flat\n",
                "    \n",
                "    def _get_game_state(self) -> np.ndarray:\n",
                "        state_vector = []\n",
                "        \n",
                "        hole_cards = self._flatten_cards(self.state.hole_cards[self.agent_player_index])\n",
                "        for i in range(2):\n",
                "            if i < len(hole_cards):\n",
                "                state_vector.extend(self._encode_card(hole_cards[i]))\n",
                "            else:\n",
                "                state_vector.extend(np.zeros(52, dtype=np.float32))\n",
                "        \n",
                "        board_cards = self._flatten_cards(self.state.board_cards)\n",
                "        for i in range(5):\n",
                "            if i < len(board_cards):\n",
                "                state_vector.extend(self._encode_card(board_cards[i]))\n",
                "            else:\n",
                "                state_vector.extend(np.zeros(52, dtype=np.float32))\n",
                "        \n",
                "        for i in range(self.num_players):\n",
                "            stack = self.state.stacks[i] / self.starting_stack\n",
                "            state_vector.append(min(stack, 2.0))\n",
                "        \n",
                "        total_pot = sum(self.state.bets)\n",
                "        state_vector.append(total_pot / (self.starting_stack * self.num_players))\n",
                "        \n",
                "        if self.state.actor_index is not None:\n",
                "            state_vector.append(self.state.actor_index / (self.num_players - 1))\n",
                "        else:\n",
                "            state_vector.append(0.0)\n",
                "        \n",
                "        street = [0.0, 0.0, 0.0, 0.0]\n",
                "        num_board = len(board_cards)\n",
                "        if num_board == 0:\n",
                "            street[0] = 1.0\n",
                "        elif num_board == 3:\n",
                "            street[1] = 1.0\n",
                "        elif num_board == 4:\n",
                "            street[2] = 1.0\n",
                "        else:\n",
                "            street[3] = 1.0\n",
                "        state_vector.extend(street)\n",
                "        \n",
                "        return np.array(state_vector, dtype=np.float32)\n",
                "    \n",
                "    def _get_action_history(self) -> np.ndarray:\n",
                "        history = list(self.action_history)\n",
                "        while len(history) < MAX_HISTORY_LEN:\n",
                "            history.insert(0, ACTION_PAD)\n",
                "        return np.array(history[-MAX_HISTORY_LEN:], dtype=np.int64)\n",
                "    \n",
                "    def _get_observation(self) -> Dict[str, np.ndarray]:\n",
                "        return {\n",
                "            'game_state': self._get_game_state(),\n",
                "            'action_history': self._get_action_history()\n",
                "        }\n",
                "    \n",
                "    def _get_legal_actions(self) -> List[int]:\n",
                "        legal = []\n",
                "        if self.state.can_fold():\n",
                "            legal.append(ENV_FOLD)\n",
                "        if self.state.can_check_or_call():\n",
                "            legal.append(ENV_CHECK_CALL)\n",
                "        if self.state.can_complete_bet_or_raise_to():\n",
                "            legal.append(ENV_BET_RAISE)\n",
                "        return legal if legal else [ENV_CHECK_CALL]\n",
                "    \n",
                "    def _execute_action(self, action: int) -> None:\n",
                "        if action == ENV_FOLD:\n",
                "            if self.state.can_fold():\n",
                "                self.state.fold()\n",
                "            elif self.state.can_check_or_call():\n",
                "                self.state.check_or_call()\n",
                "        elif action == ENV_CHECK_CALL:\n",
                "            if self.state.can_check_or_call():\n",
                "                self.state.check_or_call()\n",
                "            elif self.state.can_fold():\n",
                "                self.state.fold()\n",
                "        elif action == ENV_BET_RAISE:\n",
                "            if self.state.can_complete_bet_or_raise_to():\n",
                "                min_raise = self.state.min_completion_betting_or_raising_to_amount\n",
                "                max_raise = self.state.max_completion_betting_or_raising_to_amount\n",
                "                raise_amount = min(min_raise * 2, max_raise)\n",
                "                self.state.complete_bet_or_raise_to(raise_amount)\n",
                "            elif self.state.can_check_or_call():\n",
                "                self.state.check_or_call()\n",
                "    \n",
                "    def _run_automations(self) -> None:\n",
                "        while self.state.can_burn_card():\n",
                "            self.state.burn_card('??')\n",
                "        while self.state.can_deal_board():\n",
                "            self.state.deal_board()\n",
                "        while self.state.can_push_chips():\n",
                "            self.state.push_chips()\n",
                "        while self.state.can_pull_chips():\n",
                "            self.state.pull_chips()\n",
                "    \n",
                "    def _action_to_token(self, action: int) -> int:\n",
                "        if action == ENV_FOLD:\n",
                "            return ACTION_FOLD\n",
                "        elif action == ENV_CHECK_CALL:\n",
                "            return ACTION_CHECK_CALL\n",
                "        else:\n",
                "            return ACTION_BET_RAISE\n",
                "    \n",
                "    def reset(self, seed=None, options=None) -> Tuple[Dict[str, np.ndarray], Dict]:\n",
                "        super().reset(seed=seed)\n",
                "        \n",
                "        self.state = NoLimitTexasHoldem.create_state(\n",
                "            automations=(\n",
                "                Automation.ANTE_POSTING,\n",
                "                Automation.BET_COLLECTION,\n",
                "                Automation.BLIND_OR_STRADDLE_POSTING,\n",
                "                Automation.HOLE_CARDS_SHOWING_OR_MUCKING,\n",
                "                Automation.HAND_KILLING,\n",
                "                Automation.CHIPS_PUSHING,\n",
                "                Automation.CHIPS_PULLING,\n",
                "            ),\n",
                "            ante_trimming_status=True,\n",
                "            raw_antes={-1: 0},\n",
                "            raw_blinds_or_straddles=(self.small_blind, self.big_blind),\n",
                "            min_bet=self.big_blind,\n",
                "            raw_starting_stacks=[self.starting_stack] * self.num_players,\n",
                "            player_count=self.num_players,\n",
                "        )\n",
                "        \n",
                "        while self.state.can_deal_hole():\n",
                "            self.state.deal_hole()\n",
                "        \n",
                "        self._run_automations()\n",
                "        \n",
                "        return self._get_observation(), {'legal_actions': self._get_legal_actions()}\n",
                "    \n",
                "    def reset_session(self) -> None:\n",
                "        self.action_history.clear()\n",
                "    \n",
                "    def step(self, action: int) -> Tuple[Dict[str, np.ndarray], float, bool, bool, Dict]:\n",
                "        current_player = self.state.actor_index\n",
                "        \n",
                "        if current_player != self.agent_player_index:\n",
                "            self.action_history.append(self._action_to_token(action))\n",
                "        \n",
                "        self._execute_action(action)\n",
                "        self._run_automations()\n",
                "        \n",
                "        done = self.state.status is False\n",
                "        \n",
                "        reward = 0.0\n",
                "        if done:\n",
                "            final_stack = self.state.stacks[self.agent_player_index]\n",
                "            reward = (final_stack - self.starting_stack) / self.big_blind\n",
                "        \n",
                "        obs = self._get_observation()\n",
                "        info = {\n",
                "            'legal_actions': self._get_legal_actions() if not done else [],\n",
                "            'current_player': self.state.actor_index if not done else None\n",
                "        }\n",
                "        \n",
                "        return obs, reward, done, False, info\n",
                "    \n",
                "    def get_current_player(self) -> Optional[int]:\n",
                "        if self.state.status is False:\n",
                "            return None\n",
                "        return self.state.actor_index"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 4: Sequence Replay Buffer\n",
                "\n",
                "class SequenceReplayBuffer:\n",
                "    \"\"\"Replay buffer that stores full episodes with session information.\"\"\"\n",
                "    \n",
                "    def __init__(self, capacity: int = 5000):\n",
                "        self.buffer = deque(maxlen=capacity)\n",
                "    \n",
                "    def push(self, episode: List[Tuple], initial_hidden: Tuple[torch.Tensor, torch.Tensor]):\n",
                "        self.buffer.append((episode, initial_hidden))\n",
                "    \n",
                "    def sample(self, batch_size: int):\n",
                "        batch = random.sample(list(self.buffer), min(batch_size, len(self.buffer)))\n",
                "        return batch\n",
                "    \n",
                "    def __len__(self):\n",
                "        return len(self.buffer)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 5: Dual-Input Agent Model\n",
                "\n",
                "class DualInputAgent(nn.Module):\n",
                "    \"\"\"DRQN Agent with dual-input architecture.\"\"\"\n",
                "    \n",
                "    def __init__(self, game_state_dim: int, action_vocab_size: int = NUM_ACTION_TOKENS,\n",
                "                 hidden_dim: int = 128, embed_dim: int = 32, num_actions: int = NUM_ACTIONS):\n",
                "        super().__init__()\n",
                "        \n",
                "        self.hidden_dim = hidden_dim\n",
                "        \n",
                "        # MLP Branch for game state\n",
                "        self.mlp = nn.Sequential(\n",
                "            nn.Linear(game_state_dim, 256),\n",
                "            nn.ReLU(),\n",
                "            nn.Linear(256, 128),\n",
                "            nn.ReLU(),\n",
                "            nn.Linear(128, hidden_dim),\n",
                "            nn.ReLU()\n",
                "        )\n",
                "        \n",
                "        # LSTM Branch for action history\n",
                "        self.action_embedding = nn.Embedding(action_vocab_size, embed_dim, padding_idx=ACTION_PAD)\n",
                "        self.lstm = nn.LSTM(input_size=embed_dim, hidden_size=hidden_dim, \n",
                "                           num_layers=1, batch_first=True)\n",
                "        \n",
                "        # Output head\n",
                "        self.output_head = nn.Sequential(\n",
                "            nn.Linear(hidden_dim * 2, 128),\n",
                "            nn.ReLU(),\n",
                "            nn.Linear(128, num_actions)\n",
                "        )\n",
                "    \n",
                "    def forward(self, game_state: torch.Tensor, history_seq: torch.Tensor, \n",
                "                hidden: Optional[Tuple[torch.Tensor, torch.Tensor]] = None) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n",
                "        batch_size = game_state.size(0)\n",
                "        \n",
                "        mlp_out = self.mlp(game_state)\n",
                "        action_embeds = self.action_embedding(history_seq)\n",
                "        \n",
                "        if hidden is None:\n",
                "            hidden = self.init_hidden(batch_size, game_state.device)\n",
                "        \n",
                "        lstm_out, new_hidden = self.lstm(action_embeds, hidden)\n",
                "        lstm_out = lstm_out[:, -1, :]\n",
                "        \n",
                "        combined = torch.cat([mlp_out, lstm_out], dim=1)\n",
                "        q_values = self.output_head(combined)\n",
                "        \n",
                "        return q_values, new_hidden\n",
                "    \n",
                "    def init_hidden(self, batch_size: int, device: torch.device) -> Tuple[torch.Tensor, torch.Tensor]:\n",
                "        h = torch.zeros(1, batch_size, self.hidden_dim, device=device)\n",
                "        c = torch.zeros(1, batch_size, self.hidden_dim, device=device)\n",
                "        return (h, c)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 6: Fixed-Strategy Opponents\n",
                "\n",
                "class ManiacAgent:\n",
                "    \"\"\"Always raises/bets when possible, otherwise calls.\"\"\"\n",
                "    \n",
                "    def select_action(self, legal_actions: List[int]) -> int:\n",
                "        if ENV_BET_RAISE in legal_actions:\n",
                "            return ENV_BET_RAISE\n",
                "        if ENV_CHECK_CALL in legal_actions:\n",
                "            return ENV_CHECK_CALL\n",
                "        return ENV_FOLD\n",
                "\n",
                "\n",
                "class NitAgent:\n",
                "    \"\"\"Always folds when facing aggression, checks/calls otherwise.\"\"\"\n",
                "    \n",
                "    def __init__(self):\n",
                "        self.facing_raise = False\n",
                "    \n",
                "    def select_action(self, legal_actions: List[int]) -> int:\n",
                "        if ENV_FOLD in legal_actions and ENV_CHECK_CALL in legal_actions:\n",
                "            if random.random() < 0.8:\n",
                "                return ENV_FOLD\n",
                "        if ENV_CHECK_CALL in legal_actions:\n",
                "            return ENV_CHECK_CALL\n",
                "        return ENV_FOLD\n",
                "\n",
                "\n",
                "class RandomAgent:\n",
                "    \"\"\"Randomly selects from legal actions.\"\"\"\n",
                "    \n",
                "    def select_action(self, legal_actions: List[int]) -> int:\n",
                "        return random.choice(legal_actions)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 7: Agent Wrapper with Epsilon-Greedy - FIXED VERSION\n",
                "\n",
                "class DRQNAgentWrapper:\n",
                "    \"\"\"\n",
                "    Wrapper for training the DualInputAgent with DQN.\n",
                "    \n",
                "    FIXED: Epsilon decay now happens per hand, not per session.\n",
                "    \"\"\"\n",
                "    \n",
                "    def __init__(self, game_state_dim: int, hidden_dim: int = 128,\n",
                "                 lr: float = 1e-3, gamma: float = 0.99,\n",
                "                 epsilon_start: float = 1.0, epsilon_min: float = 0.05,  # FIXED: lower min\n",
                "                 epsilon_decay: float = 0.9995):  # FIXED: per-hand decay rate\n",
                "        \n",
                "        self.model = DualInputAgent(game_state_dim, hidden_dim=hidden_dim).to(device)\n",
                "        self.target_model = DualInputAgent(game_state_dim, hidden_dim=hidden_dim).to(device)\n",
                "        self.target_model.load_state_dict(self.model.state_dict())\n",
                "        \n",
                "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
                "        self.gamma = gamma\n",
                "        self.epsilon = epsilon_start\n",
                "        self.epsilon_min = epsilon_min\n",
                "        self.epsilon_decay = epsilon_decay\n",
                "        \n",
                "        self.current_hidden = None\n",
                "    \n",
                "    def start_new_session(self):\n",
                "        self.current_hidden = None\n",
                "        print(\"[Session Reset] LSTM hidden state cleared.\")\n",
                "    \n",
                "    def get_hidden_for_episode(self) -> Tuple[torch.Tensor, torch.Tensor]:\n",
                "        if self.current_hidden is None:\n",
                "            return self.model.init_hidden(1, device)\n",
                "        return (self.current_hidden[0].clone(), self.current_hidden[1].clone())\n",
                "    \n",
                "    def select_action(self, obs: Dict[str, np.ndarray], legal_actions: List[int],\n",
                "                      eval_mode: bool = False) -> int:\n",
                "        if not eval_mode and random.random() < self.epsilon:\n",
                "            return random.choice(legal_actions)\n",
                "        \n",
                "        game_state = torch.FloatTensor(obs['game_state']).unsqueeze(0).to(device)\n",
                "        history = torch.LongTensor(obs['action_history']).unsqueeze(0).to(device)\n",
                "        \n",
                "        with torch.no_grad():\n",
                "            q_values, self.current_hidden = self.model(game_state, history, self.current_hidden)\n",
                "        \n",
                "        q_values = q_values[0].cpu().numpy()\n",
                "        masked_q = np.full(NUM_ACTIONS, -np.inf)\n",
                "        for a in legal_actions:\n",
                "            masked_q[a] = q_values[a]\n",
                "        \n",
                "        return int(np.argmax(masked_q))\n",
                "    \n",
                "    def update_epsilon(self):\n",
                "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
                "    \n",
                "    def update_target_network(self):\n",
                "        self.target_model.load_state_dict(self.model.state_dict())\n",
                "    \n",
                "    def train_step(self, buffer: SequenceReplayBuffer, batch_size: int = 32) -> Optional[float]:\n",
                "        if len(buffer) < batch_size:\n",
                "            return None\n",
                "        \n",
                "        batch = buffer.sample(batch_size)\n",
                "        total_loss = 0.0\n",
                "        num_transitions = 0\n",
                "        \n",
                "        for episode, initial_hidden in batch:\n",
                "            if len(episode) == 0:\n",
                "                continue\n",
                "            \n",
                "            hidden = (initial_hidden[0].to(device), initial_hidden[1].to(device))\n",
                "            \n",
                "            for obs, action, reward, next_obs, done, legal_actions in episode:\n",
                "                game_state = torch.FloatTensor(obs['game_state']).unsqueeze(0).to(device)\n",
                "                history = torch.LongTensor(obs['action_history']).unsqueeze(0).to(device)\n",
                "                \n",
                "                q_values, hidden = self.model(game_state, history, hidden)\n",
                "                q_value = q_values[0, action]\n",
                "                \n",
                "                if done:\n",
                "                    target = reward\n",
                "                else:\n",
                "                    next_game_state = torch.FloatTensor(next_obs['game_state']).unsqueeze(0).to(device)\n",
                "                    next_history = torch.LongTensor(next_obs['action_history']).unsqueeze(0).to(device)\n",
                "                    \n",
                "                    with torch.no_grad():\n",
                "                        next_q_values, _ = self.target_model(next_game_state, next_history, hidden)\n",
                "                        target = reward + self.gamma * next_q_values.max().item()\n",
                "                \n",
                "                loss = F.mse_loss(q_value, torch.tensor(target, device=device))\n",
                "                total_loss += loss\n",
                "                num_transitions += 1\n",
                "        \n",
                "        if num_transitions > 0:\n",
                "            avg_loss = total_loss / num_transitions\n",
                "            \n",
                "            self.optimizer.zero_grad()\n",
                "            avg_loss.backward()\n",
                "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
                "            self.optimizer.step()\n",
                "            \n",
                "            return avg_loss.item()\n",
                "        \n",
                "        return None"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Starting training...\n",
                        "Sessions: 100, Hands per session: 100\n",
                        "Total hands: 10000\n",
                        "\n",
                        "[Session Reset] LSTM hidden state cleared.\n",
                        "Session 1/100 | Opponent: maniac | Avg Reward: -8.31 | Epsilon: 0.951\n",
                        "[Session Reset] LSTM hidden state cleared.\n",
                        "Session 2/100 | Opponent: maniac | Avg Reward: -10.97 | Epsilon: 0.905\n",
                        "[Session Reset] LSTM hidden state cleared.\n",
                        "Session 3/100 | Opponent: maniac | Avg Reward: -9.89 | Epsilon: 0.861\n",
                        "[Session Reset] LSTM hidden state cleared.\n",
                        "Session 4/100 | Opponent: maniac | Avg Reward: -13.21 | Epsilon: 0.819\n",
                        "[Session Reset] LSTM hidden state cleared.\n",
                        "Session 5/100 | Opponent: maniac | Avg Reward: -8.22 | Epsilon: 0.779\n",
                        "[Session Reset] LSTM hidden state cleared.\n",
                        "Session 6/100 | Opponent: maniac | Avg Reward: -9.41 | Epsilon: 0.741\n",
                        "[Session Reset] LSTM hidden state cleared.\n",
                        "Session 7/100 | Opponent: maniac | Avg Reward: -7.21 | Epsilon: 0.705\n",
                        "[Session Reset] LSTM hidden state cleared.\n",
                        "Session 8/100 | Opponent: maniac | Avg Reward: -12.17 | Epsilon: 0.670\n",
                        "[Session Reset] LSTM hidden state cleared.\n",
                        "Session 9/100 | Opponent: maniac | Avg Reward: -12.24 | Epsilon: 0.638\n",
                        "[Session Reset] LSTM hidden state cleared.\n",
                        "Session 10/100 | Opponent: maniac | Avg Reward: -8.75 | Epsilon: 0.606\n",
                        "[Session Reset] LSTM hidden state cleared.\n",
                        "Session 11/100 | Opponent: maniac | Avg Reward: -8.61 | Epsilon: 0.577\n",
                        "[Session Reset] LSTM hidden state cleared.\n",
                        "Session 12/100 | Opponent: maniac | Avg Reward: -2.33 | Epsilon: 0.549\n",
                        "[Session Reset] LSTM hidden state cleared.\n",
                        "Session 13/100 | Opponent: maniac | Avg Reward: -7.95 | Epsilon: 0.522\n",
                        "[Session Reset] LSTM hidden state cleared.\n",
                        "Session 14/100 | Opponent: maniac | Avg Reward: -4.35 | Epsilon: 0.496\n",
                        "[Session Reset] LSTM hidden state cleared.\n",
                        "Session 15/100 | Opponent: maniac | Avg Reward: -3.13 | Epsilon: 0.472\n",
                        "[Session Reset] LSTM hidden state cleared.\n",
                        "Session 16/100 | Opponent: maniac | Avg Reward: -7.12 | Epsilon: 0.449\n",
                        "[Session Reset] LSTM hidden state cleared.\n",
                        "Session 17/100 | Opponent: maniac | Avg Reward: -3.72 | Epsilon: 0.427\n",
                        "[Session Reset] LSTM hidden state cleared.\n",
                        "Session 18/100 | Opponent: maniac | Avg Reward: -1.71 | Epsilon: 0.406\n",
                        "[Session Reset] LSTM hidden state cleared.\n",
                        "Session 19/100 | Opponent: maniac | Avg Reward: 0.33 | Epsilon: 0.387\n",
                        "[Session Reset] LSTM hidden state cleared.\n",
                        "Session 20/100 | Opponent: maniac | Avg Reward: -12.77 | Epsilon: 0.368\n",
                        "[Session Reset] LSTM hidden state cleared.\n",
                        "Session 21/100 | Opponent: maniac | Avg Reward: -6.09 | Epsilon: 0.350\n",
                        "[Session Reset] LSTM hidden state cleared.\n",
                        "Session 22/100 | Opponent: maniac | Avg Reward: -1.48 | Epsilon: 0.333\n",
                        "[Session Reset] LSTM hidden state cleared.\n",
                        "Session 23/100 | Opponent: maniac | Avg Reward: -5.42 | Epsilon: 0.317\n",
                        "[Session Reset] LSTM hidden state cleared.\n",
                        "Session 24/100 | Opponent: maniac | Avg Reward: -8.16 | Epsilon: 0.301\n",
                        "[Session Reset] LSTM hidden state cleared.\n",
                        "Session 25/100 | Opponent: maniac | Avg Reward: -7.34 | Epsilon: 0.286\n",
                        "[Session Reset] LSTM hidden state cleared.\n",
                        "Session 26/100 | Opponent: maniac | Avg Reward: -1.86 | Epsilon: 0.272\n",
                        "[Session Reset] LSTM hidden state cleared.\n",
                        "Session 27/100 | Opponent: maniac | Avg Reward: -0.96 | Epsilon: 0.259\n",
                        "[Session Reset] LSTM hidden state cleared.\n",
                        "Session 28/100 | Opponent: maniac | Avg Reward: -3.50 | Epsilon: 0.247\n",
                        "[Session Reset] LSTM hidden state cleared.\n",
                        "Session 29/100 | Opponent: maniac | Avg Reward: -13.65 | Epsilon: 0.234\n",
                        "[Session Reset] LSTM hidden state cleared.\n",
                        "Session 30/100 | Opponent: maniac | Avg Reward: -2.48 | Epsilon: 0.223\n",
                        "[Session Reset] LSTM hidden state cleared.\n",
                        "Session 31/100 | Opponent: maniac | Avg Reward: -6.12 | Epsilon: 0.212\n",
                        "[Session Reset] LSTM hidden state cleared.\n",
                        "Session 32/100 | Opponent: maniac | Avg Reward: -1.93 | Epsilon: 0.202\n",
                        "[Session Reset] LSTM hidden state cleared.\n",
                        "Session 33/100 | Opponent: maniac | Avg Reward: -6.10 | Epsilon: 0.192\n",
                        "[Session Reset] LSTM hidden state cleared.\n",
                        "Session 34/100 | Opponent: maniac | Avg Reward: -5.97 | Epsilon: 0.183\n",
                        "[Session Reset] LSTM hidden state cleared.\n",
                        "Session 35/100 | Opponent: maniac | Avg Reward: -5.84 | Epsilon: 0.174\n",
                        "[Session Reset] LSTM hidden state cleared.\n",
                        "Session 36/100 | Opponent: maniac | Avg Reward: -5.65 | Epsilon: 0.165\n",
                        "[Session Reset] LSTM hidden state cleared.\n",
                        "Session 37/100 | Opponent: maniac | Avg Reward: -5.67 | Epsilon: 0.157\n",
                        "[Session Reset] LSTM hidden state cleared.\n",
                        "Session 38/100 | Opponent: maniac | Avg Reward: -4.68 | Epsilon: 0.149\n",
                        "[Session Reset] LSTM hidden state cleared.\n",
                        "Session 39/100 | Opponent: maniac | Avg Reward: 1.43 | Epsilon: 0.142\n",
                        "[Session Reset] LSTM hidden state cleared.\n",
                        "Session 40/100 | Opponent: maniac | Avg Reward: -4.23 | Epsilon: 0.135\n",
                        "[Session Reset] LSTM hidden state cleared.\n",
                        "Session 41/100 | Opponent: maniac | Avg Reward: -2.11 | Epsilon: 0.129\n",
                        "[Session Reset] LSTM hidden state cleared.\n",
                        "Session 42/100 | Opponent: maniac | Avg Reward: -5.31 | Epsilon: 0.122\n",
                        "[Session Reset] LSTM hidden state cleared.\n",
                        "Session 43/100 | Opponent: maniac | Avg Reward: -0.84 | Epsilon: 0.116\n",
                        "[Session Reset] LSTM hidden state cleared.\n",
                        "Session 44/100 | Opponent: maniac | Avg Reward: -5.51 | Epsilon: 0.111\n",
                        "[Session Reset] LSTM hidden state cleared.\n",
                        "Session 45/100 | Opponent: maniac | Avg Reward: -4.95 | Epsilon: 0.105\n",
                        "[Session Reset] LSTM hidden state cleared.\n",
                        "Session 46/100 | Opponent: maniac | Avg Reward: -4.96 | Epsilon: 0.100\n",
                        "[Session Reset] LSTM hidden state cleared.\n",
                        "Session 47/100 | Opponent: maniac | Avg Reward: -1.23 | Epsilon: 0.095\n",
                        "[Session Reset] LSTM hidden state cleared.\n",
                        "Session 48/100 | Opponent: maniac | Avg Reward: -4.47 | Epsilon: 0.091\n",
                        "[Session Reset] LSTM hidden state cleared.\n",
                        "Session 49/100 | Opponent: maniac | Avg Reward: -6.53 | Epsilon: 0.086\n",
                        "[Session Reset] LSTM hidden state cleared.\n",
                        "Session 50/100 | Opponent: maniac | Avg Reward: -5.69 | Epsilon: 0.082\n",
                        "[Session Reset] LSTM hidden state cleared.\n",
                        "Session 51/100 | Opponent: nit | Avg Reward: 0.00 | Epsilon: 0.078\n",
                        "[Session Reset] LSTM hidden state cleared.\n",
                        "Session 52/100 | Opponent: nit | Avg Reward: 0.00 | Epsilon: 0.074\n",
                        "[Session Reset] LSTM hidden state cleared.\n",
                        "Session 53/100 | Opponent: nit | Avg Reward: 0.00 | Epsilon: 0.071\n",
                        "[Session Reset] LSTM hidden state cleared.\n",
                        "Session 54/100 | Opponent: nit | Avg Reward: 0.00 | Epsilon: 0.067\n",
                        "[Session Reset] LSTM hidden state cleared.\n",
                        "Session 55/100 | Opponent: nit | Avg Reward: 0.00 | Epsilon: 0.064\n",
                        "[Session Reset] LSTM hidden state cleared.\n",
                        "Session 56/100 | Opponent: nit | Avg Reward: 0.00 | Epsilon: 0.061\n",
                        "[Session Reset] LSTM hidden state cleared.\n"
                    ]
                },
                {
                    "ename": "KeyboardInterrupt",
                    "evalue": "",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
                        "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
                        "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 102\u001b[39m\n\u001b[32m     99\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m agent, rewards_history, session_rewards\n\u001b[32m    101\u001b[39m \u001b[38;5;66;03m# Run training with sufficient iterations\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m102\u001b[39m agent, rewards_history, session_rewards = \u001b[43mtrain_agent\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    103\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_sessions\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    104\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhands_per_session\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\n\u001b[32m    105\u001b[39m \u001b[43m)\u001b[49m\n",
                        "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 82\u001b[39m, in \u001b[36mtrain_agent\u001b[39m\u001b[34m(num_sessions, hands_per_session, batch_size, target_update_freq)\u001b[39m\n\u001b[32m     80\u001b[39m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(buffer) >= batch_size:\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m     \u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     84\u001b[39m \u001b[38;5;66;03m# FIXED: Update epsilon after each hand (not session)\u001b[39;00m\n\u001b[32m     85\u001b[39m agent.update_epsilon()\n",
                        "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 78\u001b[39m, in \u001b[36mDRQNAgentWrapper.train_step\u001b[39m\u001b[34m(self, buffer, batch_size)\u001b[39m\n\u001b[32m     75\u001b[39m game_state = torch.FloatTensor(obs[\u001b[33m'\u001b[39m\u001b[33mgame_state\u001b[39m\u001b[33m'\u001b[39m]).unsqueeze(\u001b[32m0\u001b[39m).to(device)\n\u001b[32m     76\u001b[39m history = torch.LongTensor(obs[\u001b[33m'\u001b[39m\u001b[33maction_history\u001b[39m\u001b[33m'\u001b[39m]).unsqueeze(\u001b[32m0\u001b[39m).to(device)\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m q_values, hidden = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgame_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     79\u001b[39m q_value = q_values[\u001b[32m0\u001b[39m, action]\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m done:\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/repos/poker-bot-test/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1776\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/repos/poker-bot-test/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1787\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1786\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1787\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1789\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1790\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
                        "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 44\u001b[39m, in \u001b[36mDualInputAgent.forward\u001b[39m\u001b[34m(self, game_state, history_seq, hidden)\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m hidden \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     42\u001b[39m     hidden = \u001b[38;5;28mself\u001b[39m.init_hidden(batch_size, game_state.device)\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m lstm_out, new_hidden = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction_embeds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     45\u001b[39m lstm_out = lstm_out[:, -\u001b[32m1\u001b[39m, :]\n\u001b[32m     47\u001b[39m combined = torch.cat([mlp_out, lstm_out], dim=\u001b[32m1\u001b[39m)\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/repos/poker-bot-test/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1776\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/repos/poker-bot-test/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1787\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1786\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1787\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1789\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1790\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/repos/poker-bot-test/.venv/lib/python3.12/site-packages/torch/nn/modules/rnn.py:1141\u001b[39m, in \u001b[36mLSTM.forward\u001b[39m\u001b[34m(self, input, hx)\u001b[39m\n\u001b[32m   1138\u001b[39m         hx = \u001b[38;5;28mself\u001b[39m.permute_hidden(hx, sorted_indices)\n\u001b[32m   1140\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1141\u001b[39m     result = \u001b[43m_VF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1142\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1143\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1144\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   1145\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1146\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1147\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1148\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1149\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1150\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbatch_first\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1151\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1152\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1153\u001b[39m     result = _VF.lstm(\n\u001b[32m   1154\u001b[39m         \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   1155\u001b[39m         batch_sizes,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1162\u001b[39m         \u001b[38;5;28mself\u001b[39m.bidirectional,\n\u001b[32m   1163\u001b[39m     )\n",
                        "\u001b[31mKeyboardInterrupt\u001b[39m: "
                    ]
                }
            ],
            "source": [
                "# Cell 8: Training Loop - FIXED VERSION\n",
                "\"\"\"\n",
                "CRITICAL FIXES:\n",
                "1. Epsilon decay per HAND (not per session)\n",
                "2. More training iterations (10,000 hands instead of 1,000)\n",
                "\"\"\"\n",
                "\n",
                "def train_agent(num_sessions: int = 100, hands_per_session: int = 100,\n",
                "                batch_size: int = 16, target_update_freq: int = 10):\n",
                "    \"\"\"\n",
                "    Train the DRQN agent with session-based LSTM state management.\n",
                "    FIXED: Epsilon now updates per hand, not per session.\n",
                "    \"\"\"\n",
                "    \n",
                "    env = PokerKitGymEnv(num_players=2)\n",
                "    agent = DRQNAgentWrapper(game_state_dim=env.game_state_dim)\n",
                "    buffer = SequenceReplayBuffer(capacity=5000)\n",
                "    \n",
                "    opponents = {\n",
                "        'maniac': ManiacAgent(),\n",
                "        'nit': NitAgent(),\n",
                "        'random': RandomAgent()\n",
                "    }\n",
                "    \n",
                "    rewards_history = []\n",
                "    session_rewards = []\n",
                "    \n",
                "    print(\"Starting training...\")\n",
                "    print(f\"Sessions: {num_sessions}, Hands per session: {hands_per_session}\")\n",
                "    print(f\"Total hands: {num_sessions * hands_per_session}\")\n",
                "    print()\n",
                "    \n",
                "    for session in range(num_sessions):\n",
                "        if session < num_sessions // 2:\n",
                "            opponent_name = 'maniac'\n",
                "        else:\n",
                "            opponent_name = 'nit'\n",
                "        \n",
                "        opponent = opponents[opponent_name]\n",
                "        \n",
                "        agent.start_new_session()\n",
                "        env.reset_session()\n",
                "        \n",
                "        session_reward = 0.0\n",
                "        \n",
                "        for hand in range(hands_per_session):\n",
                "            initial_hidden = agent.get_hidden_for_episode()\n",
                "            obs, info = env.reset()\n",
                "            episode_transitions = []\n",
                "            done = False\n",
                "            \n",
                "            while not done:\n",
                "                current_player = env.get_current_player()\n",
                "                legal_actions = info['legal_actions']\n",
                "                \n",
                "                if current_player is None:\n",
                "                    break\n",
                "                \n",
                "                if current_player == env.agent_player_index:\n",
                "                    action = agent.select_action(obs, legal_actions)\n",
                "                    next_obs, reward, done, truncated, info = env.step(action)\n",
                "                    \n",
                "                    episode_transitions.append((\n",
                "                        obs, action, reward, next_obs, done,\n",
                "                        info.get('legal_actions', [])\n",
                "                    ))\n",
                "                    \n",
                "                    obs = next_obs\n",
                "                else:\n",
                "                    opp_action = opponent.select_action(legal_actions)\n",
                "                    obs, reward, done, truncated, info = env.step(opp_action)\n",
                "            \n",
                "            if episode_transitions:\n",
                "                buffer.push(episode_transitions, initial_hidden)\n",
                "                \n",
                "                final_reward = episode_transitions[-1][2]\n",
                "                rewards_history.append(final_reward)\n",
                "                session_reward += final_reward\n",
                "            \n",
                "            # Train\n",
                "            if len(buffer) >= batch_size:\n",
                "                agent.train_step(buffer, batch_size)\n",
                "            \n",
                "            # FIXED: Update epsilon after each hand (not session)\n",
                "            agent.update_epsilon()\n",
                "        \n",
                "        # Update target network\n",
                "        if session % target_update_freq == 0:\n",
                "            agent.update_target_network()\n",
                "        \n",
                "        avg_session_reward = session_reward / hands_per_session\n",
                "        session_rewards.append(avg_session_reward)\n",
                "        \n",
                "        print(f\"Session {session+1}/{num_sessions} | Opponent: {opponent_name} | \"\n",
                "              f\"Avg Reward: {avg_session_reward:.2f} | Epsilon: {agent.epsilon:.3f}\")\n",
                "    \n",
                "    print(\"\\nTraining complete!\")\n",
                "    \n",
                "    return agent, rewards_history, session_rewards\n",
                "\n",
                "# Run training with sufficient iterations\n",
                "agent, rewards_history, session_rewards = train_agent(\n",
                "    num_sessions=100, \n",
                "    hands_per_session=100\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "ename": "NameError",
                    "evalue": "name 'rewards_history' is not defined",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
                        "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
                        "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m fig, axes = plt.subplots(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m, figsize=(\u001b[32m14\u001b[39m, \u001b[32m5\u001b[39m))\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Plot 1: Per-hand rewards\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[43mrewards_history\u001b[49m) > \u001b[32m10\u001b[39m:\n\u001b[32m     10\u001b[39m     ma_rewards = moving_average(rewards_history, window=\u001b[32m100\u001b[39m)\n\u001b[32m     11\u001b[39m     axes[\u001b[32m0\u001b[39m].plot(ma_rewards, alpha=\u001b[32m0.8\u001b[39m)\n",
                        "\u001b[31mNameError\u001b[39m: name 'rewards_history' is not defined"
                    ]
                },
                {
                    "data": {
                        "image/png": "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",
                        "text/plain": [
                            "<Figure size 1400x500 with 2 Axes>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "# Cell 9: Visualization\n",
                "\n",
                "def moving_average(data, window=10):\n",
                "    return np.convolve(data, np.ones(window)/window, mode='valid')\n",
                "\n",
                "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "# Plot 1: Per-hand rewards\n",
                "if len(rewards_history) > 10:\n",
                "    ma_rewards = moving_average(rewards_history, window=100)\n",
                "    axes[0].plot(ma_rewards, alpha=0.8)\n",
                "else:\n",
                "    axes[0].plot(rewards_history)\n",
                "axes[0].axvline(x=len(rewards_history)//2, color='r', linestyle='--', label='Opponent Switch')\n",
                "axes[0].set_title('Per-Hand Reward (Moving Average)')\n",
                "axes[0].set_xlabel('Hand')\n",
                "axes[0].set_ylabel('Reward (BBs)')\n",
                "axes[0].legend()\n",
                "axes[0].grid(True, alpha=0.3)\n",
                "\n",
                "# Plot 2: Per-session rewards\n",
                "axes[1].plot(session_rewards, 'o-', markersize=4)\n",
                "axes[1].axvline(x=len(session_rewards)//2, color='r', linestyle='--', label='Opponent Switch')\n",
                "axes[1].set_title('Average Session Reward')\n",
                "axes[1].set_xlabel('Session')\n",
                "axes[1].set_ylabel('Avg Reward (BBs)')\n",
                "axes[1].legend()\n",
                "axes[1].grid(True, alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 10: Evaluation Function\n",
                "\n",
                "def evaluate_agent(agent: DRQNAgentWrapper, num_hands: int = 100, \n",
                "                   opponent_type: str = 'random') -> Dict:\n",
                "    \"\"\"Evaluate agent against a specific opponent type.\"\"\"\n",
                "    env = PokerKitGymEnv(num_players=2)\n",
                "    \n",
                "    opponents = {\n",
                "        'maniac': ManiacAgent(),\n",
                "        'nit': NitAgent(),\n",
                "        'random': RandomAgent()\n",
                "    }\n",
                "    opponent = opponents[opponent_type]\n",
                "    \n",
                "    agent.start_new_session()\n",
                "    env.reset_session()\n",
                "    \n",
                "    wins = 0\n",
                "    losses = 0\n",
                "    ties = 0\n",
                "    total_reward = 0.0\n",
                "    \n",
                "    for hand in range(num_hands):\n",
                "        obs, info = env.reset()\n",
                "        done = False\n",
                "        hand_reward = 0.0\n",
                "        \n",
                "        while not done:\n",
                "            current_player = env.get_current_player()\n",
                "            legal_actions = info['legal_actions']\n",
                "            \n",
                "            if current_player is None:\n",
                "                break\n",
                "            \n",
                "            if current_player == env.agent_player_index:\n",
                "                action = agent.select_action(obs, legal_actions, eval_mode=True)\n",
                "            else:\n",
                "                action = opponent.select_action(legal_actions)\n",
                "            \n",
                "            obs, reward, done, truncated, info = env.step(action)\n",
                "            \n",
                "            if current_player == env.agent_player_index:\n",
                "                hand_reward = reward\n",
                "        \n",
                "        total_reward += hand_reward\n",
                "        if hand_reward > 0:\n",
                "            wins += 1\n",
                "        elif hand_reward < 0:\n",
                "            losses += 1\n",
                "        else:\n",
                "            ties += 1\n",
                "    \n",
                "    results = {\n",
                "        'opponent': opponent_type,\n",
                "        'hands_played': num_hands,\n",
                "        'wins': wins,\n",
                "        'losses': losses,\n",
                "        'ties': ties,\n",
                "        'win_rate': wins / num_hands,\n",
                "        'total_reward': total_reward,\n",
                "        'avg_reward': total_reward / num_hands\n",
                "    }\n",
                "    \n",
                "    print(f\"\\n=== Evaluation vs {opponent_type.upper()} ===\")\n",
                "    print(f\"Hands: {num_hands}\")\n",
                "    print(f\"W/L/T: {wins}/{losses}/{ties}\")\n",
                "    print(f\"Win Rate: {results['win_rate']:.1%}\")\n",
                "    print(f\"Total Profit: {total_reward:.1f} BBs\")\n",
                "    print(f\"Avg Profit/Hand: {results['avg_reward']:.2f} BBs\")\n",
                "    \n",
                "    return results\n",
                "\n",
                "# Evaluate against each opponent type\n",
                "print(\"\\nEvaluating trained agent...\")\n",
                "for opp in ['random', 'maniac', 'nit']:\n",
                "    evaluate_agent(agent, num_hands=100, opponent_type=opp)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Summary of Fixes\n",
                "\n",
                "The original notebook had these issues:\n",
                "\n",
                "1. **Epsilon decay too slow**: Decayed per session (not per hand), so after 20 sessions epsilon was still ~0.9\n",
                "2. **Insufficient training**: Only 1,000 hands (20 sessions  50 hands)\n",
                "3. **High epsilon minimum**: 0.1 meant 10% random actions even after training\n",
                "\n",
                "### Changes Made:\n",
                "\n",
                "| Parameter | Before | After |\n",
                "|-----------|--------|-------|\n",
                "| `epsilon_min` | 0.1 | 0.05 |\n",
                "| `epsilon_decay` | 0.995 (per session) | 0.9995 (per hand) |\n",
                "| `num_sessions` | 20 | 100 |\n",
                "| `hands_per_session` | 50 | 100 |\n",
                "| **Total hands** | 1,000 | 10,000 |\n",
                "\n",
                "With these changes, epsilon will decay to ~0.05 after about 6,000 hands, allowing the agent to actually exploit what it learned."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
