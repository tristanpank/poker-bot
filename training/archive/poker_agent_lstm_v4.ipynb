{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# LSTM Poker Agent V4 - Fixed Reward Collection + Extended Training\n",
                "\n",
                "## Fixes from V3:\n",
                "1. **Reward collection fixed** - now properly captures reward when opponent folds\n",
                "2. **State update fixed** - state is updated after opponent acts\n",
                "3. **Extended training** - 150 sessions (15,000 hands) for better maniac performance\n",
                "\n",
                "## Architecture:\n",
                "- DRQN: LSTM processes full state sequences\n",
                "- Session-based training with LSTM state persistence\n",
                "- Per-hand epsilon decay"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Using device: cuda\n"
                    ]
                }
            ],
            "source": [
                "# Cell 1: Dependencies & Setup\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "import torch.nn.functional as F\n",
                "import numpy as np\n",
                "import gymnasium as gym\n",
                "from gymnasium import spaces\n",
                "from collections import deque\n",
                "import random\n",
                "import matplotlib.pyplot as plt\n",
                "from typing import Optional, Tuple, List, Dict, Any\n",
                "\n",
                "from pokerkit import Automation, NoLimitTexasHoldem, Card\n",
                "\n",
                "# Constants\n",
                "SEED = 42\n",
                "\n",
                "# Set random seeds\n",
                "random.seed(SEED)\n",
                "np.random.seed(SEED)\n",
                "torch.manual_seed(SEED)\n",
                "\n",
                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "print(f\"Using device: {device}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 2: Action Space\n",
                "ENV_FOLD = 0\n",
                "ENV_CHECK_CALL = 1\n",
                "ENV_BET_RAISE = 2\n",
                "NUM_ACTIONS = 3"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 3: Custom Gym Wrapper for PokerKit\n",
                "\n",
                "class PokerKitGymEnv(gym.Env):\n",
                "    \"\"\"Gymnasium wrapper for PokerKit's No-Limit Texas Hold'em.\"\"\"\n",
                "    \n",
                "    def __init__(self, num_players: int = 2, starting_stack: int = 1000, \n",
                "                 small_blind: int = 5, big_blind: int = 10):\n",
                "        super().__init__()\n",
                "        \n",
                "        self.num_players = num_players\n",
                "        self.starting_stack = starting_stack\n",
                "        self.small_blind = small_blind\n",
                "        self.big_blind = big_blind\n",
                "        \n",
                "        self.game_state_dim = 52*2 + 52*5 + num_players + 1 + 1 + 4  # 372\n",
                "        \n",
                "        self.observation_space = spaces.Box(\n",
                "            low=0, high=1, shape=(self.game_state_dim,), dtype=np.float32\n",
                "        )\n",
                "        self.action_space = spaces.Discrete(NUM_ACTIONS)\n",
                "        \n",
                "        self.state = None\n",
                "        self.agent_player_index = 0\n",
                "        \n",
                "    def _card_to_index(self, card: Card) -> int:\n",
                "        ranks = '23456789TJQKA'\n",
                "        suits = 'cdhs'\n",
                "        rank_idx = ranks.index(card.rank)\n",
                "        suit_idx = suits.index(card.suit)\n",
                "        return rank_idx * 4 + suit_idx\n",
                "    \n",
                "    def _encode_card(self, card: Optional[Card]) -> np.ndarray:\n",
                "        encoding = np.zeros(52, dtype=np.float32)\n",
                "        if card is not None:\n",
                "            encoding[self._card_to_index(card)] = 1.0\n",
                "        return encoding\n",
                "    \n",
                "    def _flatten_cards(self, cards) -> List:\n",
                "        flat = []\n",
                "        for item in cards:\n",
                "            if hasattr(item, 'rank'):\n",
                "                flat.append(item)\n",
                "            else:\n",
                "                flat.extend(self._flatten_cards(item))\n",
                "        return flat\n",
                "    \n",
                "    def _get_game_state(self) -> np.ndarray:\n",
                "        state_vector = []\n",
                "        \n",
                "        hole_cards = self._flatten_cards(self.state.hole_cards[self.agent_player_index])\n",
                "        for i in range(2):\n",
                "            if i < len(hole_cards):\n",
                "                state_vector.extend(self._encode_card(hole_cards[i]))\n",
                "            else:\n",
                "                state_vector.extend(np.zeros(52, dtype=np.float32))\n",
                "        \n",
                "        board_cards = self._flatten_cards(self.state.board_cards)\n",
                "        for i in range(5):\n",
                "            if i < len(board_cards):\n",
                "                state_vector.extend(self._encode_card(board_cards[i]))\n",
                "            else:\n",
                "                state_vector.extend(np.zeros(52, dtype=np.float32))\n",
                "        \n",
                "        for i in range(self.num_players):\n",
                "            stack = self.state.stacks[i] / self.starting_stack\n",
                "            state_vector.append(min(stack, 2.0))\n",
                "        \n",
                "        total_pot = sum(self.state.bets)\n",
                "        state_vector.append(total_pot / (self.starting_stack * self.num_players))\n",
                "        \n",
                "        if self.state.actor_index is not None:\n",
                "            state_vector.append(self.state.actor_index / max(1, self.num_players - 1))\n",
                "        else:\n",
                "            state_vector.append(0.0)\n",
                "        \n",
                "        street = [0.0, 0.0, 0.0, 0.0]\n",
                "        num_board = len(board_cards)\n",
                "        if num_board == 0:\n",
                "            street[0] = 1.0\n",
                "        elif num_board == 3:\n",
                "            street[1] = 1.0\n",
                "        elif num_board == 4:\n",
                "            street[2] = 1.0\n",
                "        else:\n",
                "            street[3] = 1.0\n",
                "        state_vector.extend(street)\n",
                "        \n",
                "        return np.array(state_vector, dtype=np.float32)\n",
                "    \n",
                "    def _get_legal_actions(self) -> List[int]:\n",
                "        legal = []\n",
                "        if self.state.can_fold():\n",
                "            legal.append(ENV_FOLD)\n",
                "        if self.state.can_check_or_call():\n",
                "            legal.append(ENV_CHECK_CALL)\n",
                "        if self.state.can_complete_bet_or_raise_to():\n",
                "            legal.append(ENV_BET_RAISE)\n",
                "        return legal if legal else [ENV_CHECK_CALL]\n",
                "    \n",
                "    def _execute_action(self, action: int) -> None:\n",
                "        if action == ENV_FOLD:\n",
                "            if self.state.can_fold():\n",
                "                self.state.fold()\n",
                "            elif self.state.can_check_or_call():\n",
                "                self.state.check_or_call()\n",
                "        elif action == ENV_CHECK_CALL:\n",
                "            if self.state.can_check_or_call():\n",
                "                self.state.check_or_call()\n",
                "            elif self.state.can_fold():\n",
                "                self.state.fold()\n",
                "        elif action == ENV_BET_RAISE:\n",
                "            if self.state.can_complete_bet_or_raise_to():\n",
                "                min_raise = self.state.min_completion_betting_or_raising_to_amount\n",
                "                max_raise = self.state.max_completion_betting_or_raising_to_amount\n",
                "                raise_amount = min(min_raise * 2, max_raise)\n",
                "                self.state.complete_bet_or_raise_to(raise_amount)\n",
                "            elif self.state.can_check_or_call():\n",
                "                self.state.check_or_call()\n",
                "    \n",
                "    def _run_automations(self) -> None:\n",
                "        while self.state.can_burn_card():\n",
                "            self.state.burn_card('??')\n",
                "        while self.state.can_deal_board():\n",
                "            self.state.deal_board()\n",
                "        while self.state.can_push_chips():\n",
                "            self.state.push_chips()\n",
                "        while self.state.can_pull_chips():\n",
                "            self.state.pull_chips()\n",
                "    \n",
                "    def reset(self, seed=None, options=None) -> Tuple[np.ndarray, Dict]:\n",
                "        super().reset(seed=seed)\n",
                "        \n",
                "        self.state = NoLimitTexasHoldem.create_state(\n",
                "            automations=(\n",
                "                Automation.ANTE_POSTING,\n",
                "                Automation.BET_COLLECTION,\n",
                "                Automation.BLIND_OR_STRADDLE_POSTING,\n",
                "                Automation.HOLE_CARDS_SHOWING_OR_MUCKING,\n",
                "                Automation.HAND_KILLING,\n",
                "                Automation.CHIPS_PUSHING,\n",
                "                Automation.CHIPS_PULLING,\n",
                "            ),\n",
                "            ante_trimming_status=True,\n",
                "            raw_antes={-1: 0},\n",
                "            raw_blinds_or_straddles=(self.small_blind, self.big_blind),\n",
                "            min_bet=self.big_blind,\n",
                "            raw_starting_stacks=[self.starting_stack] * self.num_players,\n",
                "            player_count=self.num_players,\n",
                "        )\n",
                "        \n",
                "        while self.state.can_deal_hole():\n",
                "            self.state.deal_hole()\n",
                "        \n",
                "        self._run_automations()\n",
                "        \n",
                "        return self._get_game_state(), {'legal_actions': self._get_legal_actions()}\n",
                "    \n",
                "    def step(self, action: int) -> Tuple[np.ndarray, float, bool, bool, Dict]:\n",
                "        self._execute_action(action)\n",
                "        self._run_automations()\n",
                "        \n",
                "        done = self.state.status is False\n",
                "        \n",
                "        reward = 0.0\n",
                "        if done:\n",
                "            final_stack = self.state.stacks[self.agent_player_index]\n",
                "            reward = (final_stack - self.starting_stack) / self.big_blind\n",
                "        \n",
                "        obs = self._get_game_state()\n",
                "        info = {\n",
                "            'legal_actions': self._get_legal_actions() if not done else [],\n",
                "            'current_player': self.state.actor_index if not done else None\n",
                "        }\n",
                "        \n",
                "        return obs, reward, done, False, info\n",
                "    \n",
                "    def get_current_player(self) -> Optional[int]:\n",
                "        if self.state.status is False:\n",
                "            return None\n",
                "        return self.state.actor_index"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 4: Sequence Replay Buffer\n",
                "\n",
                "class SequenceReplayBuffer:\n",
                "    \"\"\"Replay buffer that stores full episodes with initial hidden state.\"\"\"\n",
                "    \n",
                "    def __init__(self, capacity: int = 5000):\n",
                "        self.buffer = deque(maxlen=capacity)\n",
                "    \n",
                "    def push(self, episode: List[Tuple], initial_hidden: Tuple[torch.Tensor, torch.Tensor]):\n",
                "        if len(episode) > 0:\n",
                "            self.buffer.append((episode, initial_hidden))\n",
                "    \n",
                "    def sample(self, batch_size: int):\n",
                "        batch = random.sample(list(self.buffer), min(batch_size, len(self.buffer)))\n",
                "        return batch\n",
                "    \n",
                "    def __len__(self):\n",
                "        return len(self.buffer)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 5: DRQN Model\n",
                "\n",
                "class DRQN(nn.Module):\n",
                "    \"\"\"Deep Recurrent Q-Network - LSTM processes full state sequence.\"\"\"\n",
                "    \n",
                "    def __init__(self, input_dim: int, hidden_dim: int = 128, \n",
                "                 output_dim: int = NUM_ACTIONS, num_layers: int = 1):\n",
                "        super().__init__()\n",
                "        self.hidden_dim = hidden_dim\n",
                "        self.num_layers = num_layers\n",
                "        \n",
                "        self.fc_feat = nn.Sequential(\n",
                "            nn.Linear(input_dim, 256),\n",
                "            nn.ReLU(),\n",
                "            nn.Linear(256, 128),\n",
                "            nn.ReLU()\n",
                "        )\n",
                "        \n",
                "        self.lstm = nn.LSTM(\n",
                "            input_size=128, \n",
                "            hidden_size=hidden_dim, \n",
                "            num_layers=num_layers, \n",
                "            batch_first=True\n",
                "        )\n",
                "        \n",
                "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
                "    \n",
                "    def forward(self, x: torch.Tensor, hidden: Optional[Tuple] = None):\n",
                "        if x.dim() == 2:\n",
                "            x = x.unsqueeze(1)\n",
                "        \n",
                "        batch_size, seq_len, _ = x.size()\n",
                "        \n",
                "        x_flat = x.view(-1, x.size(2))\n",
                "        features = self.fc_feat(x_flat)\n",
                "        features = features.view(batch_size, seq_len, -1)\n",
                "        \n",
                "        if hidden is None:\n",
                "            lstm_out, new_hidden = self.lstm(features)\n",
                "        else:\n",
                "            lstm_out, new_hidden = self.lstm(features, hidden)\n",
                "        \n",
                "        if seq_len == 1:\n",
                "            q_values = self.fc_out(lstm_out[:, -1, :])\n",
                "        else:\n",
                "            q_values = self.fc_out(lstm_out)\n",
                "        \n",
                "        return q_values, new_hidden\n",
                "    \n",
                "    def init_hidden(self, batch_size: int):\n",
                "        h = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device)\n",
                "        c = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device)\n",
                "        return (h, c)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 6: Fixed-Strategy Opponents\n",
                "\n",
                "class ManiacAgent:\n",
                "    \"\"\"Always raises/bets when possible, otherwise calls.\"\"\"\n",
                "    \n",
                "    def select_action(self, legal_actions: List[int]) -> int:\n",
                "        if ENV_BET_RAISE in legal_actions:\n",
                "            return ENV_BET_RAISE\n",
                "        if ENV_CHECK_CALL in legal_actions:\n",
                "            return ENV_CHECK_CALL\n",
                "        return ENV_FOLD\n",
                "\n",
                "\n",
                "class NitAgent:\n",
                "    \"\"\"Always folds when facing aggression, checks otherwise.\"\"\"\n",
                "    \n",
                "    def select_action(self, legal_actions: List[int]) -> int:\n",
                "        if ENV_FOLD in legal_actions and ENV_CHECK_CALL in legal_actions:\n",
                "            if random.random() < 0.8:\n",
                "                return ENV_FOLD\n",
                "        if ENV_CHECK_CALL in legal_actions:\n",
                "            return ENV_CHECK_CALL\n",
                "        return ENV_FOLD\n",
                "\n",
                "\n",
                "class RandomAgent:\n",
                "    \"\"\"Randomly selects from legal actions.\"\"\"\n",
                "    \n",
                "    def select_action(self, legal_actions: List[int]) -> int:\n",
                "        return random.choice(legal_actions)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 7: DRQN Agent\n",
                "\n",
                "class DRQNAgent:\n",
                "    \"\"\"DRQN Agent with session-based LSTM state management.\"\"\"\n",
                "    \n",
                "    def __init__(self, input_dim: int, hidden_dim: int = 128,\n",
                "                 lr: float = 1e-3, gamma: float = 0.99,\n",
                "                 epsilon_start: float = 1.0, epsilon_min: float = 0.05,\n",
                "                 epsilon_decay: float = 0.9995):\n",
                "        \n",
                "        self.input_dim = input_dim\n",
                "        self.hidden_dim = hidden_dim\n",
                "        self.model = DRQN(input_dim, hidden_dim).to(device)\n",
                "        self.target_model = DRQN(input_dim, hidden_dim).to(device)\n",
                "        self.target_model.load_state_dict(self.model.state_dict())\n",
                "        \n",
                "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
                "        self.gamma = gamma\n",
                "        self.epsilon = epsilon_start\n",
                "        self.epsilon_min = epsilon_min\n",
                "        self.epsilon_decay = epsilon_decay\n",
                "        \n",
                "        self.current_hidden = None\n",
                "    \n",
                "    def start_new_session(self):\n",
                "        self.current_hidden = None\n",
                "        print(\"[Session Reset] LSTM hidden state cleared.\")\n",
                "    \n",
                "    def get_hidden_for_episode(self) -> Tuple[torch.Tensor, torch.Tensor]:\n",
                "        if self.current_hidden is None:\n",
                "            return self.model.init_hidden(1)\n",
                "        return (self.current_hidden[0].clone(), self.current_hidden[1].clone())\n",
                "    \n",
                "    def select_action(self, state: np.ndarray, legal_actions: List[int], \n",
                "                      eval_mode: bool = False) -> int:\n",
                "        if not eval_mode and random.random() < self.epsilon:\n",
                "            return random.choice(legal_actions)\n",
                "        \n",
                "        state_t = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
                "        \n",
                "        with torch.no_grad():\n",
                "            q_values, self.current_hidden = self.model(state_t, self.current_hidden)\n",
                "        \n",
                "        q_values = q_values.cpu().numpy().flatten()\n",
                "        masked_q = np.full(NUM_ACTIONS, -np.inf)\n",
                "        for a in legal_actions:\n",
                "            masked_q[a] = q_values[a]\n",
                "        \n",
                "        return int(np.argmax(masked_q))\n",
                "    \n",
                "    def update_epsilon(self):\n",
                "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
                "    \n",
                "    def update_target_network(self):\n",
                "        self.target_model.load_state_dict(self.model.state_dict())\n",
                "    \n",
                "    def train_step(self, buffer: SequenceReplayBuffer, batch_size: int = 16) -> Optional[float]:\n",
                "        if len(buffer) < batch_size:\n",
                "            return None\n",
                "        \n",
                "        batch = buffer.sample(batch_size)\n",
                "        total_loss = 0.0\n",
                "        num_transitions = 0\n",
                "        \n",
                "        for episode, initial_hidden in batch:\n",
                "            if len(episode) == 0:\n",
                "                continue\n",
                "            \n",
                "            hidden = (initial_hidden[0].to(device), initial_hidden[1].to(device))\n",
                "            \n",
                "            for obs, action, reward, next_obs, done, legal_actions in episode:\n",
                "                state_t = torch.FloatTensor(obs).unsqueeze(0).to(device)\n",
                "                q_values, hidden = self.model(state_t, hidden)\n",
                "                q_value = q_values[0, action]\n",
                "                \n",
                "                if done:\n",
                "                    target = reward\n",
                "                else:\n",
                "                    next_state_t = torch.FloatTensor(next_obs).unsqueeze(0).to(device)\n",
                "                    with torch.no_grad():\n",
                "                        next_q_values, _ = self.target_model(next_state_t, hidden)\n",
                "                        target = reward + self.gamma * next_q_values.max().item()\n",
                "                \n",
                "                loss = F.mse_loss(q_value, torch.tensor(target, device=device))\n",
                "                total_loss += loss\n",
                "                num_transitions += 1\n",
                "        \n",
                "        if num_transitions > 0:\n",
                "            avg_loss = total_loss / num_transitions\n",
                "            \n",
                "            self.optimizer.zero_grad()\n",
                "            avg_loss.backward()\n",
                "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
                "            self.optimizer.step()\n",
                "            \n",
                "            return avg_loss.item()\n",
                "        \n",
                "        return None"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 8: Training Loop (FIXED)\n",
                "\n",
                "def train_agent(num_sessions: int = 150, hands_per_session: int = 100,\n",
                "                batch_size: int = 16, target_update_freq: int = 10):\n",
                "    \"\"\"\n",
                "    Train the DRQN agent with session-based LSTM state management.\n",
                "    \n",
                "    FIXED: Reward is now correctly captured when opponent folds.\n",
                "    Extended training: 150 sessions for better maniac performance.\n",
                "    \"\"\"\n",
                "    env = PokerKitGymEnv(num_players=2)\n",
                "    agent = DRQNAgent(input_dim=env.game_state_dim)\n",
                "    buffer = SequenceReplayBuffer(capacity=5000)\n",
                "    \n",
                "    opponents = {\n",
                "        'maniac': ManiacAgent(),\n",
                "        'nit': NitAgent(),\n",
                "        'random': RandomAgent()\n",
                "    }\n",
                "    \n",
                "    rewards_history = []\n",
                "    session_rewards = []\n",
                "    loss_history = []\n",
                "    \n",
                "    print(\"Starting training...\")\n",
                "    print(f\"Sessions: {num_sessions}, Hands per session: {hands_per_session}\")\n",
                "    print(f\"Total hands: {num_sessions * hands_per_session}\")\n",
                "    print()\n",
                "    \n",
                "    for session in range(num_sessions):\n",
                "        if session < num_sessions // 2:\n",
                "            opponent_name = 'maniac'\n",
                "        else:\n",
                "            opponent_name = 'nit'\n",
                "        \n",
                "        opponent = opponents[opponent_name]\n",
                "        agent.start_new_session()\n",
                "        \n",
                "        session_reward = 0.0\n",
                "        hands_played = 0\n",
                "        \n",
                "        for hand in range(hands_per_session):\n",
                "            initial_hidden = agent.get_hidden_for_episode()\n",
                "            state, info = env.reset()\n",
                "            episode_transitions = []\n",
                "            done = False\n",
                "            \n",
                "            while not done:\n",
                "                current_player = env.get_current_player()\n",
                "                legal_actions = info['legal_actions']\n",
                "                \n",
                "                if current_player is None:\n",
                "                    break\n",
                "                \n",
                "                if current_player == env.agent_player_index:\n",
                "                    action = agent.select_action(state, legal_actions)\n",
                "                    next_state, reward, done, truncated, info = env.step(action)\n",
                "                    \n",
                "                    # Store transition (reward will be updated at end)\n",
                "                    episode_transitions.append((\n",
                "                        state, action, 0.0, next_state, done,\n",
                "                        info.get('legal_actions', [])\n",
                "                    ))\n",
                "                    \n",
                "                    state = next_state\n",
                "                else:\n",
                "                    opp_action = opponent.select_action(legal_actions)\n",
                "                    next_state, reward, done, truncated, info = env.step(opp_action)\n",
                "                    state = next_state  # FIX: Update state after opponent acts\n",
                "            \n",
                "            # FIX: Calculate final reward from stack change\n",
                "            final_reward = (env.state.stacks[env.agent_player_index] - env.starting_stack) / env.big_blind\n",
                "            \n",
                "            # Update last transition with actual reward\n",
                "            if episode_transitions:\n",
                "                last = episode_transitions[-1]\n",
                "                episode_transitions[-1] = (last[0], last[1], final_reward, last[3], True, last[5])\n",
                "                buffer.push(episode_transitions, initial_hidden)\n",
                "                rewards_history.append(final_reward)\n",
                "                session_reward += final_reward\n",
                "                hands_played += 1\n",
                "            \n",
                "            if len(buffer) >= batch_size:\n",
                "                loss = agent.train_step(buffer, batch_size)\n",
                "                if loss is not None:\n",
                "                    loss_history.append(loss)\n",
                "            \n",
                "            agent.update_epsilon()\n",
                "        \n",
                "        if session % target_update_freq == 0:\n",
                "            agent.update_target_network()\n",
                "        \n",
                "        avg_session_reward = session_reward / max(1, hands_played)\n",
                "        session_rewards.append(avg_session_reward)\n",
                "        \n",
                "        print(f\"Session {session+1}/{num_sessions} | Opponent: {opponent_name} | \"\n",
                "              f\"Avg Reward: {avg_session_reward:.2f} | Epsilon: {agent.epsilon:.3f}\")\n",
                "    \n",
                "    print(\"\\nTraining complete!\")\n",
                "    \n",
                "    return agent, rewards_history, session_rewards, loss_history"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 9: Evaluation Function (FIXED)\n",
                "\n",
                "def evaluate_agent(agent: DRQNAgent, num_hands: int = 100, \n",
                "                   opponent_type: str = 'random') -> Dict:\n",
                "    \"\"\"Evaluate agent against a specific opponent type.\"\"\"\n",
                "    env = PokerKitGymEnv(num_players=2)\n",
                "    \n",
                "    opponents = {\n",
                "        'maniac': ManiacAgent(),\n",
                "        'nit': NitAgent(),\n",
                "        'random': RandomAgent()\n",
                "    }\n",
                "    opponent = opponents[opponent_type]\n",
                "    \n",
                "    agent.start_new_session()\n",
                "    \n",
                "    wins = 0\n",
                "    losses = 0\n",
                "    ties = 0\n",
                "    total_reward = 0.0\n",
                "    \n",
                "    for hand in range(num_hands):\n",
                "        state, info = env.reset()\n",
                "        done = False\n",
                "        \n",
                "        while not done:\n",
                "            current_player = env.get_current_player()\n",
                "            legal_actions = info['legal_actions']\n",
                "            \n",
                "            if current_player is None:\n",
                "                break\n",
                "            \n",
                "            if current_player == env.agent_player_index:\n",
                "                action = agent.select_action(state, legal_actions, eval_mode=True)\n",
                "            else:\n",
                "                action = opponent.select_action(legal_actions)\n",
                "            \n",
                "            next_state, reward, done, truncated, info = env.step(action)\n",
                "            state = next_state  # FIX: Always update state\n",
                "        \n",
                "        # FIX: Calculate reward from stack change\n",
                "        hand_reward = (env.state.stacks[env.agent_player_index] - env.starting_stack) / env.big_blind\n",
                "        \n",
                "        total_reward += hand_reward\n",
                "        if hand_reward > 0:\n",
                "            wins += 1\n",
                "        elif hand_reward < 0:\n",
                "            losses += 1\n",
                "        else:\n",
                "            ties += 1\n",
                "    \n",
                "    results = {\n",
                "        'opponent': opponent_type,\n",
                "        'hands_played': num_hands,\n",
                "        'wins': wins,\n",
                "        'losses': losses,\n",
                "        'ties': ties,\n",
                "        'win_rate': wins / num_hands,\n",
                "        'total_reward': total_reward,\n",
                "        'avg_reward': total_reward / num_hands\n",
                "    }\n",
                "    \n",
                "    print(f\"\\n=== Evaluation vs {opponent_type.upper()} ===\")\n",
                "    print(f\"Hands: {num_hands}\")\n",
                "    print(f\"W/L/T: {wins}/{losses}/{ties}\")\n",
                "    print(f\"Win Rate: {results['win_rate']:.1%}\")\n",
                "    print(f\"Total Profit: {total_reward:.1f} BBs\")\n",
                "    print(f\"Avg Profit/Hand: {results['avg_reward']:.2f} BBs\")\n",
                "    \n",
                "    return results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "============================================================\n",
                        "LSTM Poker Agent V4 - Fixed Reward + Extended Training\n",
                        "============================================================\n",
                        "\n",
                        "Key fixes:\n",
                        "  - Reward correctly captured when opponent folds\n",
                        "  - State updated after opponent acts\n",
                        "  - Extended training: 150 sessions (15,000 hands)\n",
                        "============================================================\n",
                        "Starting training...\n",
                        "Sessions: 150, Hands per session: 100\n",
                        "Total hands: 15000\n",
                        "\n",
                        "[Session Reset] LSTM hidden state cleared.\n",
                        "Session 1/150 | Opponent: maniac | Avg Reward: -28.62 | Epsilon: 0.951\n",
                        "[Session Reset] LSTM hidden state cleared.\n",
                        "Session 2/150 | Opponent: maniac | Avg Reward: -25.06 | Epsilon: 0.905\n",
                        "[Session Reset] LSTM hidden state cleared.\n",
                        "Session 3/150 | Opponent: maniac | Avg Reward: -29.36 | Epsilon: 0.861\n",
                        "[Session Reset] LSTM hidden state cleared.\n",
                        "Session 4/150 | Opponent: maniac | Avg Reward: -19.84 | Epsilon: 0.819\n",
                        "[Session Reset] LSTM hidden state cleared.\n",
                        "Session 5/150 | Opponent: maniac | Avg Reward: -21.26 | Epsilon: 0.779\n",
                        "[Session Reset] LSTM hidden state cleared.\n",
                        "Session 6/150 | Opponent: maniac | Avg Reward: -14.09 | Epsilon: 0.741\n",
                        "[Session Reset] LSTM hidden state cleared.\n",
                        "Session 7/150 | Opponent: maniac | Avg Reward: -16.19 | Epsilon: 0.705\n",
                        "[Session Reset] LSTM hidden state cleared.\n",
                        "Session 8/150 | Opponent: maniac | Avg Reward: -13.90 | Epsilon: 0.670\n",
                        "[Session Reset] LSTM hidden state cleared.\n",
                        "Session 9/150 | Opponent: maniac | Avg Reward: -15.33 | Epsilon: 0.638\n",
                        "[Session Reset] LSTM hidden state cleared.\n",
                        "Session 10/150 | Opponent: maniac | Avg Reward: -9.35 | Epsilon: 0.606\n",
                        "[Session Reset] LSTM hidden state cleared.\n",
                        "Session 11/150 | Opponent: maniac | Avg Reward: -14.99 | Epsilon: 0.577\n",
                        "[Session Reset] LSTM hidden state cleared.\n",
                        "Session 12/150 | Opponent: maniac | Avg Reward: -7.20 | Epsilon: 0.549\n",
                        "[Session Reset] LSTM hidden state cleared.\n",
                        "Session 13/150 | Opponent: maniac | Avg Reward: -9.42 | Epsilon: 0.522\n",
                        "[Session Reset] LSTM hidden state cleared.\n",
                        "Session 14/150 | Opponent: maniac | Avg Reward: -10.86 | Epsilon: 0.496\n",
                        "[Session Reset] LSTM hidden state cleared.\n",
                        "Session 15/150 | Opponent: maniac | Avg Reward: -6.40 | Epsilon: 0.472\n",
                        "[Session Reset] LSTM hidden state cleared.\n",
                        "Session 16/150 | Opponent: maniac | Avg Reward: -9.16 | Epsilon: 0.449\n",
                        "[Session Reset] LSTM hidden state cleared.\n",
                        "Session 17/150 | Opponent: maniac | Avg Reward: -8.84 | Epsilon: 0.427\n",
                        "[Session Reset] LSTM hidden state cleared.\n",
                        "Session 18/150 | Opponent: maniac | Avg Reward: -8.53 | Epsilon: 0.406\n",
                        "[Session Reset] LSTM hidden state cleared.\n",
                        "Session 19/150 | Opponent: maniac | Avg Reward: -9.14 | Epsilon: 0.387\n",
                        "[Session Reset] LSTM hidden state cleared.\n",
                        "Session 20/150 | Opponent: maniac | Avg Reward: -6.94 | Epsilon: 0.368\n",
                        "[Session Reset] LSTM hidden state cleared.\n",
                        "Session 21/150 | Opponent: maniac | Avg Reward: -7.92 | Epsilon: 0.350\n",
                        "[Session Reset] LSTM hidden state cleared.\n",
                        "Session 22/150 | Opponent: maniac | Avg Reward: -16.55 | Epsilon: 0.333\n",
                        "[Session Reset] LSTM hidden state cleared.\n",
                        "Session 23/150 | Opponent: maniac | Avg Reward: -4.30 | Epsilon: 0.317\n",
                        "[Session Reset] LSTM hidden state cleared.\n",
                        "Session 24/150 | Opponent: maniac | Avg Reward: -10.14 | Epsilon: 0.301\n",
                        "[Session Reset] LSTM hidden state cleared.\n",
                        "Session 25/150 | Opponent: maniac | Avg Reward: -7.70 | Epsilon: 0.286\n",
                        "[Session Reset] LSTM hidden state cleared.\n",
                        "Session 26/150 | Opponent: maniac | Avg Reward: -5.18 | Epsilon: 0.272\n",
                        "[Session Reset] LSTM hidden state cleared.\n",
                        "Session 27/150 | Opponent: maniac | Avg Reward: -4.48 | Epsilon: 0.259\n",
                        "[Session Reset] LSTM hidden state cleared.\n",
                        "Session 28/150 | Opponent: maniac | Avg Reward: -7.36 | Epsilon: 0.247\n",
                        "[Session Reset] LSTM hidden state cleared.\n",
                        "Session 29/150 | Opponent: maniac | Avg Reward: -2.45 | Epsilon: 0.234\n",
                        "[Session Reset] LSTM hidden state cleared.\n",
                        "Session 30/150 | Opponent: maniac | Avg Reward: -2.44 | Epsilon: 0.223\n",
                        "[Session Reset] LSTM hidden state cleared.\n",
                        "Session 31/150 | Opponent: maniac | Avg Reward: -2.25 | Epsilon: 0.212\n",
                        "[Session Reset] LSTM hidden state cleared.\n",
                        "Session 32/150 | Opponent: maniac | Avg Reward: -13.51 | Epsilon: 0.202\n",
                        "[Session Reset] LSTM hidden state cleared.\n",
                        "Session 33/150 | Opponent: maniac | Avg Reward: -12.33 | Epsilon: 0.192\n",
                        "[Session Reset] LSTM hidden state cleared.\n",
                        "Session 34/150 | Opponent: maniac | Avg Reward: -14.14 | Epsilon: 0.183\n",
                        "[Session Reset] LSTM hidden state cleared.\n",
                        "Session 35/150 | Opponent: maniac | Avg Reward: -4.91 | Epsilon: 0.174\n",
                        "[Session Reset] LSTM hidden state cleared.\n",
                        "Session 36/150 | Opponent: maniac | Avg Reward: -9.06 | Epsilon: 0.165\n",
                        "[Session Reset] LSTM hidden state cleared.\n",
                        "Session 37/150 | Opponent: maniac | Avg Reward: -8.84 | Epsilon: 0.157\n",
                        "[Session Reset] LSTM hidden state cleared.\n",
                        "Session 38/150 | Opponent: maniac | Avg Reward: -10.93 | Epsilon: 0.149\n",
                        "[Session Reset] LSTM hidden state cleared.\n",
                        "Session 39/150 | Opponent: maniac | Avg Reward: -8.49 | Epsilon: 0.142\n",
                        "[Session Reset] LSTM hidden state cleared.\n",
                        "Session 40/150 | Opponent: maniac | Avg Reward: -2.92 | Epsilon: 0.135\n",
                        "[Session Reset] LSTM hidden state cleared.\n",
                        "Session 41/150 | Opponent: maniac | Avg Reward: -7.27 | Epsilon: 0.129\n",
                        "[Session Reset] LSTM hidden state cleared.\n",
                        "Session 42/150 | Opponent: maniac | Avg Reward: -8.24 | Epsilon: 0.122\n",
                        "[Session Reset] LSTM hidden state cleared.\n",
                        "Session 43/150 | Opponent: maniac | Avg Reward: -9.17 | Epsilon: 0.116\n",
                        "[Session Reset] LSTM hidden state cleared.\n",
                        "Session 44/150 | Opponent: maniac | Avg Reward: -4.07 | Epsilon: 0.111\n",
                        "[Session Reset] LSTM hidden state cleared.\n",
                        "Session 45/150 | Opponent: maniac | Avg Reward: -6.41 | Epsilon: 0.105\n",
                        "[Session Reset] LSTM hidden state cleared.\n",
                        "Session 46/150 | Opponent: maniac | Avg Reward: -5.05 | Epsilon: 0.100\n",
                        "[Session Reset] LSTM hidden state cleared.\n"
                    ]
                },
                {
                    "ename": "KeyboardInterrupt",
                    "evalue": "",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
                        "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
                        "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m  - Extended training: 150 sessions (15,000 hands)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m60\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m agent, rewards_history, session_rewards, loss_history = \u001b[43mtrain_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
                        "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 84\u001b[39m, in \u001b[36mtrain_agent\u001b[39m\u001b[34m(num_sessions, hands_per_session, batch_size, target_update_freq)\u001b[39m\n\u001b[32m     81\u001b[39m     hands_played += \u001b[32m1\u001b[39m\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(buffer) >= batch_size:\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m     loss = \u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     85\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     86\u001b[39m         loss_history.append(loss)\n",
                        "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 86\u001b[39m, in \u001b[36mDRQNAgent.train_step\u001b[39m\u001b[34m(self, buffer, batch_size)\u001b[39m\n\u001b[32m     84\u001b[39m         loss = F.mse_loss(q_value, torch.tensor(target, device=device))\n\u001b[32m     85\u001b[39m         total_loss += loss\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m         num_transitions += \u001b[32m1\u001b[39m\n\u001b[32m     88\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m num_transitions > \u001b[32m0\u001b[39m:\n\u001b[32m     89\u001b[39m     avg_loss = total_loss / num_transitions\n",
                        "\u001b[31mKeyboardInterrupt\u001b[39m: "
                    ]
                }
            ],
            "source": [
                "# Cell 10: Run Training\n",
                "print(\"=\" * 60)\n",
                "print(\"LSTM Poker Agent V4 - Fixed Reward + Extended Training\")\n",
                "print(\"=\" * 60)\n",
                "print()\n",
                "print(\"Key fixes:\")\n",
                "print(\"  - Reward correctly captured when opponent folds\")\n",
                "print(\"  - State updated after opponent acts\")\n",
                "print(\"  - Extended training: 150 sessions (15,000 hands)\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "agent, rewards_history, session_rewards, loss_history = train_agent()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 11: Visualize Training\n",
                "print(\"\\n\" + \"=\" * 60)\n",
                "print(\"TRAINING VISUALIZATION\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "# Session rewards\n",
                "axes[0].plot(session_rewards, alpha=0.7)\n",
                "axes[0].axhline(y=0, color='r', linestyle='--', label='Break-even')\n",
                "axes[0].axvline(x=75, color='g', linestyle='--', alpha=0.5, label='Switch to Nit')\n",
                "axes[0].set_xlabel('Session')\n",
                "axes[0].set_ylabel('Average Reward (BBs)')\n",
                "axes[0].set_title('Training Progress - Session Rewards')\n",
                "axes[0].legend()\n",
                "axes[0].grid(True, alpha=0.3)\n",
                "\n",
                "# Loss history\n",
                "if loss_history:\n",
                "    window = 100\n",
                "    if len(loss_history) > window:\n",
                "        smoothed_loss = np.convolve(loss_history, np.ones(window)/window, mode='valid')\n",
                "        axes[1].plot(smoothed_loss, alpha=0.7)\n",
                "    else:\n",
                "        axes[1].plot(loss_history, alpha=0.7)\n",
                "    axes[1].set_xlabel('Training Step')\n",
                "    axes[1].set_ylabel('Loss')\n",
                "    axes[1].set_title('Training Loss (Smoothed)')\n",
                "    axes[1].grid(True, alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('training_results_v4.png', dpi=150)\n",
                "plt.show()\n",
                "print(\"\\nTraining plot saved to training_results_v4.png\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 12: Final Evaluation\n",
                "print(\"\\n\" + \"=\" * 60)\n",
                "print(\"FINAL EVALUATION\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "results = {}\n",
                "for opp in ['random', 'maniac', 'nit']:\n",
                "    results[opp] = evaluate_agent(agent, num_hands=100, opponent_type=opp)\n",
                "\n",
                "print(\"\\n\" + \"=\" * 60)\n",
                "print(\"SUMMARY\")\n",
                "print(\"=\" * 60)\n",
                "for opp, r in results.items():\n",
                "    status = \" PASS\" if r['avg_reward'] > 0 else \" FAIL\"\n",
                "    print(f\"{opp.capitalize():10} | Avg: {r['avg_reward']:+.2f} BB/hand | {status}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
